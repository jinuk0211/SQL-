{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Generic, TypeVar, Union, NamedTuple, Protocol, Optional, runtime_checkable, Tuple\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from transformers import StoppingCriteriaList\n",
        "from datetime import datetime\n",
        "import os, sys, pickle\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "State = TypeVar(\"State\")\n",
        "Action = TypeVar(\"Action\")\n",
        "Example = TypeVar(\"Example\")\n",
        "Trace = tuple[list[State], list[Action]]\n",
        "\n",
        "def create_directory_if_not_exists(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "class GenerateOutput(NamedTuple):\n",
        "    text: list[str]\n",
        "    log_prob: Optional[list[np.ndarray]] = None\n",
        "\n",
        "\n",
        "class LanguageModel(ABC):\n",
        "    @abstractmethod\n",
        "    def generate(self,\n",
        "                 inputs: list[str],\n",
        "                 max_length: Optional[int] = None,\n",
        "                 max_new_tokens: Optional[int] = None,\n",
        "                 do_sample: bool = False,\n",
        "                 temperature: float = 1.0,\n",
        "                 top_k: int = 50,\n",
        "                 top_p: float = 1.0,\n",
        "                 num_return_sequences: int = 1,\n",
        "                 eos_token_id: Union[None, str, int, list[str, int]] = None,\n",
        "                 hide_input: bool = True,\n",
        "                 output_log_probs: bool = False,\n",
        "                 stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
        "                 **kwargs) -> GenerateOutput:\n",
        "        \"\"\"Generate text from a list of prompts.\n",
        "\n",
        "        :param inputs: List of prompts.\n",
        "        :param max_length: Maximum length of the total output (input + generated).\n",
        "        :param max_new_tokens: Maximum length of generated tokens. Override max_length.\n",
        "        :param do_sample: If False, do greedy decoding.\n",
        "        :param temperature: Temperature for sampling.\n",
        "        :param top_k: Top-k for sampling.\n",
        "        :param top_p: Top-p for sampling.\n",
        "        :param num_return_sequences:\n",
        "        :param eos_token_id: Token id for end of sentence. Passed *str* will be translated into token_id.\n",
        "                             Passed *list* will be treated as multiple possible tokens ending the generation.\n",
        "        :param hide_input: If set true, decode only the generated part.\n",
        "        :param output_log_probs: If set true, also output the log_probs of each generated token\n",
        "        :param stopping_criteria:\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_next_token_logits(self,\n",
        "                              prompt: Union[str, list[str]],\n",
        "                              candidates: Union[list[str], list[list[str]]],\n",
        "                              postprocess: Optional[str] = None,\n",
        "                              **kwargs) -> list[np.ndarray]:\n",
        "        \"\"\" TODO: doc\n",
        "\n",
        "        :param prompt:\n",
        "        :param candidates:\n",
        "        :param postprocess: optional, can be 'log_softmax' or 'softmax'. Apply the corresponding function to logits before returning\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_loglikelihood(self,\n",
        "                          prefix: str,\n",
        "                          contents: list[str],\n",
        "                          **kwargs) -> np.ndarray:\n",
        "        \"\"\"Get the log likelihood of the contents given the prefix.\n",
        "\n",
        "        :param prefix: The prefix to be excluded from the log likelihood.\n",
        "        :param contents: The contents to evaluate (must include the prefix).\n",
        "        \"\"\"\n",
        "        ...\n",
        "#------------------\n",
        "    # def _expand(self, node: MCTSNode):\n",
        "\n",
        "    #     if node.state is None:\n",
        "    #         node.state = self.world_model.step(node.parent.state, node.action)\n",
        "    #         # reward is calculated after the state is updated, so that the\n",
        "    #         # information can be cached and passed from the world model\n",
        "    #         # to the reward function with **aux without repetitive computation\n",
        "    #         node.reward, node.reward_details = self.search_config. \\\n",
        "    #             reward(node.parent.state, node.action, **node.fast_reward_details)\n",
        "    #         node.is_terminal = self.world_model.is_terminal(node.state)\n",
        "\n",
        "    #     if node.is_terminal:\n",
        "    #         return\n",
        "\n",
        "    #     # print(f'Step {node.state.step_idx + 1}: ')\n",
        "    #     children = []\n",
        "    #     actions = self.search_config.get_actions(node.state)\n",
        "#-------------------------\n",
        "\n",
        "class WorldModel(ABC, Generic[State, Action, Example]):\n",
        "    def __init__(self) -> None:\n",
        "        self.example = None\n",
        "        self.prompt = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def init_state(self) -> State: ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, state: State, action: Action) -> Union[State, Tuple[State, dict]]:\n",
        "        \"\"\" Returns the next state and optionally an auxiliary data dict\n",
        "\n",
        "        :param state: The current state\n",
        "        :param action: The action to take\n",
        "        :return: The next state and optionally an auxiliary data dict\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_terminal(self, state: State) -> bool: ...\n",
        "\n",
        "    def update_example(self, example: Example, prompt = None) -> None:\n",
        "        if prompt is not None:\n",
        "            self.prompt = prompt\n",
        "        self.example = example\n",
        "\n",
        "class DefaultWorldModel(WorldModel):\n",
        "    # A default implementation of WorldModel that only\n",
        "    # saves the action sequence as the state\n",
        "\n",
        "    def __init__(self, base_model) -> None:\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def init_state(self):\n",
        "        return []\n",
        "\n",
        "    def step(self, state, action):\n",
        "        return state + [action], {}\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        # By default the state is never terminal\n",
        "        return False\n",
        "\n",
        "\n",
        "class SearchConfig(ABC, Generic[State, Action, Example]):\n",
        "    def __init__(self) -> None:\n",
        "        self.example = None\n",
        "        self.prompt = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_actions(self, state: State) -> list[Action]: ...\n",
        "\n",
        "    def fast_reward(self, state: State, action: Action) -> tuple[float, dict]:\n",
        "        return 0, {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def reward(self, state, action, **kwargs) -> tuple[float, dict]: ...\n",
        "\n",
        "    def update_example(self, example: Example, prompt = None) -> None:\n",
        "        if prompt is not None:\n",
        "            self.prompt = prompt\n",
        "        self.example = example\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class AlgorithmOutput(Protocol[State]):\n",
        "    terminal_state: State\n",
        "    trace: Trace\n",
        "\n",
        "\n",
        "class SearchAlgorithm(ABC):\n",
        "    def __init__(self, **kwargs): ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, world_model: WorldModel, search_config: SearchConfig, **kwargs) -> AlgorithmOutput: ...\n",
        "\n",
        "\n",
        "class Reasoner(ABC, Generic[State, Action, Example]):\n",
        "    def __init__(self,\n",
        "                 world_model: WorldModel[State, Action, Example],\n",
        "                 search_config: SearchConfig[State, Action, Example],\n",
        "                 search_algo: SearchAlgorithm) -> None:\n",
        "        self.world_model = world_model\n",
        "        self.search_config = search_config\n",
        "        self.search_algo = search_algo\n",
        "\n",
        "    def __call__(self, example: Example, prompt = None, **kwargs) -> AlgorithmOutput[State]:\n",
        "        self.world_model.update_example(example, prompt=prompt)\n",
        "        self.search_config.update_example(example, prompt=prompt)\n",
        "        return self.search_algo(self.world_model, self.search_config, **kwargs)\n"
      ],
      "metadata": {
        "id": "I1kiRyB5ovfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "import sqlparse\n",
        "import requests\n",
        "import re\n",
        "AgentAction = str\n",
        "\n",
        "CLAUSE_KEYWORDS = ['select', 'from', 'where', 'group by', 'having', 'order by', 'limit', 'intersect', 'union', 'except', 'union all']\n",
        "JOIN_KEYWORDS = ['join', 'on', 'as', 'right join', 'inner join', 'left join']\n",
        "OTHER_KEYWORDS = ['distinct']\n",
        "BIRD_KEYWORDS = ['if', 'else', 'datediff', 'over', 'instr', 'case', 'partition by', 'iif', 'float', 'real', 'when', 'int', 'using', 'timestampdiff', 'then', 'substr', 'cast', 'integer', 'strftime', 'end']\n",
        "WHERE_OPS = ['not', 'between', 'in', 'like', 'is', 'exists', 'not null', 'null']\n",
        "AGG_OPS = ['max', 'min', 'count', 'sum', 'avg']\n",
        "COND_OPS = ['and', 'or']\n",
        "ORDER_OPS = ['desc', 'asc']\n",
        "SQL_KEYWORDS = []\n",
        "SQL_KEYWORDS.extend(CLAUSE_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(JOIN_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(OTHER_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(BIRD_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(WHERE_OPS)\n",
        "SQL_KEYWORDS.extend(AGG_OPS)\n",
        "SQL_KEYWORDS.extend(COND_OPS)\n",
        "SQL_KEYWORDS.extend(ORDER_OPS)\n",
        "SQL_KEYWORDS = [i.upper() for i in SQL_KEYWORDS]\n",
        "\n",
        "class AgentState(NamedTuple):\n",
        "    step_idx: int\n",
        "    last_blocks_state: str\n",
        "    blocks_state: str\n",
        "    buffered_action: AgentAction\n",
        "\n",
        "class AgentWorldModel(WorldModel):\n",
        "    def __init__(self,\n",
        "                 base_model: LanguageModel,\n",
        "                 prompt: dict,\n",
        "                 max_steps: int = 4,\n",
        "                 batch_size: int = 1) -> None:\n",
        "        super().__init__()\n",
        "        self.max_steps = max_steps\n",
        "        self.base_model = base_model\n",
        "        self.prompt = prompt\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def init_state(self) -> AgentState:\n",
        "        return AgentState(step_idx=0,\n",
        "                          last_blocks_state=\"\",\n",
        "                          blocks_state=\"\",\n",
        "                          buffered_action=\"\")\n",
        "\n",
        "    def step(self, state: AgentState, action: AgentAction) -> tuple[AgentState, dict]:\n",
        "        step_idx = state.step_idx\n",
        "        # blocks_state = state.blocks_state + action + (\"; \" if action != \"done\" and action != \"none\" else \"\")\n",
        "\n",
        "        if action == \";\" or action == \" ;\" or action.endswith(\";\"):\n",
        "            # blocks_state = state.blocks_state + (\"\" if state.blocks_state.endswith(\";\") or state.blocks_state.endswith(\"; \") else \"; \") + action\n",
        "            # blocks_state = state.blocks_state + \" \" + action\n",
        "            blocks_state = state.blocks_state + action if not state.blocks_state else state.blocks_state + \" \" + action\n",
        "        else:\n",
        "            blocks_state = state.blocks_state + action if not state.blocks_state else state.blocks_state + \" \" + action\n",
        "\n",
        "        new_buffered_action = action\n",
        "\n",
        "        state = AgentState(step_idx=step_idx + 1,\n",
        "                        last_blocks_state=state.blocks_state,\n",
        "                        blocks_state=blocks_state,\n",
        "                        buffered_action=new_buffered_action)\n",
        "        return state\n",
        "\n",
        "    def is_terminal(self, state: AgentState) -> bool:\n",
        "        if state.buffered_action in [';', ' ;'] or state.buffered_action.endswith(\";\"):\n",
        "            return True\n",
        "        elif state.step_idx == self.max_steps:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "class AgentConfig(SearchConfig):\n",
        "    def __init__(self,\n",
        "                 base_model: LanguageModel,\n",
        "                 prompt: dict,\n",
        "                 batch_size: int = 1,\n",
        "                 reward_alpha: float = 0.5,\n",
        "                 goal_reward_default: float = 0.,\n",
        "                 goal_reached_reward: float = 100.) -> None:\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.example = None\n",
        "        self.prompt = prompt\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_alpha = reward_alpha\n",
        "        self.goal_reward_default = goal_reward_default\n",
        "        self.goal_reached_reward = goal_reached_reward\n",
        "\n",
        "    def lexical(self, query, values):\n",
        "        if isinstance(query, str):\n",
        "            for placeholder, value in values.items():\n",
        "                query = query.replace(placeholder, value)\n",
        "        elif isinstance(query, list):\n",
        "            for i in range(len(query)):\n",
        "                if query[i] in values:\n",
        "                    query[i] = values[query[i]]\n",
        "        return query\n",
        "\n",
        "    def delexical(self, query):\n",
        "        values = {}\n",
        "        new_query = \"\"\n",
        "        in_value = False\n",
        "        in_col = False\n",
        "        value = \"\"\n",
        "        placeholder_id = 0\n",
        "        new_query = \"\"\n",
        "        for char in query:\n",
        "            if char == \"'\":\n",
        "                in_value = not in_value\n",
        "                value += char\n",
        "                if not in_value:\n",
        "                    values[f\"value_{placeholder_id}\"] = value\n",
        "                    new_query += f\"value_{placeholder_id}\"\n",
        "                    placeholder_id += 1\n",
        "                    value = \"\"\n",
        "            else:\n",
        "                if not in_value:\n",
        "                    new_query += char\n",
        "                else:\n",
        "                    value += char\n",
        "        return new_query, values\n",
        "\n",
        "    def format_query(self, q, format_type):\n",
        "        if format_type == 'unnormalized':\n",
        "            return q[\"query\"]\n",
        "        elif format_type == 'normalized':\n",
        "            return q[\"gold\"][\"query_normalized\"]\n",
        "        else:\n",
        "            raise ValueError(f\"format_type {format_type} not supported\")\n",
        "\n",
        "    def _is_whitespace(self, sqlparse_token):\n",
        "        return sqlparse_token.ttype == sqlparse.tokens.Whitespace\n",
        "\n",
        "\n",
        "\n",
        "    def normalize_sql(self, sql_exp):\n",
        "        sql_exp = sql_exp.replace('\"', \"'\")\n",
        "        if sql_exp.count(\n",
        "                \"'\") % 2 != 0:  # odd number of single quotes, meaning the value is incomplete or value contains a single quote\n",
        "            odd_quotes = True\n",
        "        else:\n",
        "            odd_quotes = False\n",
        "\n",
        "        if not odd_quotes:\n",
        "            sql_exp, values = self.delexical(sql_exp)\n",
        "            sql_exp = sql_exp.lower()\n",
        "\n",
        "        sql_exp = sql_exp.rstrip(\";\")\n",
        "        parse = sqlparse.parse(sql_exp)\n",
        "        sql = parse[0]\n",
        "        flat_tokens = sql.flatten()\n",
        "        sql_tokens = [\n",
        "            (token.value.upper() if token.value in SQL_KEYWORDS else token.value)\n",
        "            for token in flat_tokens if not self._is_whitespace(token)\n",
        "        ]\n",
        "\n",
        "        sql_lower = ' '.join(sql_tokens)\n",
        "        sql_lower = sql_lower.replace(' . ', '.')\n",
        "        for op in AGG_OPS:\n",
        "            sql_lower = sql_lower.replace(f\" {op.upper()} (\", f\" {op.upper()}(\")\n",
        "        sql_lower = sql_lower.replace('( ', '(')\n",
        "        sql_lower = sql_lower.replace(' )', ')')\n",
        "        sql_lower = sql_lower.replace(' ,', ',')\n",
        "\n",
        "        ### BIRD-SQL special cases ###\n",
        "        sql_lower = sql_lower.replace(' AS text', ' AS TEXT')\n",
        "        sql_lower = sql_lower.replace(' length(', ' LENGTH(')\n",
        "        sql_lower = sql_lower.replace(' total(', ' TOTAL(')\n",
        "        sql_lower = sql_lower.replace(' round(', ' ROUND(')\n",
        "        ### END ###\n",
        "\n",
        "        sql_lower = sql_lower.rstrip(\";\")\n",
        "        sql_lower += ';'\n",
        "\n",
        "        if not odd_quotes:\n",
        "            # sql_tokens = self.lexical(sql_tokens, values)\n",
        "            sql_lower = self.lexical(sql_lower, values)\n",
        "        # else:\n",
        "        #     print(\"Cannot process the following SQL\")\n",
        "        #     print(sql_exp, sql_tokens)\n",
        "        return sql_lower\n",
        "\n",
        "    def segment_step(self, sql_completion):\n",
        "        try:\n",
        "            parse = sqlparse.parse(sql_completion)\n",
        "            sql = parse[0]\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "        flat_tokens = sql.flatten()\n",
        "        sql_tokens = [\n",
        "            (token.value.upper() if token.value in SQL_KEYWORDS else token.value)\n",
        "            for token in flat_tokens\n",
        "        ]\n",
        "\n",
        "        step_length = 0\n",
        "        for i, token in enumerate(sql_tokens[1:]):\n",
        "            if token.lower() in CLAUSE_KEYWORDS:\n",
        "                step_length = i + 1\n",
        "                break\n",
        "\n",
        "        if step_length == 0:\n",
        "            # No more clauses, the entire completion is a step\n",
        "            return sql_completion\n",
        "        else:\n",
        "            return \"\".join(sql_tokens[:step_length])\n",
        "\n",
        "    def get_actions(self, state: AgentState) -> list[AgentAction]:\n",
        "        if state.step_idx == self.prompt['deapth_limit']-1:\n",
        "            if self.example['target'].startswith(state.blocks_state):\n",
        "                return [('done',100.0)]\n",
        "            else:\n",
        "                return [('done',99.99)]\n",
        "\n",
        "            # if self.example['output'].startswith(state.blocks_state):\n",
        "            #     return [('done',100.0)]\n",
        "            # else:\n",
        "            #     return [('done',99.99)]\n",
        "        else:\n",
        "            # output = requests.post(self.base_model['select'], json={\"instruction\": self.example['instruction'], \"input\": self.example['instruction'] + \"\\n\" +self.example['input']+state.blocks_state, \"output\": [] }).json()\n",
        "            # print(self.example['input'])\n",
        "            print(state.blocks_state)\n",
        "            print(self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state))\n",
        "            # input()\n",
        "            input = self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state)\n",
        "            # output = requests.post(self.base_model['select'], json={ \"input\": self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state), \"output\": [] }).json()\n",
        "            output = self.base_model.chat.completions.create(\n",
        "                  model=\"gpt-4o-mini\",\n",
        "                  messages=[\n",
        "                      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": input}\n",
        "                  ]\n",
        "              )\n",
        "            # def is_valid_string(s):\n",
        "            #     pattern = r'^(\\[[^\\]]+\\]: <[^>]+>)'\n",
        "            #     if \"; \" not in s:\n",
        "            #         return bool(s in ['none','done'])\n",
        "            #     else:\n",
        "            #         if not s.endswith(\"; done\"):\n",
        "            #             return False\n",
        "            #         else:\n",
        "            #             #  and x.split('<')[-1].split('>')[0] in self.example['input']\n",
        "            #             return all([bool(re.match(pattern, x)) for x in s.split(\"; \")[:-1]])\n",
        "\n",
        "            # def is_valid_string(s):\n",
        "            #     if \";\" not in s:\n",
        "            #         if s == \"done\" or s == \" done\":\n",
        "            #             return s\n",
        "            #         return \"\"\n",
        "            #     else:\n",
        "            #         if s == \"; done\" or s == \";done\":\n",
        "            #             return \"; done\"\n",
        "            #         elif s.endswith(\"done\"):\n",
        "            #             return s.split(\"done\")[0]\n",
        "            #         else:\n",
        "            #             return s\n",
        "\n",
        "            # sql_completions = []\n",
        "            # for key in output.keys():\n",
        "            #     key = is_valid_string(key)\n",
        "            #     if key:\n",
        "            #         if key not in [\"done\", \" done\", \"; done\", \";done\"]:\n",
        "            #             sql_completions.append(self.normalize_sql(key))\n",
        "            #         else:\n",
        "            #             sql_completions.append(key)\n",
        "            #     else:\n",
        "            #         continue\n",
        "\n",
        "            def is_valid_string(s):\n",
        "                if \";\" not in s:\n",
        "                    return False\n",
        "                else:\n",
        "                    return True\n",
        "\n",
        "            sql_completions = [key for key in output.keys() if is_valid_string(key)]\n",
        "            # sql_completions = [self.normalize_sql(key) for key in output.keys() if is_valid_string(key)]\n",
        "\n",
        "            actions = set([\n",
        "                (\n",
        "                    self.segment_step(sql[len(state.blocks_state):].lstrip()).rstrip()\n",
        "                    if len(sql) > len(state.blocks_state)\n",
        "                    else sql\n",
        "                )\n",
        "                for sql in sql_completions\n",
        "            ])\n",
        "\n",
        "            actions = list(actions)\n",
        "\n",
        "            # p_reward = requests.post(self.base_model['select'], json={\"input\": self.example['instruction'] + \"\\n\" + self.example['input']+state.blocks_state, \"output\": actions}).json()\n",
        "\n",
        "            p_reward = requests.post(self.base_model['select'], json={\"input\": self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state), \"output\": actions}).json()\n",
        "            actions_scores_list = [(a,min(r,99.99)) for a,r in zip(actions, p_reward)]\n",
        "            actions_scores_list = sorted(actions_scores_list, key=lambda x: x[1], reverse=True)[:self.prompt['step_topk']]\n",
        "\n",
        "            # if self.example['output'].startswith(state.blocks_state):\n",
        "            #     gt_action = self.example['output'][len(state.blocks_state):]\n",
        "            #     actions_scores_list = [(gt_action, 100.0)]+[(a,r) for a,r in actions_scores_list if a!=gt_action]\n",
        "                # actions_scores_list = [(gt_action, requests.post(self.base_model['select'], json={ \"input\": self.example['input']+state.blocks_state, \"output\": [gt_action] }).json()[0])]+[(a,r) for a,r in actions_scores_list if a!=gt_action]\n",
        "            return actions_scores_list\n",
        "\n",
        "    def fast_reward(self, state: AgentState, action: AgentAction) -> tuple[float, dict]:\n",
        "        intuition = action[1]\n",
        "        self_eval = intuition\n",
        "\n",
        "        return (self.calculate_reward(intuition, self_eval),\n",
        "                {'intuition': intuition, \"self_eval\": self_eval})\n",
        "\n",
        "    def calculate_reward(self, intuition, goal_reached=None) -> float:\n",
        "        # to provide a unified interface for reward and fast_reward\n",
        "        if goal_reached is None:\n",
        "            goal_reward = self.goal_reward_default\n",
        "        elif goal_reached[0]:\n",
        "            goal_reward = goal_reached[1]\n",
        "        else:\n",
        "            goal_reward = goal_reached[1]\n",
        "        return intuition * self.reward_alpha + goal_reward * (1 - self.reward_alpha)\n",
        "\n",
        "    def reward(self, state: AgentState, action: AgentAction,\n",
        "               intuition: float = None) -> tuple[float, dict]:\n",
        "        # if action == \"done\" or action == \"none\" or action == \" done\":\n",
        "        if action.endswith(\";\"):\n",
        "            goal_reached_if = True\n",
        "            # goal_reached_score = requests.post(self.base_model['reward'], json={ \"input\": self.example['instruction'] + \"\\n\" + self.example['input'], \"output\": [state.blocks_state+action]}).json()[0]\n",
        "            goal_reached_score = requests.post(self.base_model['reward'], json={ \"input\":self.example['input'], \"output\": [state.blocks_state+action]}).json()[0]\n",
        "\n",
        "            goal_reached = (goal_reached_if, goal_reached_score)\n",
        "        else:\n",
        "            goal_reached = (False, 0.0)\n",
        "        return (self.calculate_reward(intuition, goal_reached),\n",
        "                {'intuition': intuition, 'goal_reached': goal_reached})\n"
      ],
      "metadata": {
        "id": "9hLzEo8xolh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import Generic, Optional, NamedTuple, Callable, Hashable\n",
        "import itertools\n",
        "from abc import ABC\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "\n",
        "class MCTSNode(Generic[State, Action, Example]):\n",
        "    id_iter = itertools.count()\n",
        "\n",
        "    @classmethod\n",
        "    def reset_id(cls):\n",
        "        cls.id_iter = itertools.count()\n",
        "\n",
        "    def __init__(self, state: Optional[State], action: Optional[Action], parent: \"Optional[MCTSNode]\" = None,\n",
        "                 fast_reward: float = 0., fast_reward_details=None,\n",
        "                 is_terminal: bool = False, calc_q: Callable[[list[float]], float] = np.mean):\n",
        "        \"\"\"\n",
        "        A node in the MCTS search tree\n",
        "\n",
        "        :param state: the current state\n",
        "        :param action: the action of the last step, i.e., the action from parent node to current node\n",
        "        :param parent: the parent node, None if root of the tree\n",
        "        :param fast_reward: an estimation of the reward of the last step\n",
        "        :param is_terminal: whether the current state is a terminal state\n",
        "        :param calc_q: the way to calculate the Q value from histories. Defaults: np.mean\n",
        "        \"\"\"\n",
        "        self.id = next(MCTSNode.id_iter)\n",
        "        if fast_reward_details is None:\n",
        "            fast_reward_details = {}\n",
        "        self.cum_rewards: list[float] = []\n",
        "        self.fast_reward = self.reward = fast_reward\n",
        "        self.fast_reward_details = fast_reward_details\n",
        "        self.is_terminal = is_terminal\n",
        "        self.action = action\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children: 'Optional[list[MCTSNode]]' = None\n",
        "        self.calc_q = calc_q\n",
        "        if parent is None:\n",
        "            self.depth = 0\n",
        "        else:\n",
        "            self.depth = parent.depth + 1\n",
        "\n",
        "    # noinspection PyPep8Naming\n",
        "    @property\n",
        "    def Q(self) -> float:\n",
        "        if self.state is None:\n",
        "            return self.fast_reward\n",
        "        else:\n",
        "            return self.calc_q(self.cum_rewards)\n",
        "\n",
        "\n",
        "class MCTSResult(NamedTuple):\n",
        "    terminal_state: State\n",
        "    cum_reward: float\n",
        "    trace: Trace\n",
        "    trace_worst: Trace\n",
        "    # trace_all: Trace\n",
        "    trace_of_nodes: list[MCTSNode]\n",
        "    tree_state: MCTSNode\n",
        "    trace_in_each_iter: list[list[MCTSNode]] = None\n",
        "    tree_state_after_each_iter: list[MCTSNode] = None\n",
        "    aggregated_result: Optional[Hashable] = None\n",
        "\n",
        "class MCTSAggregation(Generic[State, Action, Example], ABC):\n",
        "    def __init__(self, retrieve_answer: Callable[[State], Hashable],\n",
        "                 weight_policy: str = 'edge'):\n",
        "        assert weight_policy in ['edge', 'edge_inverse_depth', 'uniform']\n",
        "        self.retrieve_answer = retrieve_answer\n",
        "        self.weight_policy = weight_policy\n",
        "\n",
        "    def __call__(self, tree_state: MCTSNode[State, Action,Example]) -> Optional[Hashable]:\n",
        "        answer_dict = defaultdict(lambda: 0)\n",
        "\n",
        "        def visit(cur: MCTSNode[State, Action, Example]):\n",
        "            if cur.state is None:\n",
        "                return []\n",
        "            if cur.is_terminal:\n",
        "                answer = self.retrieve_answer(cur.state)\n",
        "                if answer is None:\n",
        "                    print(\"MCTSAggregation: no answer retrieved.\")\n",
        "                    return []\n",
        "                if self.weight_policy == 'edge':\n",
        "                    answer_dict[answer] += cur.reward\n",
        "                elif self.weight_policy == 'edge_inverse_depth':\n",
        "                    answer_dict[answer] += cur.reward / cur.depth\n",
        "                elif self.weight_policy == 'uniform':\n",
        "                    answer_dict[answer] += 1.0\n",
        "                return [(answer, cur.depth)]\n",
        "            depth_list = defaultdict(list)\n",
        "            cur_list = []\n",
        "            for child in cur.children:\n",
        "                cur_list.extend(child_info := visit(child))\n",
        "                for answer, depth in child_info:\n",
        "                    depth_list[answer].append(depth)\n",
        "            for answer, depths in depth_list.items():\n",
        "                if self.weight_policy == 'edge':\n",
        "                    answer_dict[answer] += cur.reward\n",
        "                elif self.weight_policy == 'edge_inverse_depth':\n",
        "                    answer_dict[answer] += cur.reward / np.mean(depths)\n",
        "            return cur_list\n",
        "\n",
        "        visit(tree_state)\n",
        "\n",
        "        if len(answer_dict) == 0:\n",
        "            return None\n",
        "        return max(answer_dict, key=lambda answer: answer_dict[answer])"
      ],
      "metadata": {
        "id": "v5FVs_v70Ta8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpx1QoUznmsG"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "import sqlparse\n",
        "import requests\n",
        "import re\n",
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import Generic, Optional, NamedTuple, Callable, Hashable\n",
        "import itertools\n",
        "from abc import ABC\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "State = TypeVar(\"State\")\n",
        "Action = TypeVar(\"Action\")\n",
        "Example = TypeVar(\"Example\")\n",
        "Trace = tuple[list[State], list[Action]]\n",
        "\n",
        "class MCTS(SearchAlgorithm, Generic[State, Action, Example]):\n",
        "    def __init__(self,\n",
        "                 output_trace_in_each_iter: bool = False,\n",
        "                 w_exp: float = 1.,\n",
        "                 depth_limit: int = 5,\n",
        "                 n_iters: int = 10,\n",
        "                 cum_reward: Callable[[list[float]], float] = sum,\n",
        "                 calc_q: Callable[[list[float]], float] = np.mean,\n",
        "                 simulate_strategy: str | Callable[[list[float]], int] = 'max',\n",
        "                 output_strategy: str = 'max_reward',\n",
        "                 uct_with_fast_reward: bool = True,\n",
        "                 aggregator: Optional[MCTSAggregation] = None,\n",
        "                 disable_tqdm: bool = True,\n",
        "                 node_visualizer: Callable[[MCTSNode], dict] = lambda x: x.__dict__):\n",
        "        \"\"\"\n",
        "        MCTS algorithm\n",
        "\n",
        "        :param output_trace_in_each_iter: whether to output the trace of the chosen trajectory in each iteration ; the trace is *deepcopy*-ed\n",
        "                                          will also output *tree_state_after_each_iter*, which is the *deepcopy*-ed root\n",
        "        :param w_exp: the weight of exploration in UCT\n",
        "        :param cum_reward: the way to calculate the cumulative reward from each step. Defaults: sum\n",
        "        :param calc_q: the way to calculate the Q value from histories. Defaults: np.mean\n",
        "        :param simulate_strategy: simulate strategy. Options: 'max', 'sample', 'random', or use a custom function\n",
        "        :param output_strategy: the way to output the result. The nodes are not *deepcopy*-ed, so the information is after all iterations\n",
        "                                Options: 'max_reward': dfs on the final tree to find a trajectory with max reward using :param cum_reward:\n",
        "                                         'follow_max': starting from root, choose the maximum reward child at each step. May output a non-terminal node if dead end\n",
        "                                         'max_visit': the terminal node with maximum number of visits\n",
        "                                         'max_iter': the trajectory with a terminal node and max reward among those in each iteration\n",
        "                                         'last_iter': the last trajectory. May output a non-terminal node if the last iteration leads to a dead end\n",
        "                                         'last_terminal_iter': the last trajectory with a terminal node\n",
        "                                Outputs *None* if no trajectory with terminal node but required\n",
        "        :param uct_with_fast_reward: if True, use fast_reward instead of reward for unvisited children in UCT\n",
        "                                     Otherwise, visit the *unvisited* children with maximum fast_reward first\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.world_model = None\n",
        "        self.search_config = None\n",
        "        self.output_trace_in_each_iter = output_trace_in_each_iter\n",
        "        self.w_exp = w_exp\n",
        "        self.depth_limit = depth_limit\n",
        "        self.n_iters = n_iters\n",
        "        self.cum_reward = cum_reward\n",
        "        self.calc_q = calc_q\n",
        "        default_simulate_strategies: dict[str, Callable[[list[float]], int]] = {\n",
        "            'max': lambda x: np.argmax(x),\n",
        "            'sample': lambda x: np.random.choice(len(x), p=x),\n",
        "            'random': lambda x: np.random.choice(len(x)),\n",
        "        }\n",
        "        self.simulate_choice: Callable[[list[float]], int] = default_simulate_strategies.get(simulate_strategy,\n",
        "                                                                                             simulate_strategy)\n",
        "        assert output_strategy in ['max_reward', 'follow_max', 'max_visit', 'max_iter', 'last_iter',\n",
        "                                   'last_terminal_iter']\n",
        "        self.output_strategy = output_strategy\n",
        "        self.uct_with_fast_reward = uct_with_fast_reward\n",
        "        self._output_iter: list[MCTSNode] = None\n",
        "        self._output_cum_reward = -math.inf\n",
        "        self.trace_in_each_iter: list[list[MCTSNode]] = None\n",
        "        self.root: Optional[MCTSNode] = None\n",
        "        self.disable_tqdm = disable_tqdm\n",
        "        self.node_visualizer = node_visualizer\n",
        "        self.aggregator = aggregator\n",
        "        self.node_visualizer = node_visualizer\n",
        "        self.aggregator = aggregator\n",
        "\n",
        "    def iterate(self, node: MCTSNode) -> list[MCTSNode]:\n",
        "        path = self._select(node)\n",
        "\n",
        "\n",
        "        if not self._is_terminal_with_depth_limit(path[-1]):\n",
        "            self._expand(path[-1])\n",
        "            self._simulate(path)\n",
        "\n",
        "\n",
        "        # while not self._is_terminal_with_depth_limit(path[-1]):\n",
        "        #     self._expand(path[-1])\n",
        "        #     # ### debug mode\n",
        "        #     # if path[-1].parent is not None:\n",
        "        #     #     self._back_propagate(path)\n",
        "        #     if self._is_terminal_with_depth_limit(path[-1]) or len(path[-1].children) == 0:\n",
        "        #         break\n",
        "        #     fast_rewards = [child.fast_reward for child in path[-1].children]\n",
        "        #     node = path[-1].children[self.simulate_choice(fast_rewards)]\n",
        "        #     path.append(node)\n",
        "\n",
        "        cum_reward = self._back_propagate(path)\n",
        "        if self.output_strategy == 'max_iter' and path[-1].is_terminal and cum_reward > self._output_cum_reward:\n",
        "            self._output_cum_reward = cum_reward\n",
        "            self._output_iter = path\n",
        "        if self.output_strategy == 'last_iter':\n",
        "            self._output_cum_reward = cum_reward\n",
        "            self._output_iter = path\n",
        "        if self.output_strategy == 'last_terminal_iter' and path[-1].is_terminal:\n",
        "            self._output_cum_reward = cum_reward\n",
        "            self._output_iter = path\n",
        "        return cum_reward, path\n",
        "\n",
        "    def _is_terminal_with_depth_limit(self, node: MCTSNode):\n",
        "        return node.is_terminal or node.depth >= self.depth_limit\n",
        "\n",
        "    def _select(self, node: MCTSNode) -> list[MCTSNode]:\n",
        "        path = []\n",
        "        while True:\n",
        "            path.append(node)\n",
        "            if node.children is None or len(node.children) == 0 or self._is_terminal_with_depth_limit(node):\n",
        "                return path\n",
        "            node = self._uct_select(node)\n",
        "\n",
        "    def _uct(self, node: MCTSNode) -> float:\n",
        "        return node.Q + self.w_exp * np.sqrt(np.log(len(node.parent.cum_rewards)) / max(1, len(node.cum_rewards)))\n",
        "\n",
        "    def _uct_select(self, node: MCTSNode) -> MCTSNode:\n",
        "        if self.uct_with_fast_reward or all(x.state is not None for x in node.children):\n",
        "            expl = [c for c in node.children if c.fast_reward_details['intuition']!=100.0] # expl = [c for c in node.children]\n",
        "            if any([len(c.cum_rewards)>0 for c in expl]):\n",
        "                # return max([c for c in node.children if c.fast_reward_details['intuition']==100.0], key=self._uct)\n",
        "                return max([c for c in node.children], key=self._uct)\n",
        "            else:\n",
        "                return max([c for c in node.children if c.fast_reward_details['intuition']==100.0 or len(c.cum_rewards)==0], key=self._uct)\n",
        "        else:\n",
        "            unvisited_children = filter(lambda x: x.state is None, node.children)\n",
        "            return max(unvisited_children, key=lambda x: x.fast_reward)\n",
        "\n",
        "    def _expand(self, node: MCTSNode):\n",
        "\n",
        "        if node.state is None:\n",
        "            node.state = self.world_model.step(node.parent.state, node.action)\n",
        "            # reward is calculated after the state is updated, so that the\n",
        "            # information can be cached and passed from the world model\n",
        "            # to the reward function with **aux without repetitive computation\n",
        "            node.reward, node.reward_details = self.search_config. \\\n",
        "                reward(node.parent.state, node.action, **node.fast_reward_details)\n",
        "            node.is_terminal = self.world_model.is_terminal(node.state)\n",
        "\n",
        "        if node.is_terminal:\n",
        "            return\n",
        "\n",
        "        # print(f'Step {node.state.step_idx + 1}: ')\n",
        "        children = []\n",
        "        actions = self.search_config.get_actions(node.state)\n",
        "        for action in actions:\n",
        "            fast_reward, fast_reward_details = action[1], {'intuition': action[1]}\n",
        "            # print(action[0])\n",
        "            # print(fast_reward)\n",
        "            child = MCTSNode(state=None, action=action[0], parent=node,\n",
        "                             fast_reward=fast_reward, fast_reward_details=fast_reward_details, calc_q=self.calc_q)\n",
        "            children.append(child)\n",
        "        # print()\n",
        "\n",
        "        node.children = children\n",
        "\n",
        "    def _simulate(self, path: list[MCTSNode]):\n",
        "        node = path[-1]\n",
        "        while True:\n",
        "            if node.state is None:\n",
        "                self._expand(node)\n",
        "            if self._is_terminal_with_depth_limit(node) or len(node.children) == 0:\n",
        "                return\n",
        "            fast_rewards = [child.fast_reward for child in node.children]\n",
        "            node = node.children[self.simulate_choice(fast_rewards)]\n",
        "            path.append(node)\n",
        "\n",
        "    def _back_propagate(self, path: list[MCTSNode]):\n",
        "        rewards = []\n",
        "        cum_reward = -math.inf\n",
        "        for node in reversed(path):\n",
        "            rewards.append(node.reward)\n",
        "            cum_reward = self.cum_reward(rewards[::-1])\n",
        "            node.cum_rewards.append(cum_reward)\n",
        "        return cum_reward\n",
        "\n",
        "    def _dfs_max_reward(self, path: list[MCTSNode]) -> tuple[float, list[MCTSNode]]:\n",
        "        cur = path[-1]\n",
        "        if cur.is_terminal:\n",
        "            return self.cum_reward([node.reward for node in path[1:]]), path\n",
        "        if cur.children is None:\n",
        "            return -math.inf, path\n",
        "        visited_children = [x for x in cur.children if x.state is not None]\n",
        "        if len(visited_children) == 0:\n",
        "            return -math.inf, path\n",
        "        return max((self._dfs_max_reward(path + [child]) for child in visited_children), key=lambda x: x[0])\n",
        "\n",
        "\n",
        "    def _dfs_reward(self, path: list[MCTSNode]) -> tuple[float, list[MCTSNode]]:\n",
        "        cur = path[-1]\n",
        "        if cur.is_terminal:\n",
        "            return (self.cum_reward([node.reward for node in path[1:]]), path)\n",
        "        if cur.children is None:\n",
        "            return (-math.inf, path)\n",
        "        visited_children = [x for x in cur.children if x.state is not None]\n",
        "        if len(visited_children) == 0:\n",
        "            return (-math.inf, path)\n",
        "        return [self._dfs_max_reward(path + [child]) for child in visited_children]\n",
        "\n",
        "    def _dfs_min_reward(self, path: list[MCTSNode]) -> tuple[float, list[MCTSNode]]:\n",
        "        cur = path[-1]\n",
        "        if cur.is_terminal:\n",
        "            return self.cum_reward([node.reward for node in path[1:]]), path\n",
        "        if cur.children is None:\n",
        "            return -math.inf, path\n",
        "        visited_children = [x for x in cur.children if x.state is not None]\n",
        "        if len(visited_children) == 0:\n",
        "            return -math.inf, path\n",
        "        return min((self._dfs_max_reward(path + [child]) for child in visited_children), key=lambda x: x[0])\n",
        "\n",
        "    def search(self):\n",
        "        self._output_cum_reward = -math.inf\n",
        "        self._output_iter = None\n",
        "        self.root = MCTSNode(state=self.world_model.init_state(), action=None, parent=None, calc_q=self.calc_q)\n",
        "        if self.output_trace_in_each_iter:\n",
        "            self.trace_in_each_iter = []\n",
        "\n",
        "        for _ in trange(self.n_iters, disable=self.disable_tqdm, desc='MCTS iteration', leave=False):\n",
        "            cum_reward, path = self.iterate(self.root)\n",
        "            if self.output_trace_in_each_iter:\n",
        "                # self.trace_in_each_iter.append(deepcopy(path))\n",
        "                self.trace_in_each_iter.append(deepcopy((cum_reward, path)))\n",
        "\n",
        "        if self.output_strategy == 'follow_max':\n",
        "            self._output_iter = []\n",
        "            cur = self.root\n",
        "            while True:\n",
        "                self._output_iter.append(cur)\n",
        "                if cur.is_terminal:\n",
        "                    break\n",
        "                visited_children = [x for x in cur.children if x.state is not None]\n",
        "                if len(visited_children) == 0:\n",
        "                    break\n",
        "                cur = max(visited_children, key=lambda x: x.reward)\n",
        "            self._output_cum_reward = self.cum_reward([node.reward for node in self._output_iter[1::-1]])\n",
        "        if self.output_strategy == 'max_reward':\n",
        "            self._output_cum_reward, self._output_iter = self._dfs_max_reward([self.root])\n",
        "            self._output_cum_reward_worst, self._output_iter_worst = self._dfs_min_reward([self.root])\n",
        "            # self._output_iter_all = self._dfs_reward([self.root])\n",
        "\n",
        "\n",
        "            if self._output_cum_reward == -math.inf:\n",
        "                self._output_iter = None\n",
        "\n",
        "            if self._output_cum_reward_worst == -math.inf:\n",
        "                self._output_iter_worst = None\n",
        "\n",
        "    def __call__(self,\n",
        "                 world_model: WorldModel[State, Action, Example],\n",
        "                 search_config: SearchConfig[State, Action, Example],\n",
        "                 log_file: Optional[str] = None,\n",
        "                 **kwargs) -> MCTSResult:\n",
        "        MCTSNode.reset_id()\n",
        "        self.world_model = world_model\n",
        "        self.search_config = search_config\n",
        "\n",
        "        self.search()\n",
        "\n",
        "        if self._output_iter_worst is None:\n",
        "            terminal_state_worst = trace_worst = None\n",
        "        else:\n",
        "            terminal_state_worst = self._output_iter_worst[-1].state\n",
        "            trace_worst = [node.state for node in self._output_iter_worst], [node.action[0] for node in self._output_iter_worst[1:]]\n",
        "\n",
        "        if self._output_iter is None:\n",
        "            terminal_state = trace = None\n",
        "        else:\n",
        "            terminal_state = self._output_iter[-1].state\n",
        "            trace = [node.state for node in self._output_iter], [node.action[0] for node in self._output_iter[1:]]\n",
        "\n",
        "        if self.output_trace_in_each_iter:\n",
        "            trace_in_each_iter = self.trace_in_each_iter\n",
        "            tree_state_after_each_iter = [trace[0] for trace in trace_in_each_iter]\n",
        "        else:\n",
        "            trace_in_each_iter = tree_state_after_each_iter = None\n",
        "        result = MCTSResult(terminal_state=terminal_state,\n",
        "                            cum_reward=self._output_cum_reward,\n",
        "                            trace=(self._output_cum_reward, trace),\n",
        "                            trace_worst=(self._output_cum_reward_worst, trace_worst),\n",
        "                            # trace_all=self._output_iter_all,\n",
        "                            trace_of_nodes=self._output_iter,\n",
        "                            tree_state=self.root,\n",
        "                            trace_in_each_iter=trace_in_each_iter,\n",
        "                            tree_state_after_each_iter=tree_state_after_each_iter)\n",
        "        if self.aggregator is not None:\n",
        "            result = MCTSResult(\n",
        "                terminal_state=result.terminal_state,\n",
        "                cum_reward=result.cum_reward,\n",
        "                trace=result.trace,\n",
        "                trace_worst=result.trace_worst,\n",
        "                # trace_all=result._output_iter_all,\n",
        "                trace_of_nodes=result.trace_of_nodes,\n",
        "                tree_state=result.tree_state,\n",
        "                trace_in_each_iter=result.trace_in_each_iter,\n",
        "                tree_state_after_each_iter=result.tree_state_after_each_iter,\n",
        "                aggregated_result=self.aggregator(result.tree_state),\n",
        "            )\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_T1O1TC1WcQ",
        "outputId": "bbfeb8db-4392-48d8-f49f-98cc59a1c884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q = pd.read_csv('/content/drive/MyDrive/questions.csv')"
      ],
      "metadata": {
        "id": "9_ZelEiA1wnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI()\n",
        "def gpt(prompt):\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "          {\"role\": \"user\", \"content\": input}\n",
        "      ]\n",
        "  )\n"
      ],
      "metadata": {
        "id": "DZbnxIPQ2WJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "elec2324 = pd.read_csv('/content/drive/MyDrive/elec2.csv')\n",
        "po = pd.read_csv('/content/drive/MyDrive/populate.csv')\n",
        "building = pd.read_csv('/content/drive/MyDrive/building2.csv')\n",
        "gas = pd.read_csv('/content/drive/MyDrive/gas2.csv')\n",
        "building = building.drop(columns=['Unnamed: 0'])\n",
        "gas = gas.rename(columns={\"PNU\": \"pnu\"})\n"
      ],
      "metadata": {
        "id": "u_E-MyYC-X0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gas #2720010100012170\n",
        "# building #277202502104091\n",
        "# elec2324 #2720010100012170\n",
        "# elec2324[\"법정동코드PNU\"] = elec2324[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "nptLvaEE-rk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "building[\"pnu\"] = building[\"pnu\"].astype(str)\n",
        "building[\"법정동코드PNU\"] = building[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "09SQLcpQGrb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elec2324[\"pnu\"] = elec2324[\"pnu\"].astype(str)\n",
        "elec2324[\"법정동코드PNU\"] = elec2324[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "_7XhDIh0--RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gas[\"pnu\"] = gas[\"pnu\"].astype(str)\n",
        "gas[\"법정동코드PNU\"] = gas[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "6evk9l8WHPey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "po[\"pnu\"] = po[\"pnu\"].astype(str)\n",
        "po[\"법정동코드PNU\"] = po[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "iwXwJKA1IMYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vDOpsuab8qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "building.to_csv('/content/drive/MyDrive/new/building3.csv')\n",
        "elec2324.to_csv('/content/drive/MyDrive/new/elec3.csv')\n",
        "gas.to_csv('/content/drive/MyDrive/new/gas3.csv')\n",
        "po.to_csv('/content/drive/MyDrive/new/po3.csv')\n"
      ],
      "metadata": {
        "id": "FEWwnUucb-z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(po.head())\n",
        "print(elec2324.head())\n",
        "print(building.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqrp8C_aHVsi",
        "outputId": "1369e7f1-8634-4dd8-ea16-d2a6de0ac904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            지번주소 2023년_총인구수 2023년_세대수  2023년_세대당 인구 2023년_남자 인구수 2023년_여자 인구수  \\\n",
            "0       대구광역시 중구     81,015    41,163          1.97       38,868       42,147   \n",
            "1   대구광역시 중구 동인동      7,891     4,809          1.64        3,940        3,951   \n",
            "2   대구광역시 중구 삼덕동      6,395     4,093          1.56        3,050        3,345   \n",
            "3  대구광역시 중구 성내1동      4,816     3,425          1.41        2,313        2,503   \n",
            "4  대구광역시 중구 성내2동      4,789     3,001          1.60        2,416        2,373   \n",
            "\n",
            "  2024년_총인구수 2024년_세대수  2024년_세대당 인구 2024년_남자 인구수 2024년_여자 인구수         pnu  \\\n",
            "0     89,685    45,209          1.98       42,950       46,735      271100   \n",
            "1      8,401     5,092          1.65        4,182        4,219  2711051700   \n",
            "2      6,678     4,177          1.60        3,243        3,435  2711054500   \n",
            "3      4,829     3,538          1.36        2,308        2,521  2711056500   \n",
            "4      6,389     3,751          1.70        3,188        3,201  2711057500   \n",
            "\n",
            "     법정동코드PNU  \n",
            "0      271100  \n",
            "1  2711051700  \n",
            "2  2711054500  \n",
            "3  2711056500  \n",
            "4  2711057500  \n",
            "                  지번주소    사용년월                          도로명주소   전기사용량  \\\n",
            "0  대구광역시 남구 이천동 121-70  202401  대구광역시 남구 명덕로68길 지상  79 (이천동)   180038   \n",
            "1  대구광역시 남구 이천동 121-70  202402  대구광역시 남구 명덕로68길 지상  77 (이천동)   181483   \n",
            "2  대구광역시 남구 이천동 121-70  202403  대구광역시 남구 명덕로68길 지상  77 (이천동)   163726   \n",
            "3  대구광역시 남구 이천동 121-70  202404  대구광역시 남구 명덕로68길 지상  79 (이천동)   165443   \n",
            "4  대구광역시 남구 이천동 121-70  202405  대구광역시 남구 명덕로68길 지상  77 (이천동)   151984   \n",
            "\n",
            "                pnu  \n",
            "0  2720010100012170  \n",
            "1  2720010100012170  \n",
            "2  2720010100012170  \n",
            "3  2720010100012170  \n",
            "4  2720010100012170  \n",
            "               pnu                       지번주소                           도로명주소  \\\n",
            "0  277202502104091  대구광역시 군위군 군위읍 동부리 409-1번지   대구광역시 군위군 동서5길 5-10 (군위읍 동부리)   \n",
            "1   27720250220221   대구광역시 군위군 군위읍 서부리 22-1번지                             NaN   \n",
            "2  277202502201960    대구광역시 군위군 군위읍 서부리 196번지  대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)   \n",
            "3  277202502202810    대구광역시 군위군 군위읍 서부리 281번지   대구광역시 군위군 중앙3길 5-13 (군위읍 서부리)   \n",
            "4  277202502203954  대구광역시 군위군 군위읍 서부리 395-4번지     대구광역시 군위군 중앙4길 19 (군위읍 서부리)   \n",
            "\n",
            "   지상층수  지하층수     사용승인일       착공일      면적         용도    법정동코드PNU  \n",
            "0     1     0  19921123  19920629   22.68       단독주택  2772025021  \n",
            "1     3     0  19901006       NaN  189.72  제1종근린생활시설  2772025022  \n",
            "2     1     0  19871022       NaN   95.18       단독주택  2772025022  \n",
            "3     1     0  1965           NaN   56.80       단독주택  2772025022  \n",
            "4     2     0  20040413  20040108  198.90      노유자시설  2772025022  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#시작\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "from sqlalchemy.orm import Session\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite:///database.db\")  # 또는 \"postgresql://user:password@host/dbname\"\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    elec2324.to_sql(\"electricity\", con=conn, if_exists=\"replace\", index=False)\n",
        "    building.to_sql(\"building\", con=conn, if_exists=\"replace\", index=False)\n",
        "    po.to_sql(\"population\", con=conn, if_exists=\"replace\", index=False)\n",
        "    gas.to_sql(\"gas\", con=conn, if_exists=\"replace\", index=False)"
      ],
      "metadata": {
        "id": "-Nfi3EIc-UO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install func_timeout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Qfxb9EETe6",
        "outputId": "65c8921a-6dd8-4659-c00e-c90b1a3f21d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting func_timeout\n",
            "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: func_timeout\n",
            "  Building wheel for func_timeout (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for func_timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15077 sha256=58d80359e5f604c59c63fe8145821131918502ac7ccb2144fc26242297124d04\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/e6/86/f23164d12c3134966614102db8e7956ab359faf7ffd78703ce\n",
            "Successfully built func_timeout\n",
            "Installing collected packages: func_timeout\n",
            "Successfully installed func_timeout-4.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python preprocess_data.py --dataset bird --mode train --LLM_model meta-llama/Meta-Llama-3-8B-Instruct --PSG --data_path /data/vda/dataset --output_path ./dataset\n",
        "#\n",
        "# -*- coding: utf-8 -*-\n",
        "# --dataset bird --mode test --LLM_model gpt --PSG --data_path /content/drive/MyDrive/questions.csv --output_path\n",
        "import argparse\n",
        "import copy\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import sqlite3\n",
        "import traceback\n",
        "import os\n",
        "# from vllm import LLM, SamplingParams\n",
        "from func_timeout import func_set_timeout\n",
        "import func_timeout\n",
        "import tqdm\n",
        "\n",
        "prompt_cw_temp_sft = \"\"\"Given the following database schema and question, your task is to write a valid SQL query whose execution will accurately answer the question. If the value below the incomplete SQL query is not empty, your task is to complete it into a full SQL query. Remember to end the query with a semicolom ```;```.\n",
        "\n",
        "Database schema:\n",
        "{ds}\n",
        "\n",
        "Sample rows of each table:\n",
        "{sr}\n",
        "\n",
        "Question:\n",
        "{qs}{hint}\n",
        "\n",
        "Question hint:\n",
        "{sql}\n",
        "\n",
        "The incomplete SQL query:\n",
        "{sql}\n",
        "\n",
        "Answer the question by a SQL query only with no explanation:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
        "            data = json.load(file)\n",
        "            return data\n",
        "    except Exception as e:\n",
        "        print(\"=\" * 10, e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# class LLM_Model(object):\n",
        "#     def __init__(self, model=''):\n",
        "\n",
        "#         self.model = model\n",
        "#         model = model.lower().replace('_', '').replace('-', '')\n",
        "#         if 'qwen2' in model:\n",
        "#             self.tag = 'qwen2'\n",
        "#         elif 'llama3' in model:\n",
        "#             self.tag = 'llama3'\n",
        "#         elif 'llama2' in model:\n",
        "#             self.tag = 'llam2'\n",
        "#         elif 'deepseek' in model:\n",
        "#             self.tag = 'deepseek'\n",
        "#         elif 'mistral' in model:\n",
        "#             self.tag = 'mistral'\n",
        "#         elif 'codellama' in model:\n",
        "#             self.tag = 'codellama'\n",
        "#         elif 'gpt' in model:\n",
        "#             self.tag = 'gpt'\n",
        "#         else:\n",
        "#             raise TypeError(f\"Unexpect model: {model}.\")\n",
        "\n",
        "#         self.llm = LLM(model=self.model,\n",
        "#                        seed=123,\n",
        "#                        tensor_parallel_size=args.gpus,\n",
        "#                        trust_remote_code=True,\n",
        "#                        gpu_memory_utilization=0.9\n",
        "#                        )\n",
        "#         self.tokenizer = self.llm.get_tokenizer()\n",
        "#         self.llm = client\n",
        "\n",
        "#     def generate_response(self, prompts, max_tokens=1024, temperature=0.01, top_p=0.5):\n",
        "#         sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens,\n",
        "#                                          skip_special_tokens=True, stop=self.tokenizer.eos_token)\n",
        "#         if self.tag in ['mistral']:\n",
        "#             messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "#         else:\n",
        "#             messages_list = [\n",
        "#                 [{\"role\": \"system\", \"content\": \"You are a helpful SQLite assistant.\"}, {\"role\": \"user\", \"content\": p}]\n",
        "#                 for p in prompts]\n",
        "#         messages_list = self.tokenizer.apply_chat_template(messages_list, add_generation_prompt=True, tokenize=False)\n",
        "#         outputs = self.llm.generate(messages_list, sampling_params)\n",
        "#         return [output.outputs[0].text for output in outputs]\n",
        "\n",
        "class LLM_Model(object):\n",
        "    def __init__(self, model=''):\n",
        "\n",
        "        self.model = model\n",
        "        self.llm = client\n",
        "\n",
        "    def generate_response(self, prompts, max_tokens=1024, temperature=0.01, top_p=0.5):\n",
        "\n",
        "        if self.tag in ['mistral']:\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "        else:\n",
        "            messages_list = [\n",
        "                [{\"role\": \"system\", \"content\": \"You are a helpful SQLite assistant.\"}, {\"role\": \"user\", \"content\": p}]\n",
        "                for p in prompts]\n",
        "\n",
        "\n",
        "        outputs = self.chat.completions.create(model=\"gpt-4o-mini\",messages= messages_list)\n",
        "        return  [output.choices[0].message.content for output in outputs]\n",
        "\n",
        "class LLM_Online(object):\n",
        "    def __init__(self, model=\"qwen72b\", device=[0]):\n",
        "        None\n",
        "\n",
        "    def generate_response(self, prompts):\n",
        "        rs = []\n",
        "        for prompt in tqdm.tqdm(prompts):\n",
        "            res = None  # your online LLM\n",
        "            rs.append(res)\n",
        "        return rs\n",
        "\n",
        "\n",
        "def parse_dataset(data_path, mode='dev', dataset='bird'):\n",
        "    # redirect path\n",
        "    data_tuples_path = ''\n",
        "    if dataset == 'bird':\n",
        "        # data_tuples_path = os.path.join(data_path, dataset, mode, f'{mode}.json')\n",
        "        data_tuples_path = os.path.join(data_path, dataset, mode, f'{mode}.json')\n",
        "    elif 'spider_DK' == dataset:\n",
        "        data_tuples_path = os.path.join(data_path, 'spider', 'Spider_DK.json')\n",
        "    elif 'spider_real' == dataset:\n",
        "        data_tuples_path = os.path.join(data_path, 'spider', 'spider-realistic.json')\n",
        "\n",
        "    elif 'spider_syn' == dataset:\n",
        "        data_tuples_path = os.path.join(data_path, 'spider', 'dev.json')\n",
        "    elif 'spider' in dataset:\n",
        "        if mode == 'test':\n",
        "            data_tuples_path = os.path.join(data_path, 'spider', 'test.json')\n",
        "        else:\n",
        "            data_tuples_path = os.path.join(data_path, 'spider', f'{mode}.json')\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "    data_tuples = read_json_file(data_tuples_path)\n",
        "\n",
        "    return data_tuples\n",
        "\n",
        "\n",
        "def convert_fk_index(data):\n",
        "    fk_holder = []\n",
        "    table_names_original = [i.lower() for i in data['table_names_original']]  # some bug\n",
        "    column_names_original = [(i[0], i[1].lower()) for i in data['column_names_original']]\n",
        "    for fk in data[\"foreign_keys\"]:\n",
        "        tn, col, ref_tn, ref_col = fk[0][0], fk[0][1], fk[1][0], fk[1][1]\n",
        "        if type(tn) is str:\n",
        "            tn = tn.lower()\n",
        "        if type(col) is str:\n",
        "            col = col.lower()\n",
        "        if type(ref_tn) is str:\n",
        "            ref_tn = ref_tn.lower()\n",
        "        if type(ref_col) is str:\n",
        "            ref_col = ref_col.lower()\n",
        "        ref_cid, cid = None, None\n",
        "        try:\n",
        "            tid = table_names_original.index(tn)\n",
        "            ref_tid = table_names_original.index(ref_tn)\n",
        "            for i, (tab_id, col_org) in enumerate(column_names_original):\n",
        "                if tab_id == ref_tid and ref_col == col_org:\n",
        "                    ref_cid = i\n",
        "                elif tid == tab_id and col == col_org:\n",
        "                    cid = i\n",
        "            if ref_cid and cid:\n",
        "                fk_holder.append([cid, ref_cid])\n",
        "        except:\n",
        "            traceback.print_exc()\n",
        "            print(\"table_names_original: \", table_names_original)\n",
        "            print(\"finding tab name: \", tn, ref_tn)\n",
        "            print(data)\n",
        "            # sys.exit()\n",
        "    return fk_holder\n",
        "\n",
        "\n",
        "def dump_db_json_schema(db, f):\n",
        "    '''read table and column info'''\n",
        "\n",
        "    try:\n",
        "        conn = sqlite3.connect(db)\n",
        "    except:\n",
        "        print(db)\n",
        "        exit()\n",
        "    conn.execute('pragma foreign_keys=ON')\n",
        "    cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "\n",
        "    data = {'db_id': f,\n",
        "            'table_names_original': [],\n",
        "            'table_names': [],\n",
        "            'column_names_original': [(-1, '*')],\n",
        "            'column_names': [(-1, '*')],\n",
        "            'column_types': ['text'],\n",
        "            'primary_keys': [],\n",
        "            'foreign_keys': []}\n",
        "\n",
        "    fk_holder = []\n",
        "    for i, item in enumerate(cursor.fetchall()):\n",
        "        table_name = item[0]\n",
        "        data['table_names_original'].append(table_name)\n",
        "        data['table_names'].append(table_name.lower().replace(\"_\", ' '))\n",
        "        fks = conn.execute(\"PRAGMA foreign_key_list('{}') \".format(table_name)).fetchall()\n",
        "        # print(\"db:{} table:{} fks:{}\".format(f,table_name,fks))\n",
        "        fk_holder.extend([[(table_name, fk[3]), (fk[2], fk[4])] for fk in fks])\n",
        "        cur = conn.execute(\"PRAGMA table_info('{}') \".format(table_name))\n",
        "        for j, col in enumerate(cur.fetchall()):\n",
        "            data['column_names_original'].append((i, col[1]))\n",
        "            data['column_names'].append((i, col[1].lower().replace(\"_\", \" \")))\n",
        "            # varchar, '' -> text, int, numeric -> integer,\n",
        "            col_type = col[2].lower()\n",
        "            if 'char' in col_type or col_type == '' or 'text' in col_type or 'var' in col_type:\n",
        "                data['column_types'].append('text')\n",
        "            elif 'int' in col_type or 'numeric' in col_type or 'decimal' in col_type or 'number' in col_type \\\n",
        "                    or 'id' in col_type or 'real' in col_type or 'double' in col_type or 'float' in col_type:\n",
        "                data['column_types'].append('number')\n",
        "            elif 'date' in col_type or 'time' in col_type or 'year' in col_type:\n",
        "                data['column_types'].append('time')\n",
        "            elif 'boolean' in col_type:\n",
        "                data['column_types'].append('boolean')\n",
        "            else:\n",
        "                data['column_types'].append('others')\n",
        "\n",
        "            if col[5] == 1:\n",
        "                data['primary_keys'].append(len(data['column_names']) - 1)\n",
        "\n",
        "    data[\"foreign_keys\"] = fk_holder\n",
        "    data['foreign_keys'] = convert_fk_index(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_schema_dict(db, kk=3):\n",
        "    \"\"\"\n",
        "    Get database's schema, which is a dict with table name as key\n",
        "    and list of column names as value\n",
        "    :param db: database path\n",
        "    :return: schema dict\n",
        "    \"\"\"\n",
        "    data = dump_db_json_schema(db, db.split('/')[-1])\n",
        "    tables = data['table_names_original']\n",
        "    column_types = data['column_types']\n",
        "    primary_keys = data['primary_keys']\n",
        "    foreign_keys = data['foreign_keys']\n",
        "    column_names = data['column_names_original']\n",
        "\n",
        "    schema_dict = {\n",
        "        'tables': {},\n",
        "        'foreign_keys': []\n",
        "    }\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        t = {}\n",
        "        for j, c in enumerate(column_names):\n",
        "            if c[0] == i:\n",
        "                if j in primary_keys:\n",
        "                    t[c[1]] = [column_types[j].upper(), True]\n",
        "                else:\n",
        "                    t[c[1]] = [column_types[j].upper(), True]\n",
        "        schema_dict['tables'][table] = t\n",
        "\n",
        "    for foreign_key in foreign_keys:\n",
        "        t1 = tables[column_names[foreign_key[0]][0]]\n",
        "        c1 = column_names[foreign_key[0]][1]\n",
        "        t2 = tables[column_names[foreign_key[1]][0]]\n",
        "        c2 = column_names[foreign_key[1]][1]\n",
        "        schema_dict['foreign_keys'].append([t1, c1, t2, c2])\n",
        "\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "    # get exapmles\n",
        "    for table in schema_dict['tables'].keys():\n",
        "        try:\n",
        "            select_query = f'SELECT * FROM `{table}` LIMIT {kk}'\n",
        "            cursor.execute(select_query)\n",
        "            rows = cursor.fetchall()\n",
        "            cursor.execute(f\"PRAGMA table_info(`{table}`);\")\n",
        "            columns = [column[1] for column in cursor.fetchall()]\n",
        "            for i, c in enumerate(columns):\n",
        "                cls_valuse = [f\"{row[i][0:100]}...\" if type(row[i]) is str and len(row[i]) > 100 else row[i] for row in\n",
        "                              rows]\n",
        "                schema_dict['tables'][table][c].append(cls_valuse)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    return schema_dict\n",
        "\n",
        "\n",
        "def get_example_str(schema_dict, k=1):\n",
        "    tables = list(schema_dict['tables'].keys())\n",
        "    examples = {}\n",
        "    for table in tables:\n",
        "        table_dict = schema_dict['tables'][table]\n",
        "        example = []\n",
        "        for cls in table_dict.keys():\n",
        "            example.append(table_dict[cls][2])\n",
        "        example_str = []\n",
        "        for i, v in enumerate(example[0]):\n",
        "            example_str.append(tuple([e[i] for e in example]))\n",
        "            if (i + 1) == k:\n",
        "                break\n",
        "        examples[table] = example_str\n",
        "\n",
        "    e_s = ''\n",
        "    for key in examples.keys():\n",
        "        e_s += f\"{key}: \" + str(examples[key]) + '\\n'\n",
        "\n",
        "    return e_s[:-1]\n",
        "\n",
        "\n",
        "def get_schmea_str_and_examples(schema_dict):\n",
        "    schmea_str = \"\"\n",
        "    tables = list(schema_dict['tables'].keys())\n",
        "    examples = {}\n",
        "    for table in tables:\n",
        "        if ' ' in table:\n",
        "            table_str = f'CREATE TABLE \"{table}\" ('\n",
        "        else:\n",
        "            table_str = f\"CREATE TABLE {table} (\"\n",
        "        table_dict = schema_dict['tables'][table]\n",
        "\n",
        "        pk_str = ''\n",
        "        example = []\n",
        "        for cls in table_dict.keys():\n",
        "            try:\n",
        "                cls_ = f'\"{cls}\"' if ' ' in cls else cls\n",
        "                table_str += f\"{cls_} {table_dict[cls][0]}, \"\n",
        "                if table_dict[cls][1]:\n",
        "                    pk_str += cls_ + ', '\n",
        "                example.append(table_dict[cls][2])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        example_str = []\n",
        "\n",
        "        try:\n",
        "            for i, v in enumerate(example[0]):\n",
        "                example_str.append(tuple([e[i] for e in example]))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "        examples[table] = example_str\n",
        "\n",
        "        if pk_str != '':\n",
        "            table_str += f\"PRIMARY KEY({pk_str[:-2]}), \"\n",
        "\n",
        "        fk_str = ''\n",
        "        for fk in schema_dict['foreign_keys']:\n",
        "            if fk[0] == table and fk[2] in tables:\n",
        "                if fk[3] in schema_dict['tables'][fk[2]].keys():\n",
        "                    fk = [f'\"{f}\"' if ' ' in f else f for f in fk]\n",
        "                    fk_str += f'FOREIGN KEY ({fk[1]}) REFERENCES {fk[2]}({fk[3]}), '\n",
        "        if fk_str != '':\n",
        "            table_str += fk_str\n",
        "\n",
        "        schmea_str += table_str[:-2] + '); '\n",
        "\n",
        "    schmea_str = schmea_str[:-1]\n",
        "\n",
        "    e_s = ''\n",
        "    for key in examples.keys():\n",
        "        e_s += f\"{key}: \" + str(examples[key]) + '\\n'\n",
        "\n",
        "    return schmea_str, e_s[:-1]\n",
        "\n",
        "\n",
        "# parse SQL\n",
        "def parse_sql_from_string(input_string):\n",
        "    input_string = input_string.replace('\\n', ' ').replace('\\t', '')\n",
        "    rs = ''\n",
        "    if '```sql' in input_string:\n",
        "        try:\n",
        "            sql_pattern = r'```sql(.*?)```'\n",
        "            all_sqls = []\n",
        "            for match in re.finditer(sql_pattern, input_string, re.DOTALL):\n",
        "                all_sqls.append(match.group(1).strip())\n",
        "            if all_sqls:\n",
        "                rs = all_sqls[-1]\n",
        "                if 'SELECT' not in rs and len(all_sqls) > 1:\n",
        "                    rs = all_sqls[-2]\n",
        "        except:\n",
        "            None\n",
        "    if 'select' in input_string.lower() and rs == '':\n",
        "        rs = input_string[input_string.find('SELECT'):]\n",
        "    if ';' in rs:  # end\n",
        "        rs = rs[:input_string.find(';') + 1]\n",
        "    if rs == '':\n",
        "        rs = 'SELECT xx FROM xx'\n",
        "    return replace_multiple_spaces(rs).replace('```', '')\n",
        "\n",
        "\n",
        "def replace_multiple_spaces(text):\n",
        "    return re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "\n",
        "def filter_dict_by_sql(schema_dict, sql):\n",
        "    schema_dict_ = copy.deepcopy(schema_dict)\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    # tables\n",
        "    for table in keys:\n",
        "        if f'from {table.lower()}' not in sql.lower() and f'join {table.lower()}' not in sql.lower():\n",
        "            schema_dict_['tables'].pop(table, None)\n",
        "    # columns\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    for table in keys:\n",
        "        cls_keys = list(schema_dict_['tables'][table].keys())\n",
        "        cls_keys.sort(key=lambda x: - len(x))\n",
        "        tabel_dict = copy.deepcopy(schema_dict_['tables'][table])\n",
        "        for cls in cls_keys:\n",
        "            if cls.lower() not in sql.lower():\n",
        "                schema_dict_['tables'][table].pop(cls, None)\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            # schema_dict_['tables'][table] = tabel_dict  # for COUNT(*)\n",
        "            for cls in tabel_dict.keys():\n",
        "                if tabel_dict[cls][1] == True:\n",
        "                    schema_dict_['tables'][table][cls] = tabel_dict[cls]\n",
        "\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[0]] = tabel_dict[tabel_dict.keys()[0]]\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[1]] = tabel_dict[tabel_dict.keys()[1]]\n",
        "            # for COUNT(*)\n",
        "\n",
        "    return schema_dict_\n",
        "\n",
        "\n",
        "def filter_dict_by_sl(schema_dict, sql):\n",
        "    schema_dict_ = copy.deepcopy(schema_dict)\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    # tables\n",
        "    for table in keys:\n",
        "        if f'{table.lower()}' not in sql.lower():\n",
        "            schema_dict_['tables'].pop(table, None)\n",
        "    # columns\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    for table in keys:\n",
        "        cls_keys = list(schema_dict_['tables'][table].keys())\n",
        "        cls_keys.sort(key=lambda x: - len(x))\n",
        "        tabel_dict = copy.deepcopy(schema_dict_['tables'][table])\n",
        "        for cls in cls_keys:\n",
        "            if cls.lower() not in sql.lower():\n",
        "                schema_dict_['tables'][table].pop(cls, None)\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            # schema_dict_['tables'][table] = tabel_dict  # for COUNT(*)\n",
        "            for cls in tabel_dict.keys():\n",
        "                if tabel_dict[cls][1] == True:\n",
        "                    schema_dict_['tables'][table][cls] = tabel_dict[cls]\n",
        "\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[0]] = tabel_dict[tabel_dict.keys()[0]]\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[1]] = tabel_dict[tabel_dict.keys()[1]]\n",
        "            # for COUNT(*)\n",
        "\n",
        "    return schema_dict_\n",
        "\n",
        "\n",
        "@func_set_timeout(5)\n",
        "def execute_query_limit(db_path, query):\n",
        "    error = ''\n",
        "    result = None\n",
        "    conn = sqlite3.connect(db_path, timeout=5.0, check_same_thread=False)\n",
        "    cursor = conn.cursor()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    result = cursor.fetchone()\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "    return result, error\n",
        "\n",
        "\n",
        "def execute_query(db_path, query):\n",
        "    try:\n",
        "        result, error = execute_query_limit(db_path, query)\n",
        "    except func_timeout.exceptions.FunctionTimedOut:\n",
        "        error = \"SQL execution timeout\"\n",
        "        print(\"*\" * 30, error, query)\n",
        "        result = None\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        print(\"*\" * 30, error, query)\n",
        "        result = None\n",
        "    return result, error\n",
        "\n",
        "\n",
        "def replace_syn(data1, data2):\n",
        "    for i in range(len(data1)):\n",
        "        if data1[i]['question'] == data2[i]['SpiderQuestion']:\n",
        "            data1[i]['question'] = data2[i]['SpiderSynQuestion']\n",
        "    return data1\n",
        "\n",
        "\n",
        "def eval_all(args):\n",
        "    dataset = args.dataset\n",
        "    mode = args.mode\n",
        "    data_tuples = parse_dataset(args.data_path, mode, dataset)\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    if dataset == 'spider_syn':\n",
        "        data2 = read_json_file(os.path.join(args.data_path, 'spider', f'dev_syn.json'))\n",
        "        data_tuples = replace_syn(data_tuples, data2)\n",
        "        dataset = 'spider'\n",
        "        args.tag += '_syn'\n",
        "\n",
        "    if dataset == 'spider_DK':\n",
        "        args.tag += '_DK'\n",
        "        dataset = 'spider'\n",
        "\n",
        "    if dataset == 'spider_real':\n",
        "        args.tag += '_real'\n",
        "        dataset = 'spider'\n",
        "\n",
        "    if dataset == 'bird':\n",
        "        kk = 5\n",
        "    else:\n",
        "        kk = 10\n",
        "    kkkkk = 1 if dataset == 'bird' else 3\n",
        "\n",
        "    if 'online' in args.tag:\n",
        "        generator = LLM_Online()\n",
        "    else:\n",
        "        generator = LLM_Model(args.LLM_model)\n",
        "    tag = args.tag\n",
        "\n",
        "\n",
        "    # generate SQL\n",
        "    if True:\n",
        "        sql_results = []\n",
        "        data_header = [[\"NLQ\", \"Predict\", \"GOLD\", 'database']]\n",
        "        prompts = []\n",
        "        for index, row in enumerate(data_tuples):\n",
        "            if 'spider' in dataset:\n",
        "                row['SQL'] = row['query']\n",
        "            if 'drspider' in dataset:\n",
        "                row['SQL'] = row['query']\n",
        "\n",
        "            question, db_id = row['question'], row['db_id']\n",
        "            if dataset == 'spider':\n",
        "                if mode == 'test':\n",
        "                    db_path = os.path.join(args.data_path, dataset, 'test_database', db_id, f\"{db_id}.sqlite\")\n",
        "                else:\n",
        "                    db_path = os.path.join(args.data_path, dataset, 'database', db_id, f\"{db_id}.sqlite\")\n",
        "            elif dataset == 'drspider':\n",
        "                db_path = os.path.join(args.data_path, db_id, f\"{db_id}.sqlite\")\n",
        "            elif dataset == 'bird':\n",
        "                db_path = os.path.join(args.data_path, dataset, f'{mode}/{mode}_databases', db_id, f\"{db_id}.sqlite\")\n",
        "            else:\n",
        "                raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "            schema_dict = get_schema_dict(db_path, kk=kk)\n",
        "            database_schema, examples = get_schmea_str_and_examples(schema_dict)\n",
        "            schema_dict_ = schema_dict\n",
        "\n",
        "            if dataset == 'bird':\n",
        "                prompt = [question, schema_dict,\n",
        "                          f\"\\n\\n/* Question hint */\\n{row['evidence']}\" if row['evidence'] != '' else '', schema_dict_]\n",
        "            else:\n",
        "                prompt = [question, schema_dict, '', schema_dict_]\n",
        "            prompts.append([database_schema, str(examples), question, row['SQL'], db_id, prompt, db_path])\n",
        "\n",
        "        n_samples = len(data_tuples)\n",
        "        n_batches = (n_samples - 1) // batch_size + 1\n",
        "\n",
        "        prompts_collection = []\n",
        "        prompts_collection_db = []\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            start = i * batch_size\n",
        "            end = n_samples if i == n_batches - 1 else (i + 1) * batch_size\n",
        "            batch_prompts = prompts[start: end]\n",
        "            schema_dicts = []  # only keep the tables\n",
        "\n",
        "            for j, v in enumerate(batch_prompts):\n",
        "                batch_prompts[j][1] = get_example_str(batch_prompts[j][5][1], kkkkk)\n",
        "\n",
        "            # text-to-sql\n",
        "            final_prompts = [prompt_cw_temp_sft.format(ds=j[0], sr=j[1], qs=j[2], hint=j[5][2], sql='') for j in batch_prompts]\n",
        "            response_strs = generator.generate_response(prompts=final_prompts)\n",
        "\n",
        "            def contains_subquery(sql_query, tables):\n",
        "                sql = sql_query.lower()\n",
        "                select_num = 0\n",
        "                join_num = 0\n",
        "                tmp = sql\n",
        "                while 'select' in tmp:\n",
        "                    tmp = tmp[tmp.find('select') + 6:]\n",
        "                    select_num += 1\n",
        "                tmp = sql\n",
        "                while 'join' in tmp:\n",
        "                    tmp = tmp[tmp.find('select') + 6:]\n",
        "                    join_num += 1\n",
        "                table_num = len([key for key in tables if f\"from {key.lower()}\" in sql or f\"join {key.lower()}\" in sql])\n",
        "                if table_num == 1:\n",
        "                    hard = 1\n",
        "                elif table_num == 2:\n",
        "                    hard = 2\n",
        "                else:\n",
        "                    hard = 3\n",
        "                return hard\n",
        "\n",
        "            nc_idx = []\n",
        "            continue_sqls = []\n",
        "            # noisy correction\n",
        "\n",
        "            for idx, v in enumerate(response_strs):\n",
        "                pre_sql = parse_sql_from_string(response_strs[idx])\n",
        "                ex_flg3 = True if execute_query(batch_prompts[idx][6], pre_sql)[1] == '' else False\n",
        "                hard = contains_subquery(pre_sql, batch_prompts[idx][5][1]['tables'].keys())\n",
        "                if ex_flg3 == False or hard > 2:\n",
        "                    common_sql = 'SELECT '\n",
        "                    continue_sqls.append(common_sql)\n",
        "                    nc_idx.append(idx)\n",
        "\n",
        "            # PSG\n",
        "            if args.PSG:\n",
        "                cl_prompts = []\n",
        "                for j, idx in enumerate(nc_idx):\n",
        "                    v = batch_prompts[idx]\n",
        "                    ds = get_schmea_str_and_examples(v[5][1])[0]\n",
        "                    sr = get_example_str(v[5][1], kkkkk)\n",
        "                    common_sql = continue_sqls[j]\n",
        "                    if args.eval_sft == 1:\n",
        "                        cl_prompts.append(\n",
        "                            prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "                    else:\n",
        "                        cl_prompts.append(prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "\n",
        "                if len(nc_idx) > 0:\n",
        "                    response_strs_ = generator.generate_response(prompts=cl_prompts)\n",
        "                    print(\"%%%%%%%%%%%%%%%%%%\", response_strs_[0])\n",
        "                    for idx, v in enumerate(nc_idx):\n",
        "                        if execute_query(batch_prompts[v][6], parse_sql_from_string(response_strs_[idx]))[\n",
        "                            0] is not None:\n",
        "                            response_strs[v] = response_strs_[idx]\n",
        "\n",
        "            for j, response_str in enumerate(response_strs):\n",
        "                database_schema = batch_prompts[j][0]\n",
        "                question = batch_prompts[j][2]\n",
        "                gt_sql = replace_multiple_spaces(batch_prompts[j][3])\n",
        "                if gt_sql.endswith(\";;\"):\n",
        "                    gt_sql = gt_sql[:-1]\n",
        "\n",
        "                if not gt_sql.endswith(\";\"):\n",
        "                    gt_sql += \";\"\n",
        "\n",
        "                db_id = batch_prompts[j][4]\n",
        "                prompt = final_prompts[j]\n",
        "                print(f\"=={start + j + 1}/{len(data_tuples)}=={db_id}=={tag}==================\")\n",
        "\n",
        "                try:\n",
        "                    if dataset == 'spider':\n",
        "                        if mode == 'test':\n",
        "                            db_path = os.path.join(args.data_path, dataset, 'test_database', db_id, f\"{db_id}.sqlite\")\n",
        "                        else:\n",
        "                            db_path = os.path.join(args.data_path, dataset, 'database', db_id, f\"{db_id}.sqlite\")\n",
        "                    elif dataset == 'bird':\n",
        "                        db_path = os.path.join(args.data_path, dataset, f'{mode}/{mode}_databases', db_id,\n",
        "                                               f\"{db_id}.sqlite\")\n",
        "                    else:\n",
        "                        raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "                    SQL_str = parse_sql_from_string(response_str)\n",
        "                except Exception as e:\n",
        "                    res = f'error: {str(e)}'\n",
        "                    print(res, response_str)\n",
        "\n",
        "                sql_results.append([question, SQL_str, gt_sql, db_id])\n",
        "\n",
        "                if args.PSG:\n",
        "                    if j in nc_idx:\n",
        "                        prompt = prompt_cw_temp_sft.format(ds=batch_prompts[j][0], sr=batch_prompts[j][1], qs=batch_prompts[j][2], hint=batch_prompts[j][5][2], sql=truncate_sql_before_keywords(gt_sql, CLAUSE_KEYWORDS))\n",
        "                        print(prompt)\n",
        "                        print(f\"Ground: {gt_sql}\")\n",
        "                        # input()\n",
        "\n",
        "                        prompt_dict = {\n",
        "                            \"input\": prompt,\n",
        "                            \"target\": gt_sql\n",
        "                        }\n",
        "\n",
        "                        prompts_collection.append(prompt_dict)\n",
        "\n",
        "                else:\n",
        "\n",
        "\n",
        "                    print(prompt)\n",
        "                    print(f\"Ground: {gt_sql}\")\n",
        "\n",
        "                    prompt_dict1 = {\n",
        "                        \"input\": prompt,\n",
        "                        \"db_id\": db_id,\n",
        "                        \"target\": gt_sql\n",
        "                    }\n",
        "\n",
        "                    prompt_dict2 = {\n",
        "                        \"input\": prompt,\n",
        "                        \"target\": gt_sql\n",
        "                    }\n",
        "\n",
        "\n",
        "                    prompts_collection_db.append(prompt_dict1)\n",
        "                    prompts_collection.append(prompt_dict2)\n",
        "\n",
        "\n",
        "        if args.PSG:\n",
        "            filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_{args.flags}_psg.json\")\n",
        "\n",
        "            with open(filename, mode='w',encoding='utf-8') as file:\n",
        "                json.dump(prompts_collection, file, ensure_ascii=False, indent=4)\n",
        "        else:\n",
        "            filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_{args.flags}.json\")\n",
        "\n",
        "            with open(filename, mode='w',encoding='utf-8') as file:\n",
        "                json.dump(prompts_collection, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        if prompts_collection_db:\n",
        "            filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_db_id_{args.flags}.json\")\n",
        "            with open(filename, mode='w', encoding='utf-8') as file:\n",
        "                json.dump(prompts_collection_db, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # from data_process import truncate_sql_before_keywords, CLAUSE_KEYWORDS\n",
        "#     parser = argparse.ArgumentParser(description='SQL')\n",
        "#     parser.add_argument(\"--dataset\", default='bird', type=str)\n",
        "#     parser.add_argument(\"--data_path\", default='/data/vda/dataset', type=str)\n",
        "#     parser.add_argument(\"--output_path\", default='/data/vda/dataset', type=str)\n",
        "#     parser.add_argument(\"--mode\", default='dev', type=str)\n",
        "#     parser.add_argument(\"--PSG\", action='store_true', default=False)\n",
        "#     parser.add_argument(\"--tag\", default='SQL-o1', type=str)\n",
        "#     parser.add_argument(\"--gpus\", default=4, type=int)\n",
        "#     parser.add_argument(\"--eval_sft\", default=1, type=int)\n",
        "#     parser.add_argument(\"--flags\", default='0', type=str)\n",
        "#     # parser.add_argument(\"--LLM_model\", default='meta-llama/Llama-3-8B-Instruct', type=str)\n",
        "#     parser.add_argument(\"--LLM_model\", default='/data/vda/saves/llama3-8b', type=str)\n",
        "#     parser.add_argument(\"--batch_size\", default=32, type=int)\n",
        "#     args = parser.parse_args()\n",
        "#     print(args)\n",
        "#     eval_all(args)"
      ],
      "metadata": {
        "id": "wwCW_r0O1NHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=q['문제'][0]"
      ],
      "metadata": {
        "id": "iOHrIqIuJX5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(q[70:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFoKpVviPzd5",
        "outputId": "22cdc374-ebad-4d13-91a5-2626b422aba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_results = []\n",
        "data_header = [[\"NLQ\", \"Predict\", \"GOLD\", 'database']]\n",
        "prompts = []\n",
        "db_path = '/content/database.db'\n",
        "schema_dict = get_schema_dict(db_path)\n",
        "database_schema, examples = get_schmea_str_and_examples(schema_dict)\n",
        "schema_dict_ = schema_dict\n",
        "\n",
        "prompt = [question, schema_dict, '', schema_dict_]\n",
        "prompts.append([database_schema, str(examples), question, prompt, db_path])\n",
        "prompts"
      ],
      "metadata": {
        "id": "5Pg_8xyUJNBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8effcfd7-1a74-4bc6-a7a0-d0d526fae5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['CREATE TABLE electricity (지번주소 TEXT, 사용년월 NUMBER, 도로명주소 TEXT, 전기사용량 NUMBER, pnu NUMBER, PRIMARY KEY(지번주소, 사용년월, 도로명주소, 전기사용량, pnu)); CREATE TABLE building (pnu TEXT, 지번주소 TEXT, 도로명주소 TEXT, 지상층수 NUMBER, 지하층수 NUMBER, 사용승인일 TEXT, 착공일 TEXT, 면적 NUMBER, 용도 TEXT, 법정동코드PNU TEXT, PRIMARY KEY(pnu, 지번주소, 도로명주소, 지상층수, 지하층수, 사용승인일, 착공일, 면적, 용도, 법정동코드PNU)); CREATE TABLE population (지번주소 TEXT, 2023년_총인구수 TEXT, 2023년_세대수 TEXT, \"2023년_세대당 인구\" NUMBER, \"2023년_남자 인구수\" TEXT, \"2023년_여자 인구수\" TEXT, 2024년_총인구수 TEXT, 2024년_세대수 TEXT, \"2024년_세대당 인구\" NUMBER, \"2024년_남자 인구수\" TEXT, \"2024년_여자 인구수\" TEXT, pnu TEXT, 법정동코드PNU TEXT, PRIMARY KEY(지번주소, 2023년_총인구수, 2023년_세대수, \"2023년_세대당 인구\", \"2023년_남자 인구수\", \"2023년_여자 인구수\", 2024년_총인구수, 2024년_세대수, \"2024년_세대당 인구\", \"2024년_남자 인구수\", \"2024년_여자 인구수\", pnu, 법정동코드PNU)); CREATE TABLE gas (지번주소 TEXT, 사용년월 NUMBER, 도로명주소 TEXT, 가스사용량 NUMBER, pnu TEXT, 법정동코드PNU TEXT, PRIMARY KEY(지번주소, 사용년월, 도로명주소, 가스사용량, pnu, 법정동코드PNU));',\n",
              "  \"electricity: [('대구광역시 남구 이천동 121-70', 202401, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 180038, 2720010100012170), ('대구광역시 남구 이천동 121-70', 202402, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 181483, 2720010100012170), ('대구광역시 남구 이천동 121-70', 202403, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 163726, 2720010100012170)]\\nbuilding: [('277202502104091', '대구광역시 군위군 군위읍 동부리 409-1번지', '대구광역시 군위군 동서5길 5-10 (군위읍 동부리)', 1, 0, '19921123', '19920629', 22.68, '단독주택', '2772025021'), ('27720250220221', '대구광역시 군위군 군위읍 서부리 22-1번지', None, 3, 0, '19901006', None, 189.72, '제1종근린생활시설', '2772025022'), ('277202502201960', '대구광역시 군위군 군위읍 서부리 196번지', '대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)', 1, 0, '19871022', None, 95.18, '단독주택', '2772025022')]\\npopulation: [('대구광역시 중구', '81,015', '41,163', 1.97, '38,868', '42,147', '89,685', '45,209', 1.98, '42,950', '46,735', '271100', '271100'), ('대구광역시 중구 동인동', '7,891', '4,809', 1.64, '3,940', '3,951', '8,401', '5,092', 1.65, '4,182', '4,219', '2711051700', '2711051700'), ('대구광역시 중구 삼덕동', '6,395', '4,093', 1.56, '3,050', '3,345', '6,678', '4,177', 1.6, '3,243', '3,435', '2711054500', '2711054500')]\\ngas: [('대구광역시 남구 이천동 121-70', 202401, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 694234, '2720010100012170', '2720010100'), ('대구광역시 남구 이천동 121-70', 202402, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 779517, '2720010100012170', '2720010100'), ('대구광역시 남구 이천동 121-70', 202403, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 624218, '2720010100012170', '2720010100')]\",\n",
              "  '건물 수, 총 연면적, 에너지 사용량, 인구를 기준으로 동별 종합 에너지 효율 등급을 산정해주세요.',\n",
              "  ['건물 수, 총 연면적, 에너지 사용량, 인구를 기준으로 동별 종합 에너지 효율 등급을 산정해주세요.',\n",
              "   {'tables': {'electricity': {'지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "      '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '도로명주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  77 (이천동) ']],\n",
              "      '전기사용량': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "      'pnu': ['NUMBER',\n",
              "       True,\n",
              "       [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "     'building': {'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['277202502104091', '27720250220221', '277202502201960']],\n",
              "      '지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 군위군 군위읍 동부리 409-1번지',\n",
              "        '대구광역시 군위군 군위읍 서부리 22-1번지',\n",
              "        '대구광역시 군위군 군위읍 서부리 196번지']],\n",
              "      '도로명주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 군위군 동서5길 5-10 (군위읍 동부리)',\n",
              "        None,\n",
              "        '대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)']],\n",
              "      '지상층수': ['NUMBER', True, [1, 3, 1]],\n",
              "      '지하층수': ['NUMBER', True, [0, 0, 0]],\n",
              "      '사용승인일': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "      '착공일': ['TEXT', True, ['19920629', None, None]],\n",
              "      '면적': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "      '용도': ['TEXT', True, ['단독주택', '제1종근린생활시설', '단독주택']],\n",
              "      '법정동코드PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "     'population': {'지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 중구', '대구광역시 중구 동인동', '대구광역시 중구 삼덕동']],\n",
              "      '2023년_총인구수': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "      '2023년_세대수': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "      '2023년_세대당 인구': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "      '2023년_남자 인구수': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "      '2023년_여자 인구수': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "      '2024년_총인구수': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "      '2024년_세대수': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "      '2024년_세대당 인구': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "      '2024년_남자 인구수': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "      '2024년_여자 인구수': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "      'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "      '법정동코드PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "     'gas': {'지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "      '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '도로명주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  79 (이천동) ']],\n",
              "      '가스사용량': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "      'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "      '법정동코드PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "    'foreign_keys': []},\n",
              "   '',\n",
              "   {'tables': {'electricity': {'지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "      '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '도로명주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  77 (이천동) ']],\n",
              "      '전기사용량': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "      'pnu': ['NUMBER',\n",
              "       True,\n",
              "       [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "     'building': {'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['277202502104091', '27720250220221', '277202502201960']],\n",
              "      '지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 군위군 군위읍 동부리 409-1번지',\n",
              "        '대구광역시 군위군 군위읍 서부리 22-1번지',\n",
              "        '대구광역시 군위군 군위읍 서부리 196번지']],\n",
              "      '도로명주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 군위군 동서5길 5-10 (군위읍 동부리)',\n",
              "        None,\n",
              "        '대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)']],\n",
              "      '지상층수': ['NUMBER', True, [1, 3, 1]],\n",
              "      '지하층수': ['NUMBER', True, [0, 0, 0]],\n",
              "      '사용승인일': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "      '착공일': ['TEXT', True, ['19920629', None, None]],\n",
              "      '면적': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "      '용도': ['TEXT', True, ['단독주택', '제1종근린생활시설', '단독주택']],\n",
              "      '법정동코드PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "     'population': {'지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 중구', '대구광역시 중구 동인동', '대구광역시 중구 삼덕동']],\n",
              "      '2023년_총인구수': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "      '2023년_세대수': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "      '2023년_세대당 인구': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "      '2023년_남자 인구수': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "      '2023년_여자 인구수': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "      '2024년_총인구수': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "      '2024년_세대수': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "      '2024년_세대당 인구': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "      '2024년_남자 인구수': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "      '2024년_여자 인구수': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "      'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "      '법정동코드PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "     'gas': {'지번주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "      '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '도로명주소': ['TEXT',\n",
              "       True,\n",
              "       ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "        '대구광역시 남구 명덕로68길 지상  79 (이천동) ']],\n",
              "      '가스사용량': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "      'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "      '법정동코드PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "    'foreign_keys': []}],\n",
              "  '/content/database.db']]"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts[0][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcXuffqic2S3",
        "outputId": "6eda4d58-8662-4a2f-83bc-661f5953b4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['건물 수, 총 연면적, 에너지 사용량, 인구를 기준으로 동별 종합 에너지 효율 등급을 산정해주세요.',\n",
              " {'tables': {'electricity': {'지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "    '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '도로명주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  77 (이천동) ']],\n",
              "    '전기사용량': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "    'pnu': ['NUMBER',\n",
              "     True,\n",
              "     [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "   'building': {'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['277202502104091', '27720250220221', '277202502201960']],\n",
              "    '지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 군위군 군위읍 동부리 409-1번지',\n",
              "      '대구광역시 군위군 군위읍 서부리 22-1번지',\n",
              "      '대구광역시 군위군 군위읍 서부리 196번지']],\n",
              "    '도로명주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 군위군 동서5길 5-10 (군위읍 동부리)',\n",
              "      None,\n",
              "      '대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)']],\n",
              "    '지상층수': ['NUMBER', True, [1, 3, 1]],\n",
              "    '지하층수': ['NUMBER', True, [0, 0, 0]],\n",
              "    '사용승인일': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "    '착공일': ['TEXT', True, ['19920629', None, None]],\n",
              "    '면적': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "    '용도': ['TEXT', True, ['단독주택', '제1종근린생활시설', '단독주택']],\n",
              "    '법정동코드PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "   'population': {'지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 중구', '대구광역시 중구 동인동', '대구광역시 중구 삼덕동']],\n",
              "    '2023년_총인구수': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "    '2023년_세대수': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "    '2023년_세대당 인구': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "    '2023년_남자 인구수': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "    '2023년_여자 인구수': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "    '2024년_총인구수': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "    '2024년_세대수': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "    '2024년_세대당 인구': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "    '2024년_남자 인구수': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "    '2024년_여자 인구수': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "    'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "    '법정동코드PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "   'gas': {'지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "    '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '도로명주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  79 (이천동) ']],\n",
              "    '가스사용량': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "    'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "    '법정동코드PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "  'foreign_keys': []},\n",
              " '',\n",
              " {'tables': {'electricity': {'지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "    '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '도로명주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  77 (이천동) ']],\n",
              "    '전기사용량': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "    'pnu': ['NUMBER',\n",
              "     True,\n",
              "     [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "   'building': {'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['277202502104091', '27720250220221', '277202502201960']],\n",
              "    '지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 군위군 군위읍 동부리 409-1번지',\n",
              "      '대구광역시 군위군 군위읍 서부리 22-1번지',\n",
              "      '대구광역시 군위군 군위읍 서부리 196번지']],\n",
              "    '도로명주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 군위군 동서5길 5-10 (군위읍 동부리)',\n",
              "      None,\n",
              "      '대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)']],\n",
              "    '지상층수': ['NUMBER', True, [1, 3, 1]],\n",
              "    '지하층수': ['NUMBER', True, [0, 0, 0]],\n",
              "    '사용승인일': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "    '착공일': ['TEXT', True, ['19920629', None, None]],\n",
              "    '면적': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "    '용도': ['TEXT', True, ['단독주택', '제1종근린생활시설', '단독주택']],\n",
              "    '법정동코드PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "   'population': {'지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 중구', '대구광역시 중구 동인동', '대구광역시 중구 삼덕동']],\n",
              "    '2023년_총인구수': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "    '2023년_세대수': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "    '2023년_세대당 인구': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "    '2023년_남자 인구수': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "    '2023년_여자 인구수': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "    '2024년_총인구수': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "    '2024년_세대수': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "    '2024년_세대당 인구': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "    '2024년_남자 인구수': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "    '2024년_여자 인구수': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "    'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "    '법정동코드PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "   'gas': {'지번주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70', '대구광역시 남구 이천동 121-70']],\n",
              "    '사용년월': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '도로명주소': ['TEXT',\n",
              "     True,\n",
              "     ['대구광역시 남구 명덕로68길 지상  79 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  77 (이천동) ',\n",
              "      '대구광역시 남구 명덕로68길 지상  79 (이천동) ']],\n",
              "    '가스사용량': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "    'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "    '법정동코드PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "  'foreign_keys': []}]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q = pd.read_csv('/content/drive/MyDrive/questions.csv')\n",
        "q = q[90:]\n",
        "print(len(q))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w32_KUE7SB5c",
        "outputId": "c98289c2-4e69-4ce6-8709-8b8f82ef9b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in enumerate(q['문제']):\n",
        "\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBpa1GPbR45W",
        "outputId": "b2a9bcc2-fc2f-4ae9-f4c0-37a197e178b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대구시에서 인구가 5,000명 이상인 동 중, 2023년 1인당 전기 사용량이 가장 높은 5개 동은 어디인가요?\n",
            "2023년 동별 연면적 총합 대비 도시가스 사용량 비율이 가장 높은 지역은 어디인가요?\n",
            "건축 연도 기준으로 1990년 이전 건물만 필터링했을 때, 대구시 동별 평균 에너지 사용량 분포를 알려주세요.\n",
            "대구시에서 건물 용도와 층수 조합별(예: 주거용+10층 이상)로 평균 전기 사용량을 테이블로 보여주세요.\n",
            "동별 전체 건물 중 상업용 비중이 50% 이상인 지역의 2023년 월별 전기 사용량 추이를 시각화해주세요.\n",
            "대구시에서 에너지 사용량 상위 10%에 해당하는 건축물들의 평균 건축 연도는 언제인가요?\n",
            "대구시 전체에서 건물 용도별 평균 전기 사용량의 분산이 가장 큰 용도는 무엇인가요?\n",
            "2023년 에너지 사용량이 전년 대비 20% 이상 증가한 동의 수는 몇 곳이며, 해당 동들을 리스트업해주세요.\n",
            "대구시 동별로 2023년 전기 사용량 피크(가장 높은 월)는 언제이며, 각 동의 피크값을 함께 보여주세요.\n",
            "수성구에서 인구 대비 건축물 수가 많은 동의 에너지 소비 패턴이 다른 동들과 어떤 차이를 보이나요?\n",
            "2023년 기준, 대구시의 동별 평균 전기 사용량과 평균 도시가스 사용량의 상관계수는 얼마인가요?\n",
            "연면적이 1,000㎡ 이상인 건물 중, 2023년 에너지 소비 효율(사용량/면적)이 가장 낮은 건물은 어디인가요?\n",
            "대구시 전체에서 건축물 층수와 에너지 사용량 사이의 상관관계를 수치로 분석해주세요.\n",
            "대구시 동 중 2023년 여름(6~8월) 전기 사용량 증가율이 가장 높은 상위 10개 동을 시각화해주세요.\n",
            "2023년 기준, 대구시 동별 1인당 지역난방 사용량이 평균보다 높은 곳만 필터링해서 보여주세요.\n",
            "건물 용도, 층수, 에너지 사용량을 기준으로 군집화한 후, 각 군집별 평균 인구 수를 비교해주세요.\n",
            "대구시에서 전기 사용량 상위 5% 건물 중, 동일 동 내 인구수가 평균 이하인 경우는 몇 건인가요?\n",
            "2023년 기준으로 대구시에서 에너지 소비량 상위 동과 하위 동의 평균 건축 연도 차이를 알려주세요.\n",
            "대구시 내 구별로 상업시설의 평균 전기 사용량을 비교한 테이블을 보여주세요.\n",
            "건축물 수, 인구 수, 에너지 소비량 세 항목 기준으로 Z-score를 구해, 표준화된 동별 순위를 계산해주세요.\n",
            "2023년 동별로 도시가스 사용량의 표준편차가 가장 큰 동 5개는 어디인가요?\n",
            "대구시에서 2023년 에너지 사용량이 많은 동이 인구 증가율도 높은 경향이 있는지 분석해주세요.\n",
            "대구시 동 중에서 2023년 1월과 8월의 전기 사용량 차이가 가장 큰 지역은 어디인가요?\n",
            "대구시에서 동별 평균 건축물 층수와 평균 전기 사용량 간의 관계를 시각화해주세요.\n",
            "인구 1인당 건물 면적이 가장 넓은 동의 2023년 에너지 사용량은 평균보다 높나요?\n",
            "대구시 동별로 건축물 수 대비 인구 수 비율이 높은 지역과 에너지 사용량 간의 관계를 분석해주세요.\n",
            "2023년 한 해 동안 에너지 사용량 변동이 가장 일정한(표준편차 가장 낮은) 동은 어디인가요?\n",
            "2023년 대구시에서 전기, 도시가스, 지역난방 3종 모두 사용하는 건물의 평균 에너지 사용량을 분석해주세요.\n",
            "대구시 동별 건축물 용도 다변성과 에너지 사용량 분산의 관계를 통계적으로 분석해주세요.\n",
            "건물 수, 총 연면적, 에너지 사용량, 인구를 기준으로 동별 종합 에너지 효율 등급을 산정해주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    sql_results = []\n",
        "    data_header = [[\"NLQ\", \"Predict\", \"GOLD\", 'database']]\n",
        "    prompts = []\n",
        "    for index, row in enumerate(q['문제']):\n",
        "        # if 'spider' in dataset:\n",
        "        #     row['SQL'] = row['query']\n",
        "        # if 'drspider' in dataset:\n",
        "        #     row['SQL'] = row['query']\n",
        "        print(row)\n",
        "        question, db_id = row, 1\n",
        "        db_path = '/content/database.db'\n",
        "\n",
        "        schema_dict = get_schema_dict(db_path)\n",
        "        database_schema, examples = get_schmea_str_and_examples(schema_dict)\n",
        "        schema_dict_ = schema_dict\n",
        "\n",
        "        prompt = [question, schema_dict, '', schema_dict_]\n",
        "        prompts.append([database_schema, str(examples), question, 'SQL', db_id, prompt, db_path])\n",
        "    batch_size = 2\n",
        "    n_samples = len(q[70:])\n",
        "    n_batches = (n_samples - 1) // batch_size + 1\n",
        "\n",
        "    prompts_collection = []\n",
        "    prompts_collection_db = []\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = n_samples if i == n_batches - 1 else (i + 1) * batch_size\n",
        "        batch_prompts = prompts[start: end]\n",
        "        schema_dicts = []  # only keep the tables\n",
        "\n",
        "        for j, v in enumerate(batch_prompts):\n",
        "            batch_prompts[j][1] = get_example_str(batch_prompts[j][5][1], kkkkk)\n",
        "        # text-to-sql\n",
        "# prompt_cw_temp_sft = Given the following database schema and question, your task is to write a valid SQL query whose execution will accurately answer the question. If the value below the incomplete SQL query is not empty, your task is to complete it into a full SQL query. Remember to end the query with a semicolom ```;```.\n",
        "# Database schema:{ds} Sample rows of each table:{sr} Question:{qs}{hint} Question hint:{sql} The incomplete SQL query:{sql}\n",
        "# Answer the question by a SQL query only with no explanation:\n",
        "        final_prompts = [prompt_cw_temp_sft.format(ds=j[0], sr=j[1], qs=j[2], hint=j[5][2], sql='') for j in batch_prompts]\n",
        "        print(final_prompts)\n",
        "        response_strs = generator.generate_response(prompts=final_prompts)\n",
        "\n",
        "\n",
        "        nc_idx = []\n",
        "        continue_sqls = []\n",
        "        # noisy correction\n",
        "\n",
        "        for idx, v in enumerate(response_strs):\n",
        "            pre_sql = parse_sql_from_string(response_strs[idx])\n",
        "            ex_flg3 = True if execute_query(batch_prompts[idx][6], pre_sql)[1] == '' else False\n",
        "            hard = contains_subquery(pre_sql, batch_prompts[idx][5][1]['tables'].keys())\n",
        "            if ex_flg3 == False or hard > 2:\n",
        "                common_sql = 'SELECT '\n",
        "                continue_sqls.append(common_sql)\n",
        "                nc_idx.append(idx)\n",
        "\n",
        "        # PSG\n",
        "        if args.PSG:\n",
        "            cl_prompts = []\n",
        "            for j, idx in enumerate(nc_idx):\n",
        "                v = batch_prompts[idx]\n",
        "                ds = get_schmea_str_and_examples(v[5][1])[0]\n",
        "                sr = get_example_str(v[5][1], kkkkk)\n",
        "                common_sql = continue_sqls[j]\n",
        "                if args.eval_sft == 1:\n",
        "                    cl_prompts.append(\n",
        "                        prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "                else:\n",
        "                    cl_prompts.append(prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "\n",
        "            if len(nc_idx) > 0:\n",
        "                response_strs_ = generator.generate_response(prompts=cl_prompts)\n",
        "                print(\"%%%%%%%%%%%%%%%%%%\", response_strs_[0])\n",
        "                for idx, v in enumerate(nc_idx):\n",
        "                    if execute_query(batch_prompts[v][6], parse_sql_from_string(response_strs_[idx]))[\n",
        "                        0] is not None:\n",
        "                        response_strs[v] = response_strs_[idx]\n",
        "\n",
        "        for j, response_str in enumerate(response_strs):\n",
        "            database_schema = batch_prompts[j][0]\n",
        "            question = batch_prompts[j][2]\n",
        "            gt_sql = replace_multiple_spaces(batch_prompts[j][3])\n",
        "            if gt_sql.endswith(\";;\"):\n",
        "                gt_sql = gt_sql[:-1]\n",
        "\n",
        "            if not gt_sql.endswith(\";\"):\n",
        "                gt_sql += \";\"\n",
        "\n",
        "            db_id = batch_prompts[j][4]\n",
        "            prompt = final_prompts[j]\n",
        "            print(f\"=={start + j + 1}/{len(data_tuples)}=={db_id}=={tag}==================\")\n",
        "\n",
        "            try:\n",
        "                if dataset == 'spider':\n",
        "                    if mode == 'test':\n",
        "                        db_path = os.path.join(args.data_path, dataset, 'test_database', db_id, f\"{db_id}.sqlite\")\n",
        "                    else:\n",
        "                        db_path = os.path.join(args.data_path, dataset, 'database', db_id, f\"{db_id}.sqlite\")\n",
        "                elif dataset == 'bird':\n",
        "                    db_path = os.path.join(args.data_path, dataset, f'{mode}/{mode}_databases', db_id,\n",
        "                                            f\"{db_id}.sqlite\")\n",
        "                else:\n",
        "                    raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "                SQL_str = parse_sql_from_string(response_str)\n",
        "            except Exception as e:\n",
        "                res = f'error: {str(e)}'\n",
        "                print(res, response_str)\n",
        "\n",
        "            sql_results.append([question, SQL_str, gt_sql, db_id])\n",
        "\n",
        "            print(prompt)\n",
        "            print(f\"Ground: {gt_sql}\")\n",
        "\n",
        "            prompt_dict1 = {\n",
        "                \"input\": prompt,\n",
        "                \"db_id\": db_id,\n",
        "                \"target\": gt_sql\n",
        "            }\n",
        "\n",
        "            prompt_dict2 = {\n",
        "                \"input\": prompt,\n",
        "                \"target\": gt_sql\n",
        "            }\n",
        "\n",
        "\n",
        "            prompts_collection_db.append(prompt_dict1)\n",
        "            prompts_collection.append(prompt_dict2)\n",
        "\n",
        "\n",
        "    filename = os.path.join('', f\"gpt_ninewatt_test_0.json\")\n",
        "\n",
        "    with open(filename, mode='w',encoding='utf-8') as file:\n",
        "        json.dump(prompts_collection, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    if prompts_collection_db:\n",
        "        filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_db_id_{args.flags}.json\")\n",
        "        with open(filename, mode='w', encoding='utf-8') as file:\n",
        "            json.dump(prompts_collection_db, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by7VdMCJPQ65",
        "outputId": "cc338367-5fff-4ddf-cb41-fc0a88f380bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023년 동별로 도시가스 사용량의 표준편차가 가장 큰 동 5개는 어디인가요?\n",
            "대구시에서 2023년 에너지 사용량이 많은 동이 인구 증가율도 높은 경향이 있는지 분석해주세요.\n",
            "대구시 동 중에서 2023년 1월과 8월의 전기 사용량 차이가 가장 큰 지역은 어디인가요?\n",
            "대구시에서 동별 평균 건축물 층수와 평균 전기 사용량 간의 관계를 시각화해주세요.\n",
            "인구 1인당 건물 면적이 가장 넓은 동의 2023년 에너지 사용량은 평균보다 높나요?\n",
            "대구시 동별로 건축물 수 대비 인구 수 비율이 높은 지역과 에너지 사용량 간의 관계를 분석해주세요.\n",
            "2023년 한 해 동안 에너지 사용량 변동이 가장 일정한(표준편차 가장 낮은) 동은 어디인가요?\n",
            "2023년 대구시에서 전기, 도시가스, 지역난방 3종 모두 사용하는 건물의 평균 에너지 사용량을 분석해주세요.\n",
            "대구시 동별 건축물 용도 다변성과 에너지 사용량 분산의 관계를 통계적으로 분석해주세요.\n",
            "건물 수, 총 연면적, 에너지 사용량, 인구를 기준으로 동별 종합 에너지 효율 등급을 산정해주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: /content/gpt_ninewatt_test_0.json load좀\n",
        "\n",
        "data = read_json_file(\"/content/gpt_ninewatt_test_0.json\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGFcugFlSYJU",
        "outputId": "85aa206d-3636-4da9-d744-7f599e1f52a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "u-Fx4ajDKShS",
        "outputId": "71d4476c-9428-4e4d-ec02-08763655fba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"energy_usage: [('대구광역시 남구 이천동 121-70', 202401, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 180038, 2720010100012170), ('대구광역시 남구 이천동 121-70', 202402, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 181483, 2720010100012170), ('대구광역시 남구 이천동 121-70', 202403, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 163726, 2720010100012170)]\\nelectricity: [('대구광역시 남구 이천동 121-70', 202401, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 180038, 2720010100012170), ('대구광역시 남구 이천동 121-70', 202402, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 181483, 2720010100012170), ('대구광역시 남구 이천동 121-70', 202403, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 163726, 2720010100012170)]\\nbuilding: [('277202502104091', '대구광역시 군위군 군위읍 동부리 409-1번지', '대구광역시 군위군 동서5길 5-10 (군위읍 동부리)', 1, 0, '19921123', '19920629', 22.68, '단독주택', '2772025021'), ('27720250220221', '대구광역시 군위군 군위읍 서부리 22-1번지', None, 3, 0, '19901006', None, 189.72, '제1종근린생활시설', '2772025022'), ('277202502201960', '대구광역시 군위군 군위읍 서부리 196번지', '대구광역시 군위군 중앙3길 19-11 (군위읍 서부리)', 1, 0, '19871022', None, 95.18, '단독주택', '2772025022')]\\npopulation: [('대구광역시 중구', '81,015', '41,163', 1.97, '38,868', '42,147', '89,685', '45,209', 1.98, '42,950', '46,735', '271100', '271100'), ('대구광역시 중구 동인동', '7,891', '4,809', 1.64, '3,940', '3,951', '8,401', '5,092', 1.65, '4,182', '4,219', '2711051700', '2711051700'), ('대구광역시 중구 삼덕동', '6,395', '4,093', 1.56, '3,050', '3,345', '6,678', '4,177', 1.6, '3,243', '3,435', '2711054500', '2711054500')]\\ngas: [('대구광역시 남구 이천동 121-70', 202401, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 694234, '2720010100012170', '2720010100'), ('대구광역시 남구 이천동 121-70', 202402, '대구광역시 남구 명덕로68길 지상  77 (이천동) ', 779517, '2720010100012170', '2720010100'), ('대구광역시 남구 이천동 121-70', 202403, '대구광역시 남구 명덕로68길 지상  79 (이천동) ', 624218, '2720010100012170', '2720010100')]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI()\n",
        "# def gpt(prompt):\n",
        "#   response = client.chat.completions.create(\n",
        "#       model=\"gpt-4o-mini\",\n",
        "#       messages=[\n",
        "#           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#           {\"role\": \"user\", \"content\": input}\n",
        "#       ]\n",
        "#   )\n",
        "class LLM_Model(object):\n",
        "    def __init__(self, model='gpt'):\n",
        "\n",
        "        self.model = model\n",
        "        self.llm = client\n",
        "\n",
        "    def generate_response(self, prompts, max_tokens=1024, temperature=0.01, top_p=0.5):\n",
        "        outputs = []\n",
        "        # if self.tag in ['mistral']:\n",
        "            # messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "\n",
        "        messages_list = [\n",
        "                [{\"role\": \"system\", \"content\": \"You are a helpful SQLite assistant.\"}, {\"role\": \"user\", \"content\": p}]\n",
        "                for p in prompts]\n",
        "        for message in messages_list:\n",
        "          output = self.llm.chat.completions.create(model=\"gpt-4o-mini\",messages= message)\n",
        "          outputs.append(output.choices[0].message.content)\n",
        "        return  outputs\n",
        "\n",
        "\n",
        "generator = LLM_Model()\n",
        "# response_strs = generator.generate_response(prompts=final_prompts)\n",
        "final_prompts = ['hello', '1+1=?']\n",
        "generator.generate_response(prompts=final_prompts)\n"
      ],
      "metadata": {
        "id": "x2Hh7ZfDL2yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nohup python _run_explore.py --task_name bird >> result_mcts_0.txt 2>&1 &\n",
        "python validation_results.py --json_path ./mcts_results/bird_mcts_dev.json ( | spider_mcts_dev.json | spider_syn.json | spider_DK.json | spider_real.json | spider_test.json ) --db_root_path ./dataset/bird/dev/dev_databases --num_cpus 1 --diff_json_path ./dataset/bird/dev/dev.json  --output_file  spider_dev.sql (...)"
      ],
      "metadata": {
        "id": "Tsv0DSSu2lOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "from ordered_set import OrderedSet\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "def dump_json(obj, fname, indent=4, mode='w', encoding=\"utf8\", ensure_ascii=False):\n",
        "    if \"b\" in mode:\n",
        "        encoding = None\n",
        "    with open(fname, \"w\", encoding=encoding) as f:\n",
        "        return json.dump(obj, f, indent=indent, ensure_ascii=ensure_ascii)\n",
        "\n",
        "\n",
        "def log_agent(agent, file_path):\n",
        "    save_dict = agent\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    with open(file_path, 'a') as f:\n",
        "        json.dump(save_dict, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Parsing the input of agents, llms and llm context length.')\n",
        "parser.add_argument(\"--task_name\", type=str, help=\"task_name\", default=\"spider\")  # spider\n",
        "parser.add_argument(\"--input_file\", type=str, help=\"Dev file\", default=\"./\")  # spider\n",
        "# parser.add_argument(\"--output_path\", type=str, help=\"Dev file\", default=\"\")  # spider\n",
        "# parser.add_argument(\"--split\", type=int, help=\"split\", default=0)\n",
        "args = parser.parse_args()\n",
        "\n",
        "para_configs = {\n",
        "    \"mcts_iters\": 8,\n",
        "    \"deapth_limit\": 20,\n",
        "    \"explore_rate\": 100,\n",
        "    \"step_topk\": 3,\n",
        "    \"reflect_threshold\": 50.0,\n",
        "    \"reward_alpha\": 0.4\n",
        "}\n",
        "\n",
        "\n",
        "def run_text2sql():\n",
        "\n",
        "\n",
        "    llm_simulate = f'http://localhost:8000/llm'\n",
        "    llm_reward = f'http://localhost:8000/llm'\n",
        "    base_model = {'select': llm_select, 'simulate': llm_simulate, 'reward': llm_reward}\n",
        "\n",
        "    if args.task_name == \"bird\":\n",
        "        file_path = './dataset/SQL-o1_bird_dev_db_id_0.json'\n",
        "\n",
        "\n",
        "    sql_data = json.load(open(file_path))\n",
        "\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0121_bird_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0121_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0130_syn_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0130_real_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0130_DK_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0210_spider_test_db_id_0_0_0_0.json'))\n",
        "    save_sql_data = []\n",
        "    os.makedirs('mcts_results', exist_ok=True)\n",
        "    save_path = os.path.join('mcts_results', f'{args.task_name}_mcts_dev.json')\n",
        "\n",
        "    # os.makedirs(f'/data/vda/mcts', exist_ok=True)\n",
        "    # save_path = f'/data/vda/mcts/result/{args.task_name}/{args.task_name}_mcts_llama3-8b_2.json'\n",
        "\n",
        "    prompt = para_configs.copy()\n",
        "\n",
        "    for row in tqdm(sql_data):\n",
        "        # if \"such multi - national companies as Dupont , HP\" not in row['input'] and continue_flag:\n",
        "        #     continue\n",
        "        # else:\n",
        "        #     continue_flag = False\n",
        "\n",
        "        # print(row['input'])\n",
        "        world_model = AgentWorldModel(base_model=base_model, prompt=prompt, max_steps=prompt['deapth_limit'])\n",
        "        config = AgentConfig(base_model=base_model, prompt=prompt, reward_alpha=prompt['reward_alpha'])\n",
        "        algorithm = MCTS(depth_limit=prompt['deapth_limit'], disable_tqdm=False, output_trace_in_each_iter=True,\n",
        "                         n_iters=prompt['mcts_iters'], w_exp=prompt['explore_rate'], cum_reward=np.mean, calc_q=max)  #\n",
        "        reasoner_rap = Reasoner(world_model=world_model, search_config=config, search_algo=algorithm)\n",
        "        result_rap = reasoner_rap(row)\n",
        "        if row.get('target', \"\"):\n",
        "            row['target'] = row['target'][:-1] if row['target'].endswith(';;') else row['target']\n",
        "\n",
        "        # [(res[-1].state.blocks_state, res[-1].Q) for res in result_rap.trace_in_each_iter]\n",
        "        # print(\"Answer:\\n\", row['target'])\n",
        "        # for o in list(set([res[-1].state.blocks_state for res in result_rap.trace_in_each_iter])):\n",
        "        #     print(o)\n",
        "\n",
        "        # print(result_rap._output_cum_reward)\n",
        "\n",
        "\n",
        "        row['result_mcts'] = list(OrderedSet([( res[0], res[1][-1].state.blocks_state) for res in result_rap.trace_in_each_iter]))\n",
        "        if result_rap.trace_worst[1]:\n",
        "            row['result_mcts_worst'] = [(result_rap.trace_worst[0], result_rap.trace_worst[1][0][-1].blocks_state)]\n",
        "        else:\n",
        "            row['result_mcts_worst'] = ''\n",
        "\n",
        "        if result_rap.trace[1]:\n",
        "            row['result_mcts_best'] = [(result_rap.trace[0], result_rap.trace[1][0][-1].blocks_state)]\n",
        "        else:\n",
        "            row['result_mcts_best'] = ''\n",
        "\n",
        "        save_sql_data.append(copy.deepcopy(row))\n",
        "        dump_json(save_sql_data, save_path, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_text2sql()"
      ],
      "metadata": {
        "id": "i4kySMAp2oEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python validation_results.py\n",
        "--json_path ./mcts_results/bird_mcts_dev.json ( | spider_mcts_dev.json | spider_syn.json | spider_DK.json | spider_real.json | spider_test.json )\n",
        "--db_root_path ./dataset/bird/dev/dev_databases\n",
        "--num_cpus 1\n",
        "--diff_json_path ./dataset/bird/dev/dev.json\n",
        "--output_file  spider_dev.sql (...)"
      ],
      "metadata": {
        "id": "lTpSB84Y5G4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import sqlite3\n",
        "import multiprocessing as mp\n",
        "from func_timeout import func_timeout, FunctionTimedOut\n",
        "\n",
        "\n",
        "def load_json(json_path):\n",
        "    \"\"\"Load JSON file\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "\n",
        "def result_callback(result):\n",
        "    \"\"\"Callback function to store execution results\"\"\"\n",
        "    exec_result.append(result)\n",
        "\n",
        "\n",
        "def execute_sql(predicted_sql, target_sql, db_path):\n",
        "    \"\"\"Execute the predicted SQL and target SQL on the given database\"\"\"\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        cursor.execute(predicted_sql)\n",
        "        predicted_res = cursor.fetchall()\n",
        "\n",
        "        cursor.execute(target_sql)\n",
        "        target_res = cursor.fetchall()\n",
        "\n",
        "        if set(predicted_res) == set(target_res):\n",
        "            return 1  # Correct execution\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"You can only execute one statement at a time.\" in str(e):\n",
        "            return 1  # Allow single-statement constraint errors\n",
        "        return 0  # Execution error\n",
        "\n",
        "    return 0  # Default incorrect execution\n",
        "\n",
        "\n",
        "# def execute_model(predicted_sqls, target_sql, db_path, db_id, idx, meta_time_out, output_file):\n",
        "#     \"\"\"Execute multiple predicted SQLs and check if any of them succeeds\"\"\"\n",
        "#     try:\n",
        "#         for predicted_sql in predicted_sqls:\n",
        "#             res = func_timeout(meta_time_out, execute_sql, args=(predicted_sql, target_sql, db_path))\n",
        "#             if res == 1:  # If any result is correct, return success\n",
        "#                 with open(output_file, 'a', encoding='utf-8') as f:\n",
        "#                     f.write(predicted_sql + '\\t' + db_id + '\\n')\n",
        "#                 return {'sql_idx': idx, 'res': 1}\n",
        "#\n",
        "#     except KeyboardInterrupt:\n",
        "#         sys.exit(0)\n",
        "#     except FunctionTimedOut:\n",
        "#         return {'sql_idx': idx, 'res': 1}  # Timeout is considered correct\n",
        "#     except Exception:\n",
        "#         pass\n",
        "#\n",
        "#     return {'sql_idx': idx, 'res': 0}  # Default to incorrect execution\n",
        "\n",
        "def execute_model(predicted_sqls, target_sql, db_path, db_id, idx, meta_time_out, output_file):\n",
        "    \"\"\"Execute multiple predicted SQLs and check if any of them succeeds\"\"\"\n",
        "    success = False\n",
        "    flag = False\n",
        "    try:\n",
        "        for predicted_sql in predicted_sqls:\n",
        "            res = func_timeout(meta_time_out, execute_sql, args=(predicted_sql, target_sql, db_path))\n",
        "            if res == 1:  # If any result is correct, record success\n",
        "                with open(output_file, 'a', encoding='utf-8') as f:\n",
        "                    f.write(predicted_sql + '\\t' + db_id + '\\n')\n",
        "                success = True\n",
        "                flag = True\n",
        "                break\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "        # sys.exit(0)\n",
        "    except FunctionTimedOut:\n",
        "        success = False\n",
        "        flag = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if not flag and predicted_sqls:\n",
        "        with open(output_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(predicted_sqls[0] + '\\t' + db_id + '\\n')\n",
        "\n",
        "    return {'sql_idx': idx, 'res': 1 if success else 0}\n",
        "\n",
        "import re\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_sql(pre_sql):\n",
        "    return re.sub(r'\\s+', ' ', pre_sql.replace('\\n', ' ').strip())\n",
        "\n",
        "\n",
        "def extract_sql_from_backticks(pre_sql):\n",
        "    try:\n",
        "        sql_statements = re.findall(r'```(.*?)```', pre_sql, re.DOTALL)\n",
        "        if sql_statements:\n",
        "            sql_cleaned = clean_sql(sql_statements[0])\n",
        "            # 如果包含\"sql\"则去除sql关键字后面的内容\n",
        "            if 'sql' in sql_cleaned:\n",
        "                sql_cleaned = sql_cleaned.split('sql', 1)[1]  # 使用1次分割，避免丢失其他信息\n",
        "            return sql_cleaned\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting SQL: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def package_sqls(json_path, db_root_path):\n",
        "    \"\"\"Extract SQLs and database paths from JSON, sorted by result_mcts score in descending order\"\"\"\n",
        "    json_data = load_json(json_path)\n",
        "    # print(len(json_data))\n",
        "    sql_data = []\n",
        "    db_paths = []\n",
        "\n",
        "    for item in json_data:\n",
        "        db_id = item.get(\"db_id\", \"\")\n",
        "        target_sql = item.get(\"target\", \"\")\n",
        "        result_mcts = item.get(\"result_mcts\", [])\n",
        "\n",
        "        result_mcts_sorted = sorted(result_mcts, key=lambda x: x[0], reverse=True) if result_mcts else []\n",
        "\n",
        "        predicted_sqls = [sql_pair[1] for sql_pair in result_mcts_sorted if isinstance(sql_pair, list) and len(sql_pair) == 2]\n",
        "\n",
        "        # new_predicted_sqls = []\n",
        "        # for pre_sql in predicted_sqls:\n",
        "        #     if '`' in pre_sql:\n",
        "        #         sql_cleaned = extract_sql_from_backticks(pre_sql)\n",
        "        #         if not sql_cleaned:\n",
        "        #             sql_cleaned = \"SELECT\"\n",
        "        #     else:\n",
        "        #         sql_cleaned = clean_sql(pre_sql)\n",
        "        #\n",
        "        #     if not sql_cleaned:  #\n",
        "        #         sql_cleaned = \"SELECT\"\n",
        "        #         print(f\"Empty SQL cleaned for input: {pre_sql}\")\n",
        "        #         input()\n",
        "        #\n",
        "        #     new_predicted_sqls.append(sql_cleaned)\n",
        "        #\n",
        "        # #\n",
        "        # # print(new_predicted_sqls)\n",
        "        #\n",
        "        # predicted_sqls = new_predicted_sqls\n",
        "\n",
        "        if not predicted_sqls or not target_sql or not db_id:\n",
        "            continue\n",
        "\n",
        "        db_path = os.path.join(db_root_path, db_id, f\"{db_id}.sqlite\")\n",
        "        sql_data.append((predicted_sqls, target_sql, db_id))\n",
        "        db_paths.append(db_path)\n",
        "\n",
        "    return sql_data, db_paths\n",
        "\n",
        "\n",
        "def run_sqls_parallel(sql_data, db_paths, output_file, num_cpus=1, meta_time_out=30.0):\n",
        "    \"\"\"Execute SQL queries in parallel\"\"\"\n",
        "    pool = mp.Pool(processes=num_cpus)\n",
        "    for i, (predicted_sqls, target_sql, db_id) in enumerate(sql_data):\n",
        "        pool.apply_async(execute_model, args=(predicted_sqls, target_sql, db_paths[i], db_id, i, meta_time_out, output_file),\n",
        "                         callback=result_callback)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def sort_results(list_of_dicts):\n",
        "    \"\"\"Sort execution results by SQL index\"\"\"\n",
        "    return sorted(list_of_dicts, key=lambda x: x['sql_idx'])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--json_path', type=str, default='/data/vda/mcts/bird_mcts_cw_qwen_dev.json', help=\"Path to JSON file containing SQL queries\")  # bird_mcts_plus_cw_dev | bird_mcts_cw_qwen_dev.json\n",
        "    parser.add_argument('--db_root_path', type=str, default='/data/vda/dataset/bird/dev/dev_databases', help=\"Root path of databases\")\n",
        "    parser.add_argument('--num_cpus', type=int, default=1, help=\"Number of CPU cores for parallel execution\")\n",
        "    parser.add_argument('--meta_time_out', type=float, default=30.0, help=\"Timeout per query execution\")\n",
        "    parser.add_argument('--diff_json_path', type=str, default='/data/vda/dataset/bird/dev/dev.json', help=\"Path to JSON file containing difficulty levels\")\n",
        "    parser.add_argument('--output_file', type=str, default='bird_dev_queries_qwen.sql', help=\"File to store successful queries\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    exec_result = []\n",
        "\n",
        "    print(args.json_path)\n",
        "\n",
        "    with open(args.output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    sql_data, db_paths = package_sqls(args.json_path, args.db_root_path)\n",
        "    run_sqls_parallel(sql_data, db_paths, args.output_file, num_cpus=args.num_cpus, meta_time_out=args.meta_time_out)\n",
        "    # print(exec_result)\n",
        "    exec_result = sort_results(exec_result)\n",
        "    print(\"The SQL file has been generated. Please test it using the test suite.\")"
      ],
      "metadata": {
        "id": "naWchKy821jb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}