{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Generic, TypeVar, Union, NamedTuple, Protocol, Optional, runtime_checkable, Tuple\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from transformers import StoppingCriteriaList\n",
        "from datetime import datetime\n",
        "import os, sys, pickle\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "State = TypeVar(\"State\")\n",
        "Action = TypeVar(\"Action\")\n",
        "Example = TypeVar(\"Example\")\n",
        "Trace = tuple[list[State], list[Action]]\n",
        "\n",
        "def create_directory_if_not_exists(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "class GenerateOutput(NamedTuple):\n",
        "    text: list[str]\n",
        "    log_prob: Optional[list[np.ndarray]] = None\n",
        "\n",
        "\n",
        "class LanguageModel(ABC):\n",
        "    @abstractmethod\n",
        "    def generate(self,\n",
        "                 inputs: list[str],\n",
        "                 max_length: Optional[int] = None,\n",
        "                 max_new_tokens: Optional[int] = None,\n",
        "                 do_sample: bool = False,\n",
        "                 temperature: float = 1.0,\n",
        "                 top_k: int = 50,\n",
        "                 top_p: float = 1.0,\n",
        "                 num_return_sequences: int = 1,\n",
        "                 eos_token_id: Union[None, str, int, list[str, int]] = None,\n",
        "                 hide_input: bool = True,\n",
        "                 output_log_probs: bool = False,\n",
        "                 stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
        "                 **kwargs) -> GenerateOutput:\n",
        "        \"\"\"Generate text from a list of prompts.\n",
        "\n",
        "        :param inputs: List of prompts.\n",
        "        :param max_length: Maximum length of the total output (input + generated).\n",
        "        :param max_new_tokens: Maximum length of generated tokens. Override max_length.\n",
        "        :param do_sample: If False, do greedy decoding.\n",
        "        :param temperature: Temperature for sampling.\n",
        "        :param top_k: Top-k for sampling.\n",
        "        :param top_p: Top-p for sampling.\n",
        "        :param num_return_sequences:\n",
        "        :param eos_token_id: Token id for end of sentence. Passed *str* will be translated into token_id.\n",
        "                             Passed *list* will be treated as multiple possible tokens ending the generation.\n",
        "        :param hide_input: If set true, decode only the generated part.\n",
        "        :param output_log_probs: If set true, also output the log_probs of each generated token\n",
        "        :param stopping_criteria:\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_next_token_logits(self,\n",
        "                              prompt: Union[str, list[str]],\n",
        "                              candidates: Union[list[str], list[list[str]]],\n",
        "                              postprocess: Optional[str] = None,\n",
        "                              **kwargs) -> list[np.ndarray]:\n",
        "        \"\"\" TODO: doc\n",
        "\n",
        "        :param prompt:\n",
        "        :param candidates:\n",
        "        :param postprocess: optional, can be 'log_softmax' or 'softmax'. Apply the corresponding function to logits before returning\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_loglikelihood(self,\n",
        "                          prefix: str,\n",
        "                          contents: list[str],\n",
        "                          **kwargs) -> np.ndarray:\n",
        "        \"\"\"Get the log likelihood of the contents given the prefix.\n",
        "\n",
        "        :param prefix: The prefix to be excluded from the log likelihood.\n",
        "        :param contents: The contents to evaluate (must include the prefix).\n",
        "        \"\"\"\n",
        "        ...\n",
        "#------------------\n",
        "    # def _expand(self, node: MCTSNode):\n",
        "\n",
        "    #     if node.state is None:\n",
        "    #         node.state = self.world_model.step(node.parent.state, node.action)\n",
        "    #         # reward is calculated after the state is updated, so that the\n",
        "    #         # information can be cached and passed from the world model\n",
        "    #         # to the reward function with **aux without repetitive computation\n",
        "    #         node.reward, node.reward_details = self.search_config. \\\n",
        "    #             reward(node.parent.state, node.action, **node.fast_reward_details)\n",
        "    #         node.is_terminal = self.world_model.is_terminal(node.state)\n",
        "\n",
        "    #     if node.is_terminal:\n",
        "    #         return\n",
        "\n",
        "    #     # print(f'Step {node.state.step_idx + 1}: ')\n",
        "    #     children = []\n",
        "    #     actions = self.search_config.get_actions(node.state)\n",
        "#-------------------------\n",
        "\n",
        "class WorldModel(ABC, Generic[State, Action, Example]):\n",
        "    def __init__(self) -> None:\n",
        "        self.example = None\n",
        "        self.prompt = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def init_state(self) -> State: ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, state: State, action: Action) -> Union[State, Tuple[State, dict]]:\n",
        "        \"\"\" Returns the next state and optionally an auxiliary data dict\n",
        "\n",
        "        :param state: The current state\n",
        "        :param action: The action to take\n",
        "        :return: The next state and optionally an auxiliary data dict\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_terminal(self, state: State) -> bool: ...\n",
        "\n",
        "    def update_example(self, example: Example, prompt = None) -> None:\n",
        "        if prompt is not None:\n",
        "            self.prompt = prompt\n",
        "        self.example = example\n",
        "\n",
        "class DefaultWorldModel(WorldModel):\n",
        "    # A default implementation of WorldModel that only\n",
        "    # saves the action sequence as the state\n",
        "\n",
        "    def __init__(self, base_model) -> None:\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def init_state(self):\n",
        "        return []\n",
        "\n",
        "    def step(self, state, action):\n",
        "        return state + [action], {}\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        # By default the state is never terminal\n",
        "        return False\n",
        "\n",
        "\n",
        "class SearchConfig(ABC, Generic[State, Action, Example]):\n",
        "    def __init__(self) -> None:\n",
        "        self.example = None\n",
        "        self.prompt = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_actions(self, state: State) -> list[Action]: ...\n",
        "\n",
        "    def fast_reward(self, state: State, action: Action) -> tuple[float, dict]:\n",
        "        return 0, {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def reward(self, state, action, **kwargs) -> tuple[float, dict]: ...\n",
        "\n",
        "    def update_example(self, example: Example, prompt = None) -> None:\n",
        "        if prompt is not None:\n",
        "            self.prompt = prompt\n",
        "        self.example = example\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class AlgorithmOutput(Protocol[State]):\n",
        "    terminal_state: State\n",
        "    trace: Trace\n",
        "\n",
        "\n",
        "class SearchAlgorithm(ABC):\n",
        "    def __init__(self, **kwargs): ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, world_model: WorldModel, search_config: SearchConfig, **kwargs) -> AlgorithmOutput: ...\n",
        "\n",
        "\n",
        "class Reasoner(ABC, Generic[State, Action, Example]):\n",
        "    def __init__(self,\n",
        "                 world_model: WorldModel[State, Action, Example],\n",
        "                 search_config: SearchConfig[State, Action, Example],\n",
        "                 search_algo: SearchAlgorithm) -> None:\n",
        "        self.world_model = world_model\n",
        "        self.search_config = search_config\n",
        "        self.search_algo = search_algo\n",
        "\n",
        "    def __call__(self, example: Example, prompt = None, **kwargs) -> AlgorithmOutput[State]:\n",
        "        self.world_model.update_example(example, prompt=prompt)\n",
        "        self.search_config.update_example(example, prompt=prompt)\n",
        "        return self.search_algo(self.world_model, self.search_config, **kwargs)\n"
      ],
      "metadata": {
        "id": "I1kiRyB5ovfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "import sqlparse\n",
        "import requests\n",
        "import re\n",
        "AgentAction = str\n",
        "\n",
        "CLAUSE_KEYWORDS = ['select', 'from', 'where', 'group by', 'having', 'order by', 'limit', 'intersect', 'union', 'except', 'union all']\n",
        "JOIN_KEYWORDS = ['join', 'on', 'as', 'right join', 'inner join', 'left join']\n",
        "OTHER_KEYWORDS = ['distinct']\n",
        "BIRD_KEYWORDS = ['if', 'else', 'datediff', 'over', 'instr', 'case', 'partition by', 'iif', 'float', 'real', 'when', 'int', 'using', 'timestampdiff', 'then', 'substr', 'cast', 'integer', 'strftime', 'end']\n",
        "WHERE_OPS = ['not', 'between', 'in', 'like', 'is', 'exists', 'not null', 'null']\n",
        "AGG_OPS = ['max', 'min', 'count', 'sum', 'avg']\n",
        "COND_OPS = ['and', 'or']\n",
        "ORDER_OPS = ['desc', 'asc']\n",
        "SQL_KEYWORDS = []\n",
        "SQL_KEYWORDS.extend(CLAUSE_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(JOIN_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(OTHER_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(BIRD_KEYWORDS)\n",
        "SQL_KEYWORDS.extend(WHERE_OPS)\n",
        "SQL_KEYWORDS.extend(AGG_OPS)\n",
        "SQL_KEYWORDS.extend(COND_OPS)\n",
        "SQL_KEYWORDS.extend(ORDER_OPS)\n",
        "SQL_KEYWORDS = [i.upper() for i in SQL_KEYWORDS]\n",
        "\n",
        "class AgentState(NamedTuple):\n",
        "    step_idx: int\n",
        "    last_blocks_state: str\n",
        "    blocks_state: str\n",
        "    buffered_action: AgentAction\n",
        "\n",
        "class AgentWorldModel(WorldModel):\n",
        "    def __init__(self,\n",
        "                 base_model: LanguageModel,\n",
        "                 prompt: dict,\n",
        "                 max_steps: int = 4,\n",
        "                 batch_size: int = 1) -> None:\n",
        "        super().__init__()\n",
        "        self.max_steps = max_steps\n",
        "        self.base_model = base_model\n",
        "        self.prompt = prompt\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def init_state(self) -> AgentState:\n",
        "        return AgentState(step_idx=0,\n",
        "                          last_blocks_state=\"\",\n",
        "                          blocks_state=\"\",\n",
        "                          buffered_action=\"\")\n",
        "\n",
        "    def step(self, state: AgentState, action: AgentAction) -> tuple[AgentState, dict]:\n",
        "        step_idx = state.step_idx\n",
        "        # blocks_state = state.blocks_state + action + (\"; \" if action != \"done\" and action != \"none\" else \"\")\n",
        "\n",
        "        if action == \";\" or action == \" ;\" or action.endswith(\";\"):\n",
        "            # blocks_state = state.blocks_state + (\"\" if state.blocks_state.endswith(\";\") or state.blocks_state.endswith(\"; \") else \"; \") + action\n",
        "            # blocks_state = state.blocks_state + \" \" + action\n",
        "            blocks_state = state.blocks_state + action if not state.blocks_state else state.blocks_state + \" \" + action\n",
        "        else:\n",
        "            blocks_state = state.blocks_state + action if not state.blocks_state else state.blocks_state + \" \" + action\n",
        "\n",
        "        new_buffered_action = action\n",
        "\n",
        "        state = AgentState(step_idx=step_idx + 1,\n",
        "                        last_blocks_state=state.blocks_state,\n",
        "                        blocks_state=blocks_state,\n",
        "                        buffered_action=new_buffered_action)\n",
        "        return state\n",
        "\n",
        "    def is_terminal(self, state: AgentState) -> bool:\n",
        "        if state.buffered_action in [';', ' ;'] or state.buffered_action.endswith(\";\"):\n",
        "            return True\n",
        "        elif state.step_idx == self.max_steps:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "class AgentConfig(SearchConfig):\n",
        "    def __init__(self,\n",
        "                 base_model: LanguageModel,\n",
        "                 prompt: dict,\n",
        "                 batch_size: int = 1,\n",
        "                 reward_alpha: float = 0.5,\n",
        "                 goal_reward_default: float = 0.,\n",
        "                 goal_reached_reward: float = 100.) -> None:\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.example = None\n",
        "        self.prompt = prompt\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_alpha = reward_alpha\n",
        "        self.goal_reward_default = goal_reward_default\n",
        "        self.goal_reached_reward = goal_reached_reward\n",
        "\n",
        "    def lexical(self, query, values):\n",
        "        if isinstance(query, str):\n",
        "            for placeholder, value in values.items():\n",
        "                query = query.replace(placeholder, value)\n",
        "        elif isinstance(query, list):\n",
        "            for i in range(len(query)):\n",
        "                if query[i] in values:\n",
        "                    query[i] = values[query[i]]\n",
        "        return query\n",
        "\n",
        "    def delexical(self, query):\n",
        "        values = {}\n",
        "        new_query = \"\"\n",
        "        in_value = False\n",
        "        in_col = False\n",
        "        value = \"\"\n",
        "        placeholder_id = 0\n",
        "        new_query = \"\"\n",
        "        for char in query:\n",
        "            if char == \"'\":\n",
        "                in_value = not in_value\n",
        "                value += char\n",
        "                if not in_value:\n",
        "                    values[f\"value_{placeholder_id}\"] = value\n",
        "                    new_query += f\"value_{placeholder_id}\"\n",
        "                    placeholder_id += 1\n",
        "                    value = \"\"\n",
        "            else:\n",
        "                if not in_value:\n",
        "                    new_query += char\n",
        "                else:\n",
        "                    value += char\n",
        "        return new_query, values\n",
        "\n",
        "    def format_query(self, q, format_type):\n",
        "        if format_type == 'unnormalized':\n",
        "            return q[\"query\"]\n",
        "        elif format_type == 'normalized':\n",
        "            return q[\"gold\"][\"query_normalized\"]\n",
        "        else:\n",
        "            raise ValueError(f\"format_type {format_type} not supported\")\n",
        "\n",
        "    def _is_whitespace(self, sqlparse_token):\n",
        "        return sqlparse_token.ttype == sqlparse.tokens.Whitespace\n",
        "\n",
        "\n",
        "\n",
        "    def normalize_sql(self, sql_exp):\n",
        "        sql_exp = sql_exp.replace('\"', \"'\")\n",
        "        if sql_exp.count(\n",
        "                \"'\") % 2 != 0:  # odd number of single quotes, meaning the value is incomplete or value contains a single quote\n",
        "            odd_quotes = True\n",
        "        else:\n",
        "            odd_quotes = False\n",
        "\n",
        "        if not odd_quotes:\n",
        "            sql_exp, values = self.delexical(sql_exp)\n",
        "            sql_exp = sql_exp.lower()\n",
        "\n",
        "        sql_exp = sql_exp.rstrip(\";\")\n",
        "        parse = sqlparse.parse(sql_exp)\n",
        "        sql = parse[0]\n",
        "        flat_tokens = sql.flatten()\n",
        "        sql_tokens = [\n",
        "            (token.value.upper() if token.value in SQL_KEYWORDS else token.value)\n",
        "            for token in flat_tokens if not self._is_whitespace(token)\n",
        "        ]\n",
        "\n",
        "        sql_lower = ' '.join(sql_tokens)\n",
        "        sql_lower = sql_lower.replace(' . ', '.')\n",
        "        for op in AGG_OPS:\n",
        "            sql_lower = sql_lower.replace(f\" {op.upper()} (\", f\" {op.upper()}(\")\n",
        "        sql_lower = sql_lower.replace('( ', '(')\n",
        "        sql_lower = sql_lower.replace(' )', ')')\n",
        "        sql_lower = sql_lower.replace(' ,', ',')\n",
        "\n",
        "        ### BIRD-SQL special cases ###\n",
        "        sql_lower = sql_lower.replace(' AS text', ' AS TEXT')\n",
        "        sql_lower = sql_lower.replace(' length(', ' LENGTH(')\n",
        "        sql_lower = sql_lower.replace(' total(', ' TOTAL(')\n",
        "        sql_lower = sql_lower.replace(' round(', ' ROUND(')\n",
        "        ### END ###\n",
        "\n",
        "        sql_lower = sql_lower.rstrip(\";\")\n",
        "        sql_lower += ';'\n",
        "\n",
        "        if not odd_quotes:\n",
        "            # sql_tokens = self.lexical(sql_tokens, values)\n",
        "            sql_lower = self.lexical(sql_lower, values)\n",
        "        # else:\n",
        "        #     print(\"Cannot process the following SQL\")\n",
        "        #     print(sql_exp, sql_tokens)\n",
        "        return sql_lower\n",
        "\n",
        "    def segment_step(self, sql_completion):\n",
        "        try:\n",
        "            parse = sqlparse.parse(sql_completion)\n",
        "            sql = parse[0]\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "        flat_tokens = sql.flatten()\n",
        "        sql_tokens = [\n",
        "            (token.value.upper() if token.value in SQL_KEYWORDS else token.value)\n",
        "            for token in flat_tokens\n",
        "        ]\n",
        "\n",
        "        step_length = 0\n",
        "        for i, token in enumerate(sql_tokens[1:]):\n",
        "            if token.lower() in CLAUSE_KEYWORDS:\n",
        "                step_length = i + 1\n",
        "                break\n",
        "\n",
        "        if step_length == 0:\n",
        "            # No more clauses, the entire completion is a step\n",
        "            return sql_completion\n",
        "        else:\n",
        "            return \"\".join(sql_tokens[:step_length])\n",
        "\n",
        "    def get_actions(self, state: AgentState) -> list[AgentAction]:\n",
        "        if state.step_idx == self.prompt['deapth_limit']-1:\n",
        "            if self.example['target'].startswith(state.blocks_state):\n",
        "                return [('done',100.0)]\n",
        "            else:\n",
        "                return [('done',99.99)]\n",
        "\n",
        "            # if self.example['output'].startswith(state.blocks_state):\n",
        "            #     return [('done',100.0)]\n",
        "            # else:\n",
        "            #     return [('done',99.99)]\n",
        "        else:\n",
        "            # output = requests.post(self.base_model['select'], json={\"instruction\": self.example['instruction'], \"input\": self.example['instruction'] + \"\\n\" +self.example['input']+state.blocks_state, \"output\": [] }).json()\n",
        "            # print(self.example['input'])\n",
        "            print(state.blocks_state)\n",
        "            print(self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state))\n",
        "            # input()\n",
        "            input = self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state)\n",
        "            # output = requests.post(self.base_model['select'], json={ \"input\": self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state), \"output\": [] }).json()\n",
        "            output = self.base_model.chat.completions.create(\n",
        "                  model=\"gpt-4o-mini\",\n",
        "                  messages=[\n",
        "                      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": input}\n",
        "                  ]\n",
        "              )\n",
        "            # def is_valid_string(s):\n",
        "            #     pattern = r'^(\\[[^\\]]+\\]: <[^>]+>)'\n",
        "            #     if \"; \" not in s:\n",
        "            #         return bool(s in ['none','done'])\n",
        "            #     else:\n",
        "            #         if not s.endswith(\"; done\"):\n",
        "            #             return False\n",
        "            #         else:\n",
        "            #             #  and x.split('<')[-1].split('>')[0] in self.example['input']\n",
        "            #             return all([bool(re.match(pattern, x)) for x in s.split(\"; \")[:-1]])\n",
        "\n",
        "            # def is_valid_string(s):\n",
        "            #     if \";\" not in s:\n",
        "            #         if s == \"done\" or s == \" done\":\n",
        "            #             return s\n",
        "            #         return \"\"\n",
        "            #     else:\n",
        "            #         if s == \"; done\" or s == \";done\":\n",
        "            #             return \"; done\"\n",
        "            #         elif s.endswith(\"done\"):\n",
        "            #             return s.split(\"done\")[0]\n",
        "            #         else:\n",
        "            #             return s\n",
        "\n",
        "            # sql_completions = []\n",
        "            # for key in output.keys():\n",
        "            #     key = is_valid_string(key)\n",
        "            #     if key:\n",
        "            #         if key not in [\"done\", \" done\", \"; done\", \";done\"]:\n",
        "            #             sql_completions.append(self.normalize_sql(key))\n",
        "            #         else:\n",
        "            #             sql_completions.append(key)\n",
        "            #     else:\n",
        "            #         continue\n",
        "\n",
        "            def is_valid_string(s):\n",
        "                if \";\" not in s:\n",
        "                    return False\n",
        "                else:\n",
        "                    return True\n",
        "\n",
        "            sql_completions = [key for key in output.keys() if is_valid_string(key)]\n",
        "            # sql_completions = [self.normalize_sql(key) for key in output.keys() if is_valid_string(key)]\n",
        "\n",
        "            actions = set([\n",
        "                (\n",
        "                    self.segment_step(sql[len(state.blocks_state):].lstrip()).rstrip()\n",
        "                    if len(sql) > len(state.blocks_state)\n",
        "                    else sql\n",
        "                )\n",
        "                for sql in sql_completions\n",
        "            ])\n",
        "\n",
        "            actions = list(actions)\n",
        "\n",
        "            # p_reward = requests.post(self.base_model['select'], json={\"input\": self.example['instruction'] + \"\\n\" + self.example['input']+state.blocks_state, \"output\": actions}).json()\n",
        "\n",
        "            p_reward = requests.post(self.base_model['select'], json={\"input\": self.example['input'].replace(\"The incomplete SQL query:\\n\", \"The incomplete SQL query:\\n\" + state.blocks_state), \"output\": actions}).json()\n",
        "            actions_scores_list = [(a,min(r,99.99)) for a,r in zip(actions, p_reward)]\n",
        "            actions_scores_list = sorted(actions_scores_list, key=lambda x: x[1], reverse=True)[:self.prompt['step_topk']]\n",
        "\n",
        "            # if self.example['output'].startswith(state.blocks_state):\n",
        "            #     gt_action = self.example['output'][len(state.blocks_state):]\n",
        "            #     actions_scores_list = [(gt_action, 100.0)]+[(a,r) for a,r in actions_scores_list if a!=gt_action]\n",
        "                # actions_scores_list = [(gt_action, requests.post(self.base_model['select'], json={ \"input\": self.example['input']+state.blocks_state, \"output\": [gt_action] }).json()[0])]+[(a,r) for a,r in actions_scores_list if a!=gt_action]\n",
        "            return actions_scores_list\n",
        "\n",
        "    def fast_reward(self, state: AgentState, action: AgentAction) -> tuple[float, dict]:\n",
        "        intuition = action[1]\n",
        "        self_eval = intuition\n",
        "\n",
        "        return (self.calculate_reward(intuition, self_eval),\n",
        "                {'intuition': intuition, \"self_eval\": self_eval})\n",
        "\n",
        "    def calculate_reward(self, intuition, goal_reached=None) -> float:\n",
        "        # to provide a unified interface for reward and fast_reward\n",
        "        if goal_reached is None:\n",
        "            goal_reward = self.goal_reward_default\n",
        "        elif goal_reached[0]:\n",
        "            goal_reward = goal_reached[1]\n",
        "        else:\n",
        "            goal_reward = goal_reached[1]\n",
        "        return intuition * self.reward_alpha + goal_reward * (1 - self.reward_alpha)\n",
        "\n",
        "    def reward(self, state: AgentState, action: AgentAction,\n",
        "               intuition: float = None) -> tuple[float, dict]:\n",
        "        # if action == \"done\" or action == \"none\" or action == \" done\":\n",
        "        if action.endswith(\";\"):\n",
        "            goal_reached_if = True\n",
        "            # goal_reached_score = requests.post(self.base_model['reward'], json={ \"input\": self.example['instruction'] + \"\\n\" + self.example['input'], \"output\": [state.blocks_state+action]}).json()[0]\n",
        "            goal_reached_score = requests.post(self.base_model['reward'], json={ \"input\":self.example['input'], \"output\": [state.blocks_state+action]}).json()[0]\n",
        "\n",
        "            goal_reached = (goal_reached_if, goal_reached_score)\n",
        "        else:\n",
        "            goal_reached = (False, 0.0)\n",
        "        return (self.calculate_reward(intuition, goal_reached),\n",
        "                {'intuition': intuition, 'goal_reached': goal_reached})\n"
      ],
      "metadata": {
        "id": "9hLzEo8xolh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import Generic, Optional, NamedTuple, Callable, Hashable\n",
        "import itertools\n",
        "from abc import ABC\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "\n",
        "class MCTSNode(Generic[State, Action, Example]):\n",
        "    id_iter = itertools.count()\n",
        "\n",
        "    @classmethod\n",
        "    def reset_id(cls):\n",
        "        cls.id_iter = itertools.count()\n",
        "\n",
        "    def __init__(self, state: Optional[State], action: Optional[Action], parent: \"Optional[MCTSNode]\" = None,\n",
        "                 fast_reward: float = 0., fast_reward_details=None,\n",
        "                 is_terminal: bool = False, calc_q: Callable[[list[float]], float] = np.mean):\n",
        "        \"\"\"\n",
        "        A node in the MCTS search tree\n",
        "\n",
        "        :param state: the current state\n",
        "        :param action: the action of the last step, i.e., the action from parent node to current node\n",
        "        :param parent: the parent node, None if root of the tree\n",
        "        :param fast_reward: an estimation of the reward of the last step\n",
        "        :param is_terminal: whether the current state is a terminal state\n",
        "        :param calc_q: the way to calculate the Q value from histories. Defaults: np.mean\n",
        "        \"\"\"\n",
        "        self.id = next(MCTSNode.id_iter)\n",
        "        if fast_reward_details is None:\n",
        "            fast_reward_details = {}\n",
        "        self.cum_rewards: list[float] = []\n",
        "        self.fast_reward = self.reward = fast_reward\n",
        "        self.fast_reward_details = fast_reward_details\n",
        "        self.is_terminal = is_terminal\n",
        "        self.action = action\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children: 'Optional[list[MCTSNode]]' = None\n",
        "        self.calc_q = calc_q\n",
        "        if parent is None:\n",
        "            self.depth = 0\n",
        "        else:\n",
        "            self.depth = parent.depth + 1\n",
        "\n",
        "    # noinspection PyPep8Naming\n",
        "    @property\n",
        "    def Q(self) -> float:\n",
        "        if self.state is None:\n",
        "            return self.fast_reward\n",
        "        else:\n",
        "            return self.calc_q(self.cum_rewards)\n",
        "\n",
        "\n",
        "class MCTSResult(NamedTuple):\n",
        "    terminal_state: State\n",
        "    cum_reward: float\n",
        "    trace: Trace\n",
        "    trace_worst: Trace\n",
        "    # trace_all: Trace\n",
        "    trace_of_nodes: list[MCTSNode]\n",
        "    tree_state: MCTSNode\n",
        "    trace_in_each_iter: list[list[MCTSNode]] = None\n",
        "    tree_state_after_each_iter: list[MCTSNode] = None\n",
        "    aggregated_result: Optional[Hashable] = None\n",
        "\n",
        "class MCTSAggregation(Generic[State, Action, Example], ABC):\n",
        "    def __init__(self, retrieve_answer: Callable[[State], Hashable],\n",
        "                 weight_policy: str = 'edge'):\n",
        "        assert weight_policy in ['edge', 'edge_inverse_depth', 'uniform']\n",
        "        self.retrieve_answer = retrieve_answer\n",
        "        self.weight_policy = weight_policy\n",
        "\n",
        "    def __call__(self, tree_state: MCTSNode[State, Action,Example]) -> Optional[Hashable]:\n",
        "        answer_dict = defaultdict(lambda: 0)\n",
        "\n",
        "        def visit(cur: MCTSNode[State, Action, Example]):\n",
        "            if cur.state is None:\n",
        "                return []\n",
        "            if cur.is_terminal:\n",
        "                answer = self.retrieve_answer(cur.state)\n",
        "                if answer is None:\n",
        "                    print(\"MCTSAggregation: no answer retrieved.\")\n",
        "                    return []\n",
        "                if self.weight_policy == 'edge':\n",
        "                    answer_dict[answer] += cur.reward\n",
        "                elif self.weight_policy == 'edge_inverse_depth':\n",
        "                    answer_dict[answer] += cur.reward / cur.depth\n",
        "                elif self.weight_policy == 'uniform':\n",
        "                    answer_dict[answer] += 1.0\n",
        "                return [(answer, cur.depth)]\n",
        "            depth_list = defaultdict(list)\n",
        "            cur_list = []\n",
        "            for child in cur.children:\n",
        "                cur_list.extend(child_info := visit(child))\n",
        "                for answer, depth in child_info:\n",
        "                    depth_list[answer].append(depth)\n",
        "            for answer, depths in depth_list.items():\n",
        "                if self.weight_policy == 'edge':\n",
        "                    answer_dict[answer] += cur.reward\n",
        "                elif self.weight_policy == 'edge_inverse_depth':\n",
        "                    answer_dict[answer] += cur.reward / np.mean(depths)\n",
        "            return cur_list\n",
        "\n",
        "        visit(tree_state)\n",
        "\n",
        "        if len(answer_dict) == 0:\n",
        "            return None\n",
        "        return max(answer_dict, key=lambda answer: answer_dict[answer])"
      ],
      "metadata": {
        "id": "v5FVs_v70Ta8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpx1QoUznmsG"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "import sqlparse\n",
        "import requests\n",
        "import re\n",
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import Generic, Optional, NamedTuple, Callable, Hashable\n",
        "import itertools\n",
        "from abc import ABC\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "State = TypeVar(\"State\")\n",
        "Action = TypeVar(\"Action\")\n",
        "Example = TypeVar(\"Example\")\n",
        "Trace = tuple[list[State], list[Action]]\n",
        "\n",
        "class MCTS(SearchAlgorithm, Generic[State, Action, Example]):\n",
        "    def __init__(self,\n",
        "                 output_trace_in_each_iter: bool = False,\n",
        "                 w_exp: float = 1.,\n",
        "                 depth_limit: int = 5,\n",
        "                 n_iters: int = 10,\n",
        "                 cum_reward: Callable[[list[float]], float] = sum,\n",
        "                 calc_q: Callable[[list[float]], float] = np.mean,\n",
        "                 simulate_strategy: str | Callable[[list[float]], int] = 'max',\n",
        "                 output_strategy: str = 'max_reward',\n",
        "                 uct_with_fast_reward: bool = True,\n",
        "                 aggregator: Optional[MCTSAggregation] = None,\n",
        "                 disable_tqdm: bool = True,\n",
        "                 node_visualizer: Callable[[MCTSNode], dict] = lambda x: x.__dict__):\n",
        "        \"\"\"\n",
        "        MCTS algorithm\n",
        "\n",
        "        :param output_trace_in_each_iter: whether to output the trace of the chosen trajectory in each iteration ; the trace is *deepcopy*-ed\n",
        "                                          will also output *tree_state_after_each_iter*, which is the *deepcopy*-ed root\n",
        "        :param w_exp: the weight of exploration in UCT\n",
        "        :param cum_reward: the way to calculate the cumulative reward from each step. Defaults: sum\n",
        "        :param calc_q: the way to calculate the Q value from histories. Defaults: np.mean\n",
        "        :param simulate_strategy: simulate strategy. Options: 'max', 'sample', 'random', or use a custom function\n",
        "        :param output_strategy: the way to output the result. The nodes are not *deepcopy*-ed, so the information is after all iterations\n",
        "                                Options: 'max_reward': dfs on the final tree to find a trajectory with max reward using :param cum_reward:\n",
        "                                         'follow_max': starting from root, choose the maximum reward child at each step. May output a non-terminal node if dead end\n",
        "                                         'max_visit': the terminal node with maximum number of visits\n",
        "                                         'max_iter': the trajectory with a terminal node and max reward among those in each iteration\n",
        "                                         'last_iter': the last trajectory. May output a non-terminal node if the last iteration leads to a dead end\n",
        "                                         'last_terminal_iter': the last trajectory with a terminal node\n",
        "                                Outputs *None* if no trajectory with terminal node but required\n",
        "        :param uct_with_fast_reward: if True, use fast_reward instead of reward for unvisited children in UCT\n",
        "                                     Otherwise, visit the *unvisited* children with maximum fast_reward first\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.world_model = None\n",
        "        self.search_config = None\n",
        "        self.output_trace_in_each_iter = output_trace_in_each_iter\n",
        "        self.w_exp = w_exp\n",
        "        self.depth_limit = depth_limit\n",
        "        self.n_iters = n_iters\n",
        "        self.cum_reward = cum_reward\n",
        "        self.calc_q = calc_q\n",
        "        default_simulate_strategies: dict[str, Callable[[list[float]], int]] = {\n",
        "            'max': lambda x: np.argmax(x),\n",
        "            'sample': lambda x: np.random.choice(len(x), p=x),\n",
        "            'random': lambda x: np.random.choice(len(x)),\n",
        "        }\n",
        "        self.simulate_choice: Callable[[list[float]], int] = default_simulate_strategies.get(simulate_strategy,\n",
        "                                                                                             simulate_strategy)\n",
        "        assert output_strategy in ['max_reward', 'follow_max', 'max_visit', 'max_iter', 'last_iter',\n",
        "                                   'last_terminal_iter']\n",
        "        self.output_strategy = output_strategy\n",
        "        self.uct_with_fast_reward = uct_with_fast_reward\n",
        "        self._output_iter: list[MCTSNode] = None\n",
        "        self._output_cum_reward = -math.inf\n",
        "        self.trace_in_each_iter: list[list[MCTSNode]] = None\n",
        "        self.root: Optional[MCTSNode] = None\n",
        "        self.disable_tqdm = disable_tqdm\n",
        "        self.node_visualizer = node_visualizer\n",
        "        self.aggregator = aggregator\n",
        "        self.node_visualizer = node_visualizer\n",
        "        self.aggregator = aggregator\n",
        "\n",
        "    def iterate(self, node: MCTSNode) -> list[MCTSNode]:\n",
        "        path = self._select(node)\n",
        "\n",
        "\n",
        "        if not self._is_terminal_with_depth_limit(path[-1]):\n",
        "            self._expand(path[-1])\n",
        "            self._simulate(path)\n",
        "\n",
        "\n",
        "        # while not self._is_terminal_with_depth_limit(path[-1]):\n",
        "        #     self._expand(path[-1])\n",
        "        #     # ### debug mode\n",
        "        #     # if path[-1].parent is not None:\n",
        "        #     #     self._back_propagate(path)\n",
        "        #     if self._is_terminal_with_depth_limit(path[-1]) or len(path[-1].children) == 0:\n",
        "        #         break\n",
        "        #     fast_rewards = [child.fast_reward for child in path[-1].children]\n",
        "        #     node = path[-1].children[self.simulate_choice(fast_rewards)]\n",
        "        #     path.append(node)\n",
        "\n",
        "        cum_reward = self._back_propagate(path)\n",
        "        if self.output_strategy == 'max_iter' and path[-1].is_terminal and cum_reward > self._output_cum_reward:\n",
        "            self._output_cum_reward = cum_reward\n",
        "            self._output_iter = path\n",
        "        if self.output_strategy == 'last_iter':\n",
        "            self._output_cum_reward = cum_reward\n",
        "            self._output_iter = path\n",
        "        if self.output_strategy == 'last_terminal_iter' and path[-1].is_terminal:\n",
        "            self._output_cum_reward = cum_reward\n",
        "            self._output_iter = path\n",
        "        return cum_reward, path\n",
        "\n",
        "    def _is_terminal_with_depth_limit(self, node: MCTSNode):\n",
        "        return node.is_terminal or node.depth >= self.depth_limit\n",
        "\n",
        "    def _select(self, node: MCTSNode) -> list[MCTSNode]:\n",
        "        path = []\n",
        "        while True:\n",
        "            path.append(node)\n",
        "            if node.children is None or len(node.children) == 0 or self._is_terminal_with_depth_limit(node):\n",
        "                return path\n",
        "            node = self._uct_select(node)\n",
        "\n",
        "    def _uct(self, node: MCTSNode) -> float:\n",
        "        return node.Q + self.w_exp * np.sqrt(np.log(len(node.parent.cum_rewards)) / max(1, len(node.cum_rewards)))\n",
        "\n",
        "    def _uct_select(self, node: MCTSNode) -> MCTSNode:\n",
        "        if self.uct_with_fast_reward or all(x.state is not None for x in node.children):\n",
        "            expl = [c for c in node.children if c.fast_reward_details['intuition']!=100.0] # expl = [c for c in node.children]\n",
        "            if any([len(c.cum_rewards)>0 for c in expl]):\n",
        "                # return max([c for c in node.children if c.fast_reward_details['intuition']==100.0], key=self._uct)\n",
        "                return max([c for c in node.children], key=self._uct)\n",
        "            else:\n",
        "                return max([c for c in node.children if c.fast_reward_details['intuition']==100.0 or len(c.cum_rewards)==0], key=self._uct)\n",
        "        else:\n",
        "            unvisited_children = filter(lambda x: x.state is None, node.children)\n",
        "            return max(unvisited_children, key=lambda x: x.fast_reward)\n",
        "\n",
        "    def _expand(self, node: MCTSNode):\n",
        "\n",
        "        if node.state is None:\n",
        "            node.state = self.world_model.step(node.parent.state, node.action)\n",
        "            # reward is calculated after the state is updated, so that the\n",
        "            # information can be cached and passed from the world model\n",
        "            # to the reward function with **aux without repetitive computation\n",
        "            node.reward, node.reward_details = self.search_config. \\\n",
        "                reward(node.parent.state, node.action, **node.fast_reward_details)\n",
        "            node.is_terminal = self.world_model.is_terminal(node.state)\n",
        "\n",
        "        if node.is_terminal:\n",
        "            return\n",
        "\n",
        "        # print(f'Step {node.state.step_idx + 1}: ')\n",
        "        children = []\n",
        "        actions = self.search_config.get_actions(node.state)\n",
        "        for action in actions:\n",
        "            fast_reward, fast_reward_details = action[1], {'intuition': action[1]}\n",
        "            # print(action[0])\n",
        "            # print(fast_reward)\n",
        "            child = MCTSNode(state=None, action=action[0], parent=node,\n",
        "                             fast_reward=fast_reward, fast_reward_details=fast_reward_details, calc_q=self.calc_q)\n",
        "            children.append(child)\n",
        "        # print()\n",
        "\n",
        "        node.children = children\n",
        "\n",
        "    def _simulate(self, path: list[MCTSNode]):\n",
        "        node = path[-1]\n",
        "        while True:\n",
        "            if node.state is None:\n",
        "                self._expand(node)\n",
        "            if self._is_terminal_with_depth_limit(node) or len(node.children) == 0:\n",
        "                return\n",
        "            fast_rewards = [child.fast_reward for child in node.children]\n",
        "            node = node.children[self.simulate_choice(fast_rewards)]\n",
        "            path.append(node)\n",
        "\n",
        "    def _back_propagate(self, path: list[MCTSNode]):\n",
        "        rewards = []\n",
        "        cum_reward = -math.inf\n",
        "        for node in reversed(path):\n",
        "            rewards.append(node.reward)\n",
        "            cum_reward = self.cum_reward(rewards[::-1])\n",
        "            node.cum_rewards.append(cum_reward)\n",
        "        return cum_reward\n",
        "\n",
        "    def _dfs_max_reward(self, path: list[MCTSNode]) -> tuple[float, list[MCTSNode]]:\n",
        "        cur = path[-1]\n",
        "        if cur.is_terminal:\n",
        "            return self.cum_reward([node.reward for node in path[1:]]), path\n",
        "        if cur.children is None:\n",
        "            return -math.inf, path\n",
        "        visited_children = [x for x in cur.children if x.state is not None]\n",
        "        if len(visited_children) == 0:\n",
        "            return -math.inf, path\n",
        "        return max((self._dfs_max_reward(path + [child]) for child in visited_children), key=lambda x: x[0])\n",
        "\n",
        "\n",
        "    def _dfs_reward(self, path: list[MCTSNode]) -> tuple[float, list[MCTSNode]]:\n",
        "        cur = path[-1]\n",
        "        if cur.is_terminal:\n",
        "            return (self.cum_reward([node.reward for node in path[1:]]), path)\n",
        "        if cur.children is None:\n",
        "            return (-math.inf, path)\n",
        "        visited_children = [x for x in cur.children if x.state is not None]\n",
        "        if len(visited_children) == 0:\n",
        "            return (-math.inf, path)\n",
        "        return [self._dfs_max_reward(path + [child]) for child in visited_children]\n",
        "\n",
        "    def _dfs_min_reward(self, path: list[MCTSNode]) -> tuple[float, list[MCTSNode]]:\n",
        "        cur = path[-1]\n",
        "        if cur.is_terminal:\n",
        "            return self.cum_reward([node.reward for node in path[1:]]), path\n",
        "        if cur.children is None:\n",
        "            return -math.inf, path\n",
        "        visited_children = [x for x in cur.children if x.state is not None]\n",
        "        if len(visited_children) == 0:\n",
        "            return -math.inf, path\n",
        "        return min((self._dfs_max_reward(path + [child]) for child in visited_children), key=lambda x: x[0])\n",
        "\n",
        "    def search(self):\n",
        "        self._output_cum_reward = -math.inf\n",
        "        self._output_iter = None\n",
        "        self.root = MCTSNode(state=self.world_model.init_state(), action=None, parent=None, calc_q=self.calc_q)\n",
        "        if self.output_trace_in_each_iter:\n",
        "            self.trace_in_each_iter = []\n",
        "\n",
        "        for _ in trange(self.n_iters, disable=self.disable_tqdm, desc='MCTS iteration', leave=False):\n",
        "            cum_reward, path = self.iterate(self.root)\n",
        "            if self.output_trace_in_each_iter:\n",
        "                # self.trace_in_each_iter.append(deepcopy(path))\n",
        "                self.trace_in_each_iter.append(deepcopy((cum_reward, path)))\n",
        "\n",
        "        if self.output_strategy == 'follow_max':\n",
        "            self._output_iter = []\n",
        "            cur = self.root\n",
        "            while True:\n",
        "                self._output_iter.append(cur)\n",
        "                if cur.is_terminal:\n",
        "                    break\n",
        "                visited_children = [x for x in cur.children if x.state is not None]\n",
        "                if len(visited_children) == 0:\n",
        "                    break\n",
        "                cur = max(visited_children, key=lambda x: x.reward)\n",
        "            self._output_cum_reward = self.cum_reward([node.reward for node in self._output_iter[1::-1]])\n",
        "        if self.output_strategy == 'max_reward':\n",
        "            self._output_cum_reward, self._output_iter = self._dfs_max_reward([self.root])\n",
        "            self._output_cum_reward_worst, self._output_iter_worst = self._dfs_min_reward([self.root])\n",
        "            # self._output_iter_all = self._dfs_reward([self.root])\n",
        "\n",
        "\n",
        "            if self._output_cum_reward == -math.inf:\n",
        "                self._output_iter = None\n",
        "\n",
        "            if self._output_cum_reward_worst == -math.inf:\n",
        "                self._output_iter_worst = None\n",
        "\n",
        "    def __call__(self,\n",
        "                 world_model: WorldModel[State, Action, Example],\n",
        "                 search_config: SearchConfig[State, Action, Example],\n",
        "                 log_file: Optional[str] = None,\n",
        "                 **kwargs) -> MCTSResult:\n",
        "        MCTSNode.reset_id()\n",
        "        self.world_model = world_model\n",
        "        self.search_config = search_config\n",
        "\n",
        "        self.search()\n",
        "\n",
        "        if self._output_iter_worst is None:\n",
        "            terminal_state_worst = trace_worst = None\n",
        "        else:\n",
        "            terminal_state_worst = self._output_iter_worst[-1].state\n",
        "            trace_worst = [node.state for node in self._output_iter_worst], [node.action[0] for node in self._output_iter_worst[1:]]\n",
        "\n",
        "        if self._output_iter is None:\n",
        "            terminal_state = trace = None\n",
        "        else:\n",
        "            terminal_state = self._output_iter[-1].state\n",
        "            trace = [node.state for node in self._output_iter], [node.action[0] for node in self._output_iter[1:]]\n",
        "\n",
        "        if self.output_trace_in_each_iter:\n",
        "            trace_in_each_iter = self.trace_in_each_iter\n",
        "            tree_state_after_each_iter = [trace[0] for trace in trace_in_each_iter]\n",
        "        else:\n",
        "            trace_in_each_iter = tree_state_after_each_iter = None\n",
        "        result = MCTSResult(terminal_state=terminal_state,\n",
        "                            cum_reward=self._output_cum_reward,\n",
        "                            trace=(self._output_cum_reward, trace),\n",
        "                            trace_worst=(self._output_cum_reward_worst, trace_worst),\n",
        "                            # trace_all=self._output_iter_all,\n",
        "                            trace_of_nodes=self._output_iter,\n",
        "                            tree_state=self.root,\n",
        "                            trace_in_each_iter=trace_in_each_iter,\n",
        "                            tree_state_after_each_iter=tree_state_after_each_iter)\n",
        "        if self.aggregator is not None:\n",
        "            result = MCTSResult(\n",
        "                terminal_state=result.terminal_state,\n",
        "                cum_reward=result.cum_reward,\n",
        "                trace=result.trace,\n",
        "                trace_worst=result.trace_worst,\n",
        "                # trace_all=result._output_iter_all,\n",
        "                trace_of_nodes=result.trace_of_nodes,\n",
        "                tree_state=result.tree_state,\n",
        "                trace_in_each_iter=result.trace_in_each_iter,\n",
        "                tree_state_after_each_iter=result.tree_state_after_each_iter,\n",
        "                aggregated_result=self.aggregator(result.tree_state),\n",
        "            )\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_T1O1TC1WcQ",
        "outputId": "bbfeb8db-4392-48d8-f49f-98cc59a1c884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q = pd.read_csv('/content/drive/MyDrive/questions.csv')"
      ],
      "metadata": {
        "id": "9_ZelEiA1wnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI()\n",
        "def gpt(prompt):\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "          {\"role\": \"user\", \"content\": input}\n",
        "      ]\n",
        "  )\n"
      ],
      "metadata": {
        "id": "DZbnxIPQ2WJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "elec2324 = pd.read_csv('/content/drive/MyDrive/elec2.csv')\n",
        "po = pd.read_csv('/content/drive/MyDrive/populate.csv')\n",
        "building = pd.read_csv('/content/drive/MyDrive/building2.csv')\n",
        "gas = pd.read_csv('/content/drive/MyDrive/gas2.csv')\n",
        "building = building.drop(columns=['Unnamed: 0'])\n",
        "gas = gas.rename(columns={\"PNU\": \"pnu\"})\n"
      ],
      "metadata": {
        "id": "u_E-MyYC-X0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gas #2720010100012170\n",
        "# building #277202502104091\n",
        "# elec2324 #2720010100012170\n",
        "# elec2324[\"PNU\"] = elec2324[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "nptLvaEE-rk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "building[\"pnu\"] = building[\"pnu\"].astype(str)\n",
        "building[\"PNU\"] = building[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "09SQLcpQGrb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elec2324[\"pnu\"] = elec2324[\"pnu\"].astype(str)\n",
        "elec2324[\"PNU\"] = elec2324[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "_7XhDIh0--RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gas[\"pnu\"] = gas[\"pnu\"].astype(str)\n",
        "gas[\"PNU\"] = gas[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "6evk9l8WHPey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "po[\"pnu\"] = po[\"pnu\"].astype(str)\n",
        "po[\"PNU\"] = po[\"pnu\"].str[:10]"
      ],
      "metadata": {
        "id": "iwXwJKA1IMYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vDOpsuab8qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "building.to_csv('/content/drive/MyDrive/new/building3.csv')\n",
        "elec2324.to_csv('/content/drive/MyDrive/new/elec3.csv')\n",
        "gas.to_csv('/content/drive/MyDrive/new/gas3.csv')\n",
        "po.to_csv('/content/drive/MyDrive/new/po3.csv')\n"
      ],
      "metadata": {
        "id": "FEWwnUucb-z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(po.head())\n",
        "print(elec2324.head())\n",
        "print(building.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqrp8C_aHVsi",
        "outputId": "1369e7f1-8634-4dd8-ea16-d2a6de0ac904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             2023_ 2023_  2023_  2023_  2023_   \\\n",
            "0             81,015    41,163          1.97       38,868       42,147   \n",
            "1           7,891     4,809          1.64        3,940        3,951   \n",
            "2           6,395     4,093          1.56        3,050        3,345   \n",
            "3    1      4,816     3,425          1.41        2,313        2,503   \n",
            "4    2      4,789     3,001          1.60        2,416        2,373   \n",
            "\n",
            "  2024_ 2024_  2024_  2024_  2024_          pnu  \\\n",
            "0     89,685    45,209          1.98       42,950       46,735      271100   \n",
            "1      8,401     5,092          1.65        4,182        4,219  2711051700   \n",
            "2      6,678     4,177          1.60        3,243        3,435  2711054500   \n",
            "3      4,829     3,538          1.36        2,308        2,521  2711056500   \n",
            "4      6,389     3,751          1.70        3,188        3,201  2711057500   \n",
            "\n",
            "     PNU  \n",
            "0      271100  \n",
            "1  2711051700  \n",
            "2  2711054500  \n",
            "3  2711056500  \n",
            "4  2711057500  \n",
            "                                                     \\\n",
            "0     121-70  202401    68   79 ()   180038   \n",
            "1     121-70  202402    68   77 ()   181483   \n",
            "2     121-70  202403    68   77 ()   163726   \n",
            "3     121-70  202404    68   79 ()   165443   \n",
            "4     121-70  202405    68   77 ()   151984   \n",
            "\n",
            "                pnu  \n",
            "0  2720010100012170  \n",
            "1  2720010100012170  \n",
            "2  2720010100012170  \n",
            "3  2720010100012170  \n",
            "4  2720010100012170  \n",
            "               pnu                                                    \\\n",
            "0  277202502104091      409-1     5 5-10 ( )   \n",
            "1   27720250220221       22-1                             NaN   \n",
            "2  277202502201960        196    3 19-11 ( )   \n",
            "3  277202502202810        281     3 5-13 ( )   \n",
            "4  277202502203954      395-4       4 19 ( )   \n",
            "\n",
            "                                    PNU  \n",
            "0     1     0  19921123  19920629   22.68         2772025021  \n",
            "1     3     0  19901006       NaN  189.72  1  2772025022  \n",
            "2     1     0  19871022       NaN   95.18         2772025022  \n",
            "3     1     0  1965           NaN   56.80         2772025022  \n",
            "4     2     0  20040413  20040108  198.90        2772025022  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "from sqlalchemy.orm import Session\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite:///database.db\")  #  \"postgresql://user:password@host/dbname\"\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    elec2324.to_sql(\"electricity\", con=conn, if_exists=\"replace\", index=False)\n",
        "    building.to_sql(\"building\", con=conn, if_exists=\"replace\", index=False)\n",
        "    po.to_sql(\"population\", con=conn, if_exists=\"replace\", index=False)\n",
        "    gas.to_sql(\"gas\", con=conn, if_exists=\"replace\", index=False)"
      ],
      "metadata": {
        "id": "-Nfi3EIc-UO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install func_timeout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Qfxb9EETe6",
        "outputId": "65c8921a-6dd8-4659-c00e-c90b1a3f21d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting func_timeout\n",
            "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: func_timeout\n",
            "  Building wheel for func_timeout (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for func_timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15077 sha256=58d80359e5f604c59c63fe8145821131918502ac7ccb2144fc26242297124d04\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/e6/86/f23164d12c3134966614102db8e7956ab359faf7ffd78703ce\n",
            "Successfully built func_timeout\n",
            "Installing collected packages: func_timeout\n",
            "Successfully installed func_timeout-4.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python preprocess_data.py --dataset bird --mode train --LLM_model meta-llama/Meta-Llama-3-8B-Instruct --PSG --data_path /data/vda/dataset --output_path ./dataset\n",
        "#\n",
        "# -*- coding: utf-8 -*-\n",
        "# --dataset bird --mode test --LLM_model gpt --PSG --data_path /content/drive/MyDrive/questions.csv --output_path\n",
        "import argparse\n",
        "import copy\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import sqlite3\n",
        "import traceback\n",
        "import os\n",
        "# from vllm import LLM, SamplingParams\n",
        "from func_timeout import func_set_timeout\n",
        "import func_timeout\n",
        "import tqdm\n",
        "\n",
        "prompt_cw_temp_sft = \"\"\"Given the following database schema and question, your task is to write a valid SQL query whose execution will accurately answer the question. If the value below the incomplete SQL query is not empty, your task is to complete it into a full SQL query. Remember to end the query with a semicolom ```;```.\n",
        "\n",
        "Database schema:\n",
        "{ds}\n",
        "\n",
        "Sample rows of each table:\n",
        "{sr}\n",
        "\n",
        "Question:\n",
        "{qs}{hint}\n",
        "\n",
        "Question hint:\n",
        "{sql}\n",
        "\n",
        "The incomplete SQL query:\n",
        "{sql}\n",
        "\n",
        "Answer the question by a SQL query only with no explanation:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
        "            data = json.load(file)\n",
        "            return data\n",
        "    except Exception as e:\n",
        "        print(\"=\" * 10, e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# class LLM_Model(object):\n",
        "#     def __init__(self, model=''):\n",
        "\n",
        "#         self.model = model\n",
        "#         model = model.lower().replace('_', '').replace('-', '')\n",
        "#         if 'qwen2' in model:\n",
        "#             self.tag = 'qwen2'\n",
        "#         elif 'llama3' in model:\n",
        "#             self.tag = 'llama3'\n",
        "#         elif 'llama2' in model:\n",
        "#             self.tag = 'llam2'\n",
        "#         elif 'deepseek' in model:\n",
        "#             self.tag = 'deepseek'\n",
        "#         elif 'mistral' in model:\n",
        "#             self.tag = 'mistral'\n",
        "#         elif 'codellama' in model:\n",
        "#             self.tag = 'codellama'\n",
        "#         elif 'gpt' in model:\n",
        "#             self.tag = 'gpt'\n",
        "#         else:\n",
        "#             raise TypeError(f\"Unexpect model: {model}.\")\n",
        "\n",
        "#         self.llm = LLM(model=self.model,\n",
        "#                        seed=123,\n",
        "#                        tensor_parallel_size=args.gpus,\n",
        "#                        trust_remote_code=True,\n",
        "#                        gpu_memory_utilization=0.9\n",
        "#                        )\n",
        "#         self.tokenizer = self.llm.get_tokenizer()\n",
        "#         self.llm = client\n",
        "\n",
        "#     def generate_response(self, prompts, max_tokens=1024, temperature=0.01, top_p=0.5):\n",
        "#         sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens,\n",
        "#                                          skip_special_tokens=True, stop=self.tokenizer.eos_token)\n",
        "#         if self.tag in ['mistral']:\n",
        "#             messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "#         else:\n",
        "#             messages_list = [\n",
        "#                 [{\"role\": \"system\", \"content\": \"You are a helpful SQLite assistant.\"}, {\"role\": \"user\", \"content\": p}]\n",
        "#                 for p in prompts]\n",
        "#         messages_list = self.tokenizer.apply_chat_template(messages_list, add_generation_prompt=True, tokenize=False)\n",
        "#         outputs = self.llm.generate(messages_list, sampling_params)\n",
        "#         return [output.outputs[0].text for output in outputs]\n",
        "\n",
        "class LLM_Model(object):\n",
        "    def __init__(self, model=''):\n",
        "\n",
        "        self.model = model\n",
        "        self.llm = client\n",
        "\n",
        "    def generate_response(self, prompts, max_tokens=1024, temperature=0.01, top_p=0.5):\n",
        "\n",
        "        if self.tag in ['mistral']:\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "        else:\n",
        "            messages_list = [\n",
        "                [{\"role\": \"system\", \"content\": \"You are a helpful SQLite assistant.\"}, {\"role\": \"user\", \"content\": p}]\n",
        "                for p in prompts]\n",
        "\n",
        "\n",
        "        outputs = self.chat.completions.create(model=\"gpt-4o-mini\",messages= messages_list)\n",
        "        return  [output.choices[0].message.content for output in outputs]\n",
        "\n",
        "class LLM_Online(object):\n",
        "    def __init__(self, model=\"qwen72b\", device=[0]):\n",
        "        None\n",
        "\n",
        "    def generate_response(self, prompts):\n",
        "        rs = []\n",
        "        for prompt in tqdm.tqdm(prompts):\n",
        "            res = None  # your online LLM\n",
        "            rs.append(res)\n",
        "        return rs\n",
        "\n",
        "\n",
        "def parse_dataset(data_path, mode='dev', dataset='bird'):\n",
        "    # redirect path\n",
        "    data_tuples_path = ''\n",
        "    if dataset == 'bird':\n",
        "        # data_tuples_path = os.path.join(data_path, dataset, mode, f'{mode}.json')\n",
        "        data_tuples_path = os.path.join(data_path, dataset, mode, f'{mode}.json')\n",
        "    elif 'spider_DK' == dataset:\n",
        "        data_tuples_path = os.path.join(data_path, 'spider', 'Spider_DK.json')\n",
        "    elif 'spider_real' == dataset:\n",
        "        data_tuples_path = os.path.join(data_path, 'spider', 'spider-realistic.json')\n",
        "\n",
        "    elif 'spider_syn' == dataset:\n",
        "        data_tuples_path = os.path.join(data_path, 'spider', 'dev.json')\n",
        "    elif 'spider' in dataset:\n",
        "        if mode == 'test':\n",
        "            data_tuples_path = os.path.join(data_path, 'spider', 'test.json')\n",
        "        else:\n",
        "            data_tuples_path = os.path.join(data_path, 'spider', f'{mode}.json')\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "    data_tuples = read_json_file(data_tuples_path)\n",
        "\n",
        "    return data_tuples\n",
        "\n",
        "\n",
        "def convert_fk_index(data):\n",
        "    fk_holder = []\n",
        "    table_names_original = [i.lower() for i in data['table_names_original']]  # some bug\n",
        "    column_names_original = [(i[0], i[1].lower()) for i in data['column_names_original']]\n",
        "    for fk in data[\"foreign_keys\"]:\n",
        "        tn, col, ref_tn, ref_col = fk[0][0], fk[0][1], fk[1][0], fk[1][1]\n",
        "        if type(tn) is str:\n",
        "            tn = tn.lower()\n",
        "        if type(col) is str:\n",
        "            col = col.lower()\n",
        "        if type(ref_tn) is str:\n",
        "            ref_tn = ref_tn.lower()\n",
        "        if type(ref_col) is str:\n",
        "            ref_col = ref_col.lower()\n",
        "        ref_cid, cid = None, None\n",
        "        try:\n",
        "            tid = table_names_original.index(tn)\n",
        "            ref_tid = table_names_original.index(ref_tn)\n",
        "            for i, (tab_id, col_org) in enumerate(column_names_original):\n",
        "                if tab_id == ref_tid and ref_col == col_org:\n",
        "                    ref_cid = i\n",
        "                elif tid == tab_id and col == col_org:\n",
        "                    cid = i\n",
        "            if ref_cid and cid:\n",
        "                fk_holder.append([cid, ref_cid])\n",
        "        except:\n",
        "            traceback.print_exc()\n",
        "            print(\"table_names_original: \", table_names_original)\n",
        "            print(\"finding tab name: \", tn, ref_tn)\n",
        "            print(data)\n",
        "            # sys.exit()\n",
        "    return fk_holder\n",
        "\n",
        "\n",
        "def dump_db_json_schema(db, f):\n",
        "    '''read table and column info'''\n",
        "\n",
        "    try:\n",
        "        conn = sqlite3.connect(db)\n",
        "    except:\n",
        "        print(db)\n",
        "        exit()\n",
        "    conn.execute('pragma foreign_keys=ON')\n",
        "    cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "\n",
        "    data = {'db_id': f,\n",
        "            'table_names_original': [],\n",
        "            'table_names': [],\n",
        "            'column_names_original': [(-1, '*')],\n",
        "            'column_names': [(-1, '*')],\n",
        "            'column_types': ['text'],\n",
        "            'primary_keys': [],\n",
        "            'foreign_keys': []}\n",
        "\n",
        "    fk_holder = []\n",
        "    for i, item in enumerate(cursor.fetchall()):\n",
        "        table_name = item[0]\n",
        "        data['table_names_original'].append(table_name)\n",
        "        data['table_names'].append(table_name.lower().replace(\"_\", ' '))\n",
        "        fks = conn.execute(\"PRAGMA foreign_key_list('{}') \".format(table_name)).fetchall()\n",
        "        # print(\"db:{} table:{} fks:{}\".format(f,table_name,fks))\n",
        "        fk_holder.extend([[(table_name, fk[3]), (fk[2], fk[4])] for fk in fks])\n",
        "        cur = conn.execute(\"PRAGMA table_info('{}') \".format(table_name))\n",
        "        for j, col in enumerate(cur.fetchall()):\n",
        "            data['column_names_original'].append((i, col[1]))\n",
        "            data['column_names'].append((i, col[1].lower().replace(\"_\", \" \")))\n",
        "            # varchar, '' -> text, int, numeric -> integer,\n",
        "            col_type = col[2].lower()\n",
        "            if 'char' in col_type or col_type == '' or 'text' in col_type or 'var' in col_type:\n",
        "                data['column_types'].append('text')\n",
        "            elif 'int' in col_type or 'numeric' in col_type or 'decimal' in col_type or 'number' in col_type \\\n",
        "                    or 'id' in col_type or 'real' in col_type or 'double' in col_type or 'float' in col_type:\n",
        "                data['column_types'].append('number')\n",
        "            elif 'date' in col_type or 'time' in col_type or 'year' in col_type:\n",
        "                data['column_types'].append('time')\n",
        "            elif 'boolean' in col_type:\n",
        "                data['column_types'].append('boolean')\n",
        "            else:\n",
        "                data['column_types'].append('others')\n",
        "\n",
        "            if col[5] == 1:\n",
        "                data['primary_keys'].append(len(data['column_names']) - 1)\n",
        "\n",
        "    data[\"foreign_keys\"] = fk_holder\n",
        "    data['foreign_keys'] = convert_fk_index(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_schema_dict(db, kk=3):\n",
        "    \"\"\"\n",
        "    Get database's schema, which is a dict with table name as key\n",
        "    and list of column names as value\n",
        "    :param db: database path\n",
        "    :return: schema dict\n",
        "    \"\"\"\n",
        "    data = dump_db_json_schema(db, db.split('/')[-1])\n",
        "    tables = data['table_names_original']\n",
        "    column_types = data['column_types']\n",
        "    primary_keys = data['primary_keys']\n",
        "    foreign_keys = data['foreign_keys']\n",
        "    column_names = data['column_names_original']\n",
        "\n",
        "    schema_dict = {\n",
        "        'tables': {},\n",
        "        'foreign_keys': []\n",
        "    }\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        t = {}\n",
        "        for j, c in enumerate(column_names):\n",
        "            if c[0] == i:\n",
        "                if j in primary_keys:\n",
        "                    t[c[1]] = [column_types[j].upper(), True]\n",
        "                else:\n",
        "                    t[c[1]] = [column_types[j].upper(), True]\n",
        "        schema_dict['tables'][table] = t\n",
        "\n",
        "    for foreign_key in foreign_keys:\n",
        "        t1 = tables[column_names[foreign_key[0]][0]]\n",
        "        c1 = column_names[foreign_key[0]][1]\n",
        "        t2 = tables[column_names[foreign_key[1]][0]]\n",
        "        c2 = column_names[foreign_key[1]][1]\n",
        "        schema_dict['foreign_keys'].append([t1, c1, t2, c2])\n",
        "\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "    # get exapmles\n",
        "    for table in schema_dict['tables'].keys():\n",
        "        try:\n",
        "            select_query = f'SELECT * FROM `{table}` LIMIT {kk}'\n",
        "            cursor.execute(select_query)\n",
        "            rows = cursor.fetchall()\n",
        "            cursor.execute(f\"PRAGMA table_info(`{table}`);\")\n",
        "            columns = [column[1] for column in cursor.fetchall()]\n",
        "            for i, c in enumerate(columns):\n",
        "                cls_valuse = [f\"{row[i][0:100]}...\" if type(row[i]) is str and len(row[i]) > 100 else row[i] for row in\n",
        "                              rows]\n",
        "                schema_dict['tables'][table][c].append(cls_valuse)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    return schema_dict\n",
        "\n",
        "\n",
        "def get_example_str(schema_dict, k=1):\n",
        "    tables = list(schema_dict['tables'].keys())\n",
        "    examples = {}\n",
        "    for table in tables:\n",
        "        table_dict = schema_dict['tables'][table]\n",
        "        example = []\n",
        "        for cls in table_dict.keys():\n",
        "            example.append(table_dict[cls][2])\n",
        "        example_str = []\n",
        "        for i, v in enumerate(example[0]):\n",
        "            example_str.append(tuple([e[i] for e in example]))\n",
        "            if (i + 1) == k:\n",
        "                break\n",
        "        examples[table] = example_str\n",
        "\n",
        "    e_s = ''\n",
        "    for key in examples.keys():\n",
        "        e_s += f\"{key}: \" + str(examples[key]) + '\\n'\n",
        "\n",
        "    return e_s[:-1]\n",
        "\n",
        "\n",
        "def get_schmea_str_and_examples(schema_dict):\n",
        "    schmea_str = \"\"\n",
        "    tables = list(schema_dict['tables'].keys())\n",
        "    examples = {}\n",
        "    for table in tables:\n",
        "        if ' ' in table:\n",
        "            table_str = f'CREATE TABLE \"{table}\" ('\n",
        "        else:\n",
        "            table_str = f\"CREATE TABLE {table} (\"\n",
        "        table_dict = schema_dict['tables'][table]\n",
        "\n",
        "        pk_str = ''\n",
        "        example = []\n",
        "        for cls in table_dict.keys():\n",
        "            try:\n",
        "                cls_ = f'\"{cls}\"' if ' ' in cls else cls\n",
        "                table_str += f\"{cls_} {table_dict[cls][0]}, \"\n",
        "                if table_dict[cls][1]:\n",
        "                    pk_str += cls_ + ', '\n",
        "                example.append(table_dict[cls][2])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        example_str = []\n",
        "\n",
        "        try:\n",
        "            for i, v in enumerate(example[0]):\n",
        "                example_str.append(tuple([e[i] for e in example]))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "        examples[table] = example_str\n",
        "\n",
        "        if pk_str != '':\n",
        "            table_str += f\"PRIMARY KEY({pk_str[:-2]}), \"\n",
        "\n",
        "        fk_str = ''\n",
        "        for fk in schema_dict['foreign_keys']:\n",
        "            if fk[0] == table and fk[2] in tables:\n",
        "                if fk[3] in schema_dict['tables'][fk[2]].keys():\n",
        "                    fk = [f'\"{f}\"' if ' ' in f else f for f in fk]\n",
        "                    fk_str += f'FOREIGN KEY ({fk[1]}) REFERENCES {fk[2]}({fk[3]}), '\n",
        "        if fk_str != '':\n",
        "            table_str += fk_str\n",
        "\n",
        "        schmea_str += table_str[:-2] + '); '\n",
        "\n",
        "    schmea_str = schmea_str[:-1]\n",
        "\n",
        "    e_s = ''\n",
        "    for key in examples.keys():\n",
        "        e_s += f\"{key}: \" + str(examples[key]) + '\\n'\n",
        "\n",
        "    return schmea_str, e_s[:-1]\n",
        "\n",
        "\n",
        "# parse SQL\n",
        "def parse_sql_from_string(input_string):\n",
        "    input_string = input_string.replace('\\n', ' ').replace('\\t', '')\n",
        "    rs = ''\n",
        "    if '```sql' in input_string:\n",
        "        try:\n",
        "            sql_pattern = r'```sql(.*?)```'\n",
        "            all_sqls = []\n",
        "            for match in re.finditer(sql_pattern, input_string, re.DOTALL):\n",
        "                all_sqls.append(match.group(1).strip())\n",
        "            if all_sqls:\n",
        "                rs = all_sqls[-1]\n",
        "                if 'SELECT' not in rs and len(all_sqls) > 1:\n",
        "                    rs = all_sqls[-2]\n",
        "        except:\n",
        "            None\n",
        "    if 'select' in input_string.lower() and rs == '':\n",
        "        rs = input_string[input_string.find('SELECT'):]\n",
        "    if ';' in rs:  # end\n",
        "        rs = rs[:input_string.find(';') + 1]\n",
        "    if rs == '':\n",
        "        rs = 'SELECT xx FROM xx'\n",
        "    return replace_multiple_spaces(rs).replace('```', '')\n",
        "\n",
        "\n",
        "def replace_multiple_spaces(text):\n",
        "    return re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "\n",
        "def filter_dict_by_sql(schema_dict, sql):\n",
        "    schema_dict_ = copy.deepcopy(schema_dict)\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    # tables\n",
        "    for table in keys:\n",
        "        if f'from {table.lower()}' not in sql.lower() and f'join {table.lower()}' not in sql.lower():\n",
        "            schema_dict_['tables'].pop(table, None)\n",
        "    # columns\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    for table in keys:\n",
        "        cls_keys = list(schema_dict_['tables'][table].keys())\n",
        "        cls_keys.sort(key=lambda x: - len(x))\n",
        "        tabel_dict = copy.deepcopy(schema_dict_['tables'][table])\n",
        "        for cls in cls_keys:\n",
        "            if cls.lower() not in sql.lower():\n",
        "                schema_dict_['tables'][table].pop(cls, None)\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            # schema_dict_['tables'][table] = tabel_dict  # for COUNT(*)\n",
        "            for cls in tabel_dict.keys():\n",
        "                if tabel_dict[cls][1] == True:\n",
        "                    schema_dict_['tables'][table][cls] = tabel_dict[cls]\n",
        "\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[0]] = tabel_dict[tabel_dict.keys()[0]]\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[1]] = tabel_dict[tabel_dict.keys()[1]]\n",
        "            # for COUNT(*)\n",
        "\n",
        "    return schema_dict_\n",
        "\n",
        "\n",
        "def filter_dict_by_sl(schema_dict, sql):\n",
        "    schema_dict_ = copy.deepcopy(schema_dict)\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    # tables\n",
        "    for table in keys:\n",
        "        if f'{table.lower()}' not in sql.lower():\n",
        "            schema_dict_['tables'].pop(table, None)\n",
        "    # columns\n",
        "    keys = list(schema_dict_['tables'].keys())\n",
        "    keys.sort(key=lambda x: - len(x))\n",
        "    for table in keys:\n",
        "        cls_keys = list(schema_dict_['tables'][table].keys())\n",
        "        cls_keys.sort(key=lambda x: - len(x))\n",
        "        tabel_dict = copy.deepcopy(schema_dict_['tables'][table])\n",
        "        for cls in cls_keys:\n",
        "            if cls.lower() not in sql.lower():\n",
        "                schema_dict_['tables'][table].pop(cls, None)\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            # schema_dict_['tables'][table] = tabel_dict  # for COUNT(*)\n",
        "            for cls in tabel_dict.keys():\n",
        "                if tabel_dict[cls][1] == True:\n",
        "                    schema_dict_['tables'][table][cls] = tabel_dict[cls]\n",
        "\n",
        "        if len(schema_dict_['tables'][table].keys()) == 0:\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[0]] = tabel_dict[tabel_dict.keys()[0]]\n",
        "            schema_dict_['tables'][table][tabel_dict.keys()[1]] = tabel_dict[tabel_dict.keys()[1]]\n",
        "            # for COUNT(*)\n",
        "\n",
        "    return schema_dict_\n",
        "\n",
        "\n",
        "@func_set_timeout(5)\n",
        "def execute_query_limit(db_path, query):\n",
        "    error = ''\n",
        "    result = None\n",
        "    conn = sqlite3.connect(db_path, timeout=5.0, check_same_thread=False)\n",
        "    cursor = conn.cursor()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    result = cursor.fetchone()\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "    return result, error\n",
        "\n",
        "\n",
        "def execute_query(db_path, query):\n",
        "    try:\n",
        "        result, error = execute_query_limit(db_path, query)\n",
        "    except func_timeout.exceptions.FunctionTimedOut:\n",
        "        error = \"SQL execution timeout\"\n",
        "        print(\"*\" * 30, error, query)\n",
        "        result = None\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        print(\"*\" * 30, error, query)\n",
        "        result = None\n",
        "    return result, error\n",
        "\n",
        "\n",
        "def replace_syn(data1, data2):\n",
        "    for i in range(len(data1)):\n",
        "        if data1[i]['question'] == data2[i]['SpiderQuestion']:\n",
        "            data1[i]['question'] = data2[i]['SpiderSynQuestion']\n",
        "    return data1\n",
        "\n",
        "\n",
        "def eval_all(args):\n",
        "    dataset = args.dataset\n",
        "    mode = args.mode\n",
        "    data_tuples = parse_dataset(args.data_path, mode, dataset)\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    if dataset == 'spider_syn':\n",
        "        data2 = read_json_file(os.path.join(args.data_path, 'spider', f'dev_syn.json'))\n",
        "        data_tuples = replace_syn(data_tuples, data2)\n",
        "        dataset = 'spider'\n",
        "        args.tag += '_syn'\n",
        "\n",
        "    if dataset == 'spider_DK':\n",
        "        args.tag += '_DK'\n",
        "        dataset = 'spider'\n",
        "\n",
        "    if dataset == 'spider_real':\n",
        "        args.tag += '_real'\n",
        "        dataset = 'spider'\n",
        "\n",
        "    if dataset == 'bird':\n",
        "        kk = 5\n",
        "    else:\n",
        "        kk = 10\n",
        "    kkkkk = 1 if dataset == 'bird' else 3\n",
        "\n",
        "    if 'online' in args.tag:\n",
        "        generator = LLM_Online()\n",
        "    else:\n",
        "        generator = LLM_Model(args.LLM_model)\n",
        "    tag = args.tag\n",
        "\n",
        "\n",
        "    # generate SQL\n",
        "    if True:\n",
        "        sql_results = []\n",
        "        data_header = [[\"NLQ\", \"Predict\", \"GOLD\", 'database']]\n",
        "        prompts = []\n",
        "        for index, row in enumerate(data_tuples):\n",
        "            if 'spider' in dataset:\n",
        "                row['SQL'] = row['query']\n",
        "            if 'drspider' in dataset:\n",
        "                row['SQL'] = row['query']\n",
        "\n",
        "            question, db_id = row['question'], row['db_id']\n",
        "            if dataset == 'spider':\n",
        "                if mode == 'test':\n",
        "                    db_path = os.path.join(args.data_path, dataset, 'test_database', db_id, f\"{db_id}.sqlite\")\n",
        "                else:\n",
        "                    db_path = os.path.join(args.data_path, dataset, 'database', db_id, f\"{db_id}.sqlite\")\n",
        "            elif dataset == 'drspider':\n",
        "                db_path = os.path.join(args.data_path, db_id, f\"{db_id}.sqlite\")\n",
        "            elif dataset == 'bird':\n",
        "                db_path = os.path.join(args.data_path, dataset, f'{mode}/{mode}_databases', db_id, f\"{db_id}.sqlite\")\n",
        "            else:\n",
        "                raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "            schema_dict = get_schema_dict(db_path, kk=kk)\n",
        "            database_schema, examples = get_schmea_str_and_examples(schema_dict)\n",
        "            schema_dict_ = schema_dict\n",
        "\n",
        "            if dataset == 'bird':\n",
        "                prompt = [question, schema_dict,\n",
        "                          f\"\\n\\n/* Question hint */\\n{row['evidence']}\" if row['evidence'] != '' else '', schema_dict_]\n",
        "            else:\n",
        "                prompt = [question, schema_dict, '', schema_dict_]\n",
        "            prompts.append([database_schema, str(examples), question, row['SQL'], db_id, prompt, db_path])\n",
        "\n",
        "        n_samples = len(data_tuples)\n",
        "        n_batches = (n_samples - 1) // batch_size + 1\n",
        "\n",
        "        prompts_collection = []\n",
        "        prompts_collection_db = []\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            start = i * batch_size\n",
        "            end = n_samples if i == n_batches - 1 else (i + 1) * batch_size\n",
        "            batch_prompts = prompts[start: end]\n",
        "            schema_dicts = []  # only keep the tables\n",
        "\n",
        "            for j, v in enumerate(batch_prompts):\n",
        "                batch_prompts[j][1] = get_example_str(batch_prompts[j][5][1], kkkkk)\n",
        "\n",
        "            # text-to-sql\n",
        "            final_prompts = [prompt_cw_temp_sft.format(ds=j[0], sr=j[1], qs=j[2], hint=j[5][2], sql='') for j in batch_prompts]\n",
        "            response_strs = generator.generate_response(prompts=final_prompts)\n",
        "\n",
        "            def contains_subquery(sql_query, tables):\n",
        "                sql = sql_query.lower()\n",
        "                select_num = 0\n",
        "                join_num = 0\n",
        "                tmp = sql\n",
        "                while 'select' in tmp:\n",
        "                    tmp = tmp[tmp.find('select') + 6:]\n",
        "                    select_num += 1\n",
        "                tmp = sql\n",
        "                while 'join' in tmp:\n",
        "                    tmp = tmp[tmp.find('select') + 6:]\n",
        "                    join_num += 1\n",
        "                table_num = len([key for key in tables if f\"from {key.lower()}\" in sql or f\"join {key.lower()}\" in sql])\n",
        "                if table_num == 1:\n",
        "                    hard = 1\n",
        "                elif table_num == 2:\n",
        "                    hard = 2\n",
        "                else:\n",
        "                    hard = 3\n",
        "                return hard\n",
        "\n",
        "            nc_idx = []\n",
        "            continue_sqls = []\n",
        "            # noisy correction\n",
        "\n",
        "            for idx, v in enumerate(response_strs):\n",
        "                pre_sql = parse_sql_from_string(response_strs[idx])\n",
        "                ex_flg3 = True if execute_query(batch_prompts[idx][6], pre_sql)[1] == '' else False\n",
        "                hard = contains_subquery(pre_sql, batch_prompts[idx][5][1]['tables'].keys())\n",
        "                if ex_flg3 == False or hard > 2:\n",
        "                    common_sql = 'SELECT '\n",
        "                    continue_sqls.append(common_sql)\n",
        "                    nc_idx.append(idx)\n",
        "\n",
        "            # PSG\n",
        "            if args.PSG:\n",
        "                cl_prompts = []\n",
        "                for j, idx in enumerate(nc_idx):\n",
        "                    v = batch_prompts[idx]\n",
        "                    ds = get_schmea_str_and_examples(v[5][1])[0]\n",
        "                    sr = get_example_str(v[5][1], kkkkk)\n",
        "                    common_sql = continue_sqls[j]\n",
        "                    if args.eval_sft == 1:\n",
        "                        cl_prompts.append(\n",
        "                            prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "                    else:\n",
        "                        cl_prompts.append(prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "\n",
        "                if len(nc_idx) > 0:\n",
        "                    response_strs_ = generator.generate_response(prompts=cl_prompts)\n",
        "                    print(\"%%%%%%%%%%%%%%%%%%\", response_strs_[0])\n",
        "                    for idx, v in enumerate(nc_idx):\n",
        "                        if execute_query(batch_prompts[v][6], parse_sql_from_string(response_strs_[idx]))[\n",
        "                            0] is not None:\n",
        "                            response_strs[v] = response_strs_[idx]\n",
        "\n",
        "            for j, response_str in enumerate(response_strs):\n",
        "                database_schema = batch_prompts[j][0]\n",
        "                question = batch_prompts[j][2]\n",
        "                gt_sql = replace_multiple_spaces(batch_prompts[j][3])\n",
        "                if gt_sql.endswith(\";;\"):\n",
        "                    gt_sql = gt_sql[:-1]\n",
        "\n",
        "                if not gt_sql.endswith(\";\"):\n",
        "                    gt_sql += \";\"\n",
        "\n",
        "                db_id = batch_prompts[j][4]\n",
        "                prompt = final_prompts[j]\n",
        "                print(f\"=={start + j + 1}/{len(data_tuples)}=={db_id}=={tag}==================\")\n",
        "\n",
        "                try:\n",
        "                    if dataset == 'spider':\n",
        "                        if mode == 'test':\n",
        "                            db_path = os.path.join(args.data_path, dataset, 'test_database', db_id, f\"{db_id}.sqlite\")\n",
        "                        else:\n",
        "                            db_path = os.path.join(args.data_path, dataset, 'database', db_id, f\"{db_id}.sqlite\")\n",
        "                    elif dataset == 'bird':\n",
        "                        db_path = os.path.join(args.data_path, dataset, f'{mode}/{mode}_databases', db_id,\n",
        "                                               f\"{db_id}.sqlite\")\n",
        "                    else:\n",
        "                        raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "                    SQL_str = parse_sql_from_string(response_str)\n",
        "                except Exception as e:\n",
        "                    res = f'error: {str(e)}'\n",
        "                    print(res, response_str)\n",
        "\n",
        "                sql_results.append([question, SQL_str, gt_sql, db_id])\n",
        "\n",
        "                if args.PSG:\n",
        "                    if j in nc_idx:\n",
        "                        prompt = prompt_cw_temp_sft.format(ds=batch_prompts[j][0], sr=batch_prompts[j][1], qs=batch_prompts[j][2], hint=batch_prompts[j][5][2], sql=truncate_sql_before_keywords(gt_sql, CLAUSE_KEYWORDS))\n",
        "                        print(prompt)\n",
        "                        print(f\"Ground: {gt_sql}\")\n",
        "                        # input()\n",
        "\n",
        "                        prompt_dict = {\n",
        "                            \"input\": prompt,\n",
        "                            \"target\": gt_sql\n",
        "                        }\n",
        "\n",
        "                        prompts_collection.append(prompt_dict)\n",
        "\n",
        "                else:\n",
        "\n",
        "\n",
        "                    print(prompt)\n",
        "                    print(f\"Ground: {gt_sql}\")\n",
        "\n",
        "                    prompt_dict1 = {\n",
        "                        \"input\": prompt,\n",
        "                        \"db_id\": db_id,\n",
        "                        \"target\": gt_sql\n",
        "                    }\n",
        "\n",
        "                    prompt_dict2 = {\n",
        "                        \"input\": prompt,\n",
        "                        \"target\": gt_sql\n",
        "                    }\n",
        "\n",
        "\n",
        "                    prompts_collection_db.append(prompt_dict1)\n",
        "                    prompts_collection.append(prompt_dict2)\n",
        "\n",
        "\n",
        "        if args.PSG:\n",
        "            filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_{args.flags}_psg.json\")\n",
        "\n",
        "            with open(filename, mode='w',encoding='utf-8') as file:\n",
        "                json.dump(prompts_collection, file, ensure_ascii=False, indent=4)\n",
        "        else:\n",
        "            filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_{args.flags}.json\")\n",
        "\n",
        "            with open(filename, mode='w',encoding='utf-8') as file:\n",
        "                json.dump(prompts_collection, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        if prompts_collection_db:\n",
        "            filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_db_id_{args.flags}.json\")\n",
        "            with open(filename, mode='w', encoding='utf-8') as file:\n",
        "                json.dump(prompts_collection_db, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # from data_process import truncate_sql_before_keywords, CLAUSE_KEYWORDS\n",
        "#     parser = argparse.ArgumentParser(description='SQL')\n",
        "#     parser.add_argument(\"--dataset\", default='bird', type=str)\n",
        "#     parser.add_argument(\"--data_path\", default='/data/vda/dataset', type=str)\n",
        "#     parser.add_argument(\"--output_path\", default='/data/vda/dataset', type=str)\n",
        "#     parser.add_argument(\"--mode\", default='dev', type=str)\n",
        "#     parser.add_argument(\"--PSG\", action='store_true', default=False)\n",
        "#     parser.add_argument(\"--tag\", default='SQL-o1', type=str)\n",
        "#     parser.add_argument(\"--gpus\", default=4, type=int)\n",
        "#     parser.add_argument(\"--eval_sft\", default=1, type=int)\n",
        "#     parser.add_argument(\"--flags\", default='0', type=str)\n",
        "#     # parser.add_argument(\"--LLM_model\", default='meta-llama/Llama-3-8B-Instruct', type=str)\n",
        "#     parser.add_argument(\"--LLM_model\", default='/data/vda/saves/llama3-8b', type=str)\n",
        "#     parser.add_argument(\"--batch_size\", default=32, type=int)\n",
        "#     args = parser.parse_args()\n",
        "#     print(args)\n",
        "#     eval_all(args)"
      ],
      "metadata": {
        "id": "wwCW_r0O1NHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=q[''][0]"
      ],
      "metadata": {
        "id": "iOHrIqIuJX5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(q[70:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFoKpVviPzd5",
        "outputId": "22cdc374-ebad-4d13-91a5-2626b422aba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_results = []\n",
        "data_header = [[\"NLQ\", \"Predict\", \"GOLD\", 'database']]\n",
        "prompts = []\n",
        "db_path = '/content/database.db'\n",
        "schema_dict = get_schema_dict(db_path)\n",
        "database_schema, examples = get_schmea_str_and_examples(schema_dict)\n",
        "schema_dict_ = schema_dict\n",
        "\n",
        "prompt = [question, schema_dict, '', schema_dict_]\n",
        "prompts.append([database_schema, str(examples), question, prompt, db_path])\n",
        "prompts"
      ],
      "metadata": {
        "id": "5Pg_8xyUJNBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8effcfd7-1a74-4bc6-a7a0-d0d526fae5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['CREATE TABLE electricity ( TEXT,  NUMBER,  TEXT,  NUMBER, pnu NUMBER, PRIMARY KEY(, , , , pnu)); CREATE TABLE building (pnu TEXT,  TEXT,  TEXT,  NUMBER,  NUMBER,  TEXT,  TEXT,  NUMBER,  TEXT, PNU TEXT, PRIMARY KEY(pnu, , , , , , , , , PNU)); CREATE TABLE population ( TEXT, 2023_ TEXT, 2023_ TEXT, \"2023_ \" NUMBER, \"2023_ \" TEXT, \"2023_ \" TEXT, 2024_ TEXT, 2024_ TEXT, \"2024_ \" NUMBER, \"2024_ \" TEXT, \"2024_ \" TEXT, pnu TEXT, PNU TEXT, PRIMARY KEY(, 2023_, 2023_, \"2023_ \", \"2023_ \", \"2023_ \", 2024_, 2024_, \"2024_ \", \"2024_ \", \"2024_ \", pnu, PNU)); CREATE TABLE gas ( TEXT,  NUMBER,  TEXT,  NUMBER, pnu TEXT, PNU TEXT, PRIMARY KEY(, , , , pnu, PNU));',\n",
              "  \"electricity: [('   121-70', 202401, '  68   79 () ', 180038, 2720010100012170), ('   121-70', 202402, '  68   77 () ', 181483, 2720010100012170), ('   121-70', 202403, '  68   77 () ', 163726, 2720010100012170)]\\nbuilding: [('277202502104091', '    409-1', '  5 5-10 ( )', 1, 0, '19921123', '19920629', 22.68, '', '2772025021'), ('27720250220221', '    22-1', None, 3, 0, '19901006', None, 189.72, '1', '2772025022'), ('277202502201960', '    196', '  3 19-11 ( )', 1, 0, '19871022', None, 95.18, '', '2772025022')]\\npopulation: [(' ', '81,015', '41,163', 1.97, '38,868', '42,147', '89,685', '45,209', 1.98, '42,950', '46,735', '271100', '271100'), ('  ', '7,891', '4,809', 1.64, '3,940', '3,951', '8,401', '5,092', 1.65, '4,182', '4,219', '2711051700', '2711051700'), ('  ', '6,395', '4,093', 1.56, '3,050', '3,345', '6,678', '4,177', 1.6, '3,243', '3,435', '2711054500', '2711054500')]\\ngas: [('   121-70', 202401, '  68   79 () ', 694234, '2720010100012170', '2720010100'), ('   121-70', 202402, '  68   77 () ', 779517, '2720010100012170', '2720010100'), ('   121-70', 202403, '  68   79 () ', 624218, '2720010100012170', '2720010100')]\",\n",
              "  ' ,  ,  ,        .',\n",
              "  [' ,  ,  ,        .',\n",
              "   {'tables': {'electricity': {'': ['TEXT',\n",
              "       True,\n",
              "       ['   121-70', '   121-70', '   121-70']],\n",
              "      '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['  68   79 () ',\n",
              "        '  68   77 () ',\n",
              "        '  68   77 () ']],\n",
              "      '': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "      'pnu': ['NUMBER',\n",
              "       True,\n",
              "       [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "     'building': {'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['277202502104091', '27720250220221', '277202502201960']],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['    409-1',\n",
              "        '    22-1',\n",
              "        '    196']],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['  5 5-10 ( )',\n",
              "        None,\n",
              "        '  3 19-11 ( )']],\n",
              "      '': ['NUMBER', True, [1, 3, 1]],\n",
              "      '': ['NUMBER', True, [0, 0, 0]],\n",
              "      '': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "      '': ['TEXT', True, ['19920629', None, None]],\n",
              "      '': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "      '': ['TEXT', True, ['', '1', '']],\n",
              "      'PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "     'population': {'': ['TEXT',\n",
              "       True,\n",
              "       [' ', '  ', '  ']],\n",
              "      '2023_': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "      '2023_': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "      '2023_ ': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "      '2023_ ': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "      '2023_ ': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "      '2024_': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "      '2024_': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "      '2024_ ': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "      '2024_ ': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "      '2024_ ': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "      'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "      'PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "     'gas': {'': ['TEXT',\n",
              "       True,\n",
              "       ['   121-70', '   121-70', '   121-70']],\n",
              "      '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['  68   79 () ',\n",
              "        '  68   77 () ',\n",
              "        '  68   79 () ']],\n",
              "      '': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "      'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "      'PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "    'foreign_keys': []},\n",
              "   '',\n",
              "   {'tables': {'electricity': {'': ['TEXT',\n",
              "       True,\n",
              "       ['   121-70', '   121-70', '   121-70']],\n",
              "      '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['  68   79 () ',\n",
              "        '  68   77 () ',\n",
              "        '  68   77 () ']],\n",
              "      '': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "      'pnu': ['NUMBER',\n",
              "       True,\n",
              "       [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "     'building': {'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['277202502104091', '27720250220221', '277202502201960']],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['    409-1',\n",
              "        '    22-1',\n",
              "        '    196']],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['  5 5-10 ( )',\n",
              "        None,\n",
              "        '  3 19-11 ( )']],\n",
              "      '': ['NUMBER', True, [1, 3, 1]],\n",
              "      '': ['NUMBER', True, [0, 0, 0]],\n",
              "      '': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "      '': ['TEXT', True, ['19920629', None, None]],\n",
              "      '': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "      '': ['TEXT', True, ['', '1', '']],\n",
              "      'PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "     'population': {'': ['TEXT',\n",
              "       True,\n",
              "       [' ', '  ', '  ']],\n",
              "      '2023_': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "      '2023_': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "      '2023_ ': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "      '2023_ ': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "      '2023_ ': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "      '2024_': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "      '2024_': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "      '2024_ ': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "      '2024_ ': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "      '2024_ ': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "      'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "      'PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "     'gas': {'': ['TEXT',\n",
              "       True,\n",
              "       ['   121-70', '   121-70', '   121-70']],\n",
              "      '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "      '': ['TEXT',\n",
              "       True,\n",
              "       ['  68   79 () ',\n",
              "        '  68   77 () ',\n",
              "        '  68   79 () ']],\n",
              "      '': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "      'pnu': ['TEXT',\n",
              "       True,\n",
              "       ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "      'PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "    'foreign_keys': []}],\n",
              "  '/content/database.db']]"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts[0][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcXuffqic2S3",
        "outputId": "6eda4d58-8662-4a2f-83bc-661f5953b4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ,  ,  ,        .',\n",
              " {'tables': {'electricity': {'': ['TEXT',\n",
              "     True,\n",
              "     ['   121-70', '   121-70', '   121-70']],\n",
              "    '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['  68   79 () ',\n",
              "      '  68   77 () ',\n",
              "      '  68   77 () ']],\n",
              "    '': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "    'pnu': ['NUMBER',\n",
              "     True,\n",
              "     [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "   'building': {'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['277202502104091', '27720250220221', '277202502201960']],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['    409-1',\n",
              "      '    22-1',\n",
              "      '    196']],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['  5 5-10 ( )',\n",
              "      None,\n",
              "      '  3 19-11 ( )']],\n",
              "    '': ['NUMBER', True, [1, 3, 1]],\n",
              "    '': ['NUMBER', True, [0, 0, 0]],\n",
              "    '': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "    '': ['TEXT', True, ['19920629', None, None]],\n",
              "    '': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "    '': ['TEXT', True, ['', '1', '']],\n",
              "    'PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "   'population': {'': ['TEXT',\n",
              "     True,\n",
              "     [' ', '  ', '  ']],\n",
              "    '2023_': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "    '2023_': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "    '2023_ ': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "    '2023_ ': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "    '2023_ ': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "    '2024_': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "    '2024_': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "    '2024_ ': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "    '2024_ ': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "    '2024_ ': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "    'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "    'PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "   'gas': {'': ['TEXT',\n",
              "     True,\n",
              "     ['   121-70', '   121-70', '   121-70']],\n",
              "    '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['  68   79 () ',\n",
              "      '  68   77 () ',\n",
              "      '  68   79 () ']],\n",
              "    '': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "    'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "    'PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "  'foreign_keys': []},\n",
              " '',\n",
              " {'tables': {'electricity': {'': ['TEXT',\n",
              "     True,\n",
              "     ['   121-70', '   121-70', '   121-70']],\n",
              "    '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['  68   79 () ',\n",
              "      '  68   77 () ',\n",
              "      '  68   77 () ']],\n",
              "    '': ['NUMBER', True, [180038, 181483, 163726]],\n",
              "    'pnu': ['NUMBER',\n",
              "     True,\n",
              "     [2720010100012170, 2720010100012170, 2720010100012170]]},\n",
              "   'building': {'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['277202502104091', '27720250220221', '277202502201960']],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['    409-1',\n",
              "      '    22-1',\n",
              "      '    196']],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['  5 5-10 ( )',\n",
              "      None,\n",
              "      '  3 19-11 ( )']],\n",
              "    '': ['NUMBER', True, [1, 3, 1]],\n",
              "    '': ['NUMBER', True, [0, 0, 0]],\n",
              "    '': ['TEXT', True, ['19921123', '19901006', '19871022']],\n",
              "    '': ['TEXT', True, ['19920629', None, None]],\n",
              "    '': ['NUMBER', True, [22.68, 189.72, 95.18]],\n",
              "    '': ['TEXT', True, ['', '1', '']],\n",
              "    'PNU': ['TEXT', True, ['2772025021', '2772025022', '2772025022']]},\n",
              "   'population': {'': ['TEXT',\n",
              "     True,\n",
              "     [' ', '  ', '  ']],\n",
              "    '2023_': ['TEXT', True, ['81,015', '7,891', '6,395']],\n",
              "    '2023_': ['TEXT', True, ['41,163', '4,809', '4,093']],\n",
              "    '2023_ ': ['NUMBER', True, [1.97, 1.64, 1.56]],\n",
              "    '2023_ ': ['TEXT', True, ['38,868', '3,940', '3,050']],\n",
              "    '2023_ ': ['TEXT', True, ['42,147', '3,951', '3,345']],\n",
              "    '2024_': ['TEXT', True, ['89,685', '8,401', '6,678']],\n",
              "    '2024_': ['TEXT', True, ['45,209', '5,092', '4,177']],\n",
              "    '2024_ ': ['NUMBER', True, [1.98, 1.65, 1.6]],\n",
              "    '2024_ ': ['TEXT', True, ['42,950', '4,182', '3,243']],\n",
              "    '2024_ ': ['TEXT', True, ['46,735', '4,219', '3,435']],\n",
              "    'pnu': ['TEXT', True, ['271100', '2711051700', '2711054500']],\n",
              "    'PNU': ['TEXT', True, ['271100', '2711051700', '2711054500']]},\n",
              "   'gas': {'': ['TEXT',\n",
              "     True,\n",
              "     ['   121-70', '   121-70', '   121-70']],\n",
              "    '': ['NUMBER', True, [202401, 202402, 202403]],\n",
              "    '': ['TEXT',\n",
              "     True,\n",
              "     ['  68   79 () ',\n",
              "      '  68   77 () ',\n",
              "      '  68   79 () ']],\n",
              "    '': ['NUMBER', True, [694234, 779517, 624218]],\n",
              "    'pnu': ['TEXT',\n",
              "     True,\n",
              "     ['2720010100012170', '2720010100012170', '2720010100012170']],\n",
              "    'PNU': ['TEXT', True, ['2720010100', '2720010100', '2720010100']]}},\n",
              "  'foreign_keys': []}]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q = pd.read_csv('/content/drive/MyDrive/questions.csv')\n",
        "q = q[90:]\n",
        "print(len(q))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w32_KUE7SB5c",
        "outputId": "c98289c2-4e69-4ce6-8709-8b8f82ef9b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in enumerate(q['']):\n",
        "\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBpa1GPbR45W",
        "outputId": "b2a9bcc2-fc2f-4ae9-f4c0-37a197e178b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  5,000   , 2023 1     5  ?\n",
            "2023           ?\n",
            "   1990    ,       .\n",
            "    (: +10 )     .\n",
            "      50%   2023     .\n",
            "    10%      ?\n",
            "           ?\n",
            "2023     20%      ,   .\n",
            "  2023   (  ) ,     .\n",
            "              ?\n",
            "2023 ,          ?\n",
            " 1,000   , 2023   (/)    ?\n",
            "         .\n",
            "   2023 (6~8)       10  .\n",
            "2023 ,   1       .\n",
            " , ,     ,      .\n",
            "    5%  ,         ?\n",
            "2023             .\n",
            "         .\n",
            " ,  ,      Z-score ,    .\n",
            "2023        5 ?\n",
            " 2023          .\n",
            "   2023 1 8       ?\n",
            "          .\n",
            " 1      2023    ?\n",
            "              .\n",
            "2023        (  )  ?\n",
            "2023  , ,  3       .\n",
            "          .\n",
            " ,  ,  ,        .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    sql_results = []\n",
        "    data_header = [[\"NLQ\", \"Predict\", \"GOLD\", 'database']]\n",
        "    prompts = []\n",
        "    for index, row in enumerate(q['']):\n",
        "        # if 'spider' in dataset:\n",
        "        #     row['SQL'] = row['query']\n",
        "        # if 'drspider' in dataset:\n",
        "        #     row['SQL'] = row['query']\n",
        "        print(row)\n",
        "        question, db_id = row, 1\n",
        "        db_path = '/content/database.db'\n",
        "\n",
        "        schema_dict = get_schema_dict(db_path)\n",
        "        database_schema, examples = get_schmea_str_and_examples(schema_dict)\n",
        "        schema_dict_ = schema_dict\n",
        "\n",
        "        prompt = [question, schema_dict, '', schema_dict_]\n",
        "        prompts.append([database_schema, str(examples), question, 'SQL', db_id, prompt, db_path])\n",
        "    batch_size = 2\n",
        "    n_samples = len(q[70:])\n",
        "    n_batches = (n_samples - 1) // batch_size + 1\n",
        "\n",
        "    prompts_collection = []\n",
        "    prompts_collection_db = []\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = n_samples if i == n_batches - 1 else (i + 1) * batch_size\n",
        "        batch_prompts = prompts[start: end]\n",
        "        schema_dicts = []  # only keep the tables\n",
        "\n",
        "        for j, v in enumerate(batch_prompts):\n",
        "            batch_prompts[j][1] = get_example_str(batch_prompts[j][5][1], kkkkk)\n",
        "        # text-to-sql\n",
        "# prompt_cw_temp_sft = Given the following database schema and question, your task is to write a valid SQL query whose execution will accurately answer the question. If the value below the incomplete SQL query is not empty, your task is to complete it into a full SQL query. Remember to end the query with a semicolom ```;```.\n",
        "# Database schema:{ds} Sample rows of each table:{sr} Question:{qs}{hint} Question hint:{sql} The incomplete SQL query:{sql}\n",
        "# Answer the question by a SQL query only with no explanation:\n",
        "        final_prompts = [prompt_cw_temp_sft.format(ds=j[0], sr=j[1], qs=j[2], hint=j[5][2], sql='') for j in batch_prompts]\n",
        "        print(final_prompts)\n",
        "        response_strs = generator.generate_response(prompts=final_prompts)\n",
        "\n",
        "\n",
        "        nc_idx = []\n",
        "        continue_sqls = []\n",
        "        # noisy correction\n",
        "\n",
        "        for idx, v in enumerate(response_strs):\n",
        "            pre_sql = parse_sql_from_string(response_strs[idx])\n",
        "            ex_flg3 = True if execute_query(batch_prompts[idx][6], pre_sql)[1] == '' else False\n",
        "            hard = contains_subquery(pre_sql, batch_prompts[idx][5][1]['tables'].keys())\n",
        "            if ex_flg3 == False or hard > 2:\n",
        "                common_sql = 'SELECT '\n",
        "                continue_sqls.append(common_sql)\n",
        "                nc_idx.append(idx)\n",
        "\n",
        "        # PSG\n",
        "        if args.PSG:\n",
        "            cl_prompts = []\n",
        "            for j, idx in enumerate(nc_idx):\n",
        "                v = batch_prompts[idx]\n",
        "                ds = get_schmea_str_and_examples(v[5][1])[0]\n",
        "                sr = get_example_str(v[5][1], kkkkk)\n",
        "                common_sql = continue_sqls[j]\n",
        "                if args.eval_sft == 1:\n",
        "                    cl_prompts.append(\n",
        "                        prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "                else:\n",
        "                    cl_prompts.append(prompt_cw_temp_sft.format(ds=ds, sr=sr, qs=v[2], hint=v[5][2], sql=common_sql))\n",
        "\n",
        "            if len(nc_idx) > 0:\n",
        "                response_strs_ = generator.generate_response(prompts=cl_prompts)\n",
        "                print(\"%%%%%%%%%%%%%%%%%%\", response_strs_[0])\n",
        "                for idx, v in enumerate(nc_idx):\n",
        "                    if execute_query(batch_prompts[v][6], parse_sql_from_string(response_strs_[idx]))[\n",
        "                        0] is not None:\n",
        "                        response_strs[v] = response_strs_[idx]\n",
        "\n",
        "        for j, response_str in enumerate(response_strs):\n",
        "            database_schema = batch_prompts[j][0]\n",
        "            question = batch_prompts[j][2]\n",
        "            gt_sql = replace_multiple_spaces(batch_prompts[j][3])\n",
        "            if gt_sql.endswith(\";;\"):\n",
        "                gt_sql = gt_sql[:-1]\n",
        "\n",
        "            if not gt_sql.endswith(\";\"):\n",
        "                gt_sql += \";\"\n",
        "\n",
        "            db_id = batch_prompts[j][4]\n",
        "            prompt = final_prompts[j]\n",
        "            print(f\"=={start + j + 1}/{len(data_tuples)}=={db_id}=={tag}==================\")\n",
        "\n",
        "            try:\n",
        "                if dataset == 'spider':\n",
        "                    if mode == 'test':\n",
        "                        db_path = os.path.join(args.data_path, dataset, 'test_database', db_id, f\"{db_id}.sqlite\")\n",
        "                    else:\n",
        "                        db_path = os.path.join(args.data_path, dataset, 'database', db_id, f\"{db_id}.sqlite\")\n",
        "                elif dataset == 'bird':\n",
        "                    db_path = os.path.join(args.data_path, dataset, f'{mode}/{mode}_databases', db_id,\n",
        "                                            f\"{db_id}.sqlite\")\n",
        "                else:\n",
        "                    raise TypeError(f\"Unexpect dataset: {dataset}.\")\n",
        "\n",
        "                SQL_str = parse_sql_from_string(response_str)\n",
        "            except Exception as e:\n",
        "                res = f'error: {str(e)}'\n",
        "                print(res, response_str)\n",
        "\n",
        "            sql_results.append([question, SQL_str, gt_sql, db_id])\n",
        "\n",
        "            print(prompt)\n",
        "            print(f\"Ground: {gt_sql}\")\n",
        "\n",
        "            prompt_dict1 = {\n",
        "                \"input\": prompt,\n",
        "                \"db_id\": db_id,\n",
        "                \"target\": gt_sql\n",
        "            }\n",
        "\n",
        "            prompt_dict2 = {\n",
        "                \"input\": prompt,\n",
        "                \"target\": gt_sql\n",
        "            }\n",
        "\n",
        "\n",
        "            prompts_collection_db.append(prompt_dict1)\n",
        "            prompts_collection.append(prompt_dict2)\n",
        "\n",
        "\n",
        "    filename = os.path.join('', f\"gpt_ninewatt_test_0.json\")\n",
        "\n",
        "    with open(filename, mode='w',encoding='utf-8') as file:\n",
        "        json.dump(prompts_collection, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    if prompts_collection_db:\n",
        "        filename = os.path.join(args.output_path, f\"{tag}_{dataset}_{mode}_db_id_{args.flags}.json\")\n",
        "        with open(filename, mode='w', encoding='utf-8') as file:\n",
        "            json.dump(prompts_collection_db, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by7VdMCJPQ65",
        "outputId": "cc338367-5fff-4ddf-cb41-fc0a88f380bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023        5 ?\n",
            " 2023          .\n",
            "   2023 1 8       ?\n",
            "          .\n",
            " 1      2023    ?\n",
            "              .\n",
            "2023        (  )  ?\n",
            "2023  , ,  3       .\n",
            "          .\n",
            " ,  ,  ,        .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: /content/gpt_ninewatt_test_0.json load\n",
        "\n",
        "data = read_json_file(\"/content/gpt_ninewatt_test_0.json\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGFcugFlSYJU",
        "outputId": "85aa206d-3636-4da9-d744-7f599e1f52a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "u-Fx4ajDKShS",
        "outputId": "71d4476c-9428-4e4d-ec02-08763655fba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"energy_usage: [('   121-70', 202401, '  68   79 () ', 180038, 2720010100012170), ('   121-70', 202402, '  68   77 () ', 181483, 2720010100012170), ('   121-70', 202403, '  68   77 () ', 163726, 2720010100012170)]\\nelectricity: [('   121-70', 202401, '  68   79 () ', 180038, 2720010100012170), ('   121-70', 202402, '  68   77 () ', 181483, 2720010100012170), ('   121-70', 202403, '  68   77 () ', 163726, 2720010100012170)]\\nbuilding: [('277202502104091', '    409-1', '  5 5-10 ( )', 1, 0, '19921123', '19920629', 22.68, '', '2772025021'), ('27720250220221', '    22-1', None, 3, 0, '19901006', None, 189.72, '1', '2772025022'), ('277202502201960', '    196', '  3 19-11 ( )', 1, 0, '19871022', None, 95.18, '', '2772025022')]\\npopulation: [(' ', '81,015', '41,163', 1.97, '38,868', '42,147', '89,685', '45,209', 1.98, '42,950', '46,735', '271100', '271100'), ('  ', '7,891', '4,809', 1.64, '3,940', '3,951', '8,401', '5,092', 1.65, '4,182', '4,219', '2711051700', '2711051700'), ('  ', '6,395', '4,093', 1.56, '3,050', '3,345', '6,678', '4,177', 1.6, '3,243', '3,435', '2711054500', '2711054500')]\\ngas: [('   121-70', 202401, '  68   79 () ', 694234, '2720010100012170', '2720010100'), ('   121-70', 202402, '  68   77 () ', 779517, '2720010100012170', '2720010100'), ('   121-70', 202403, '  68   79 () ', 624218, '2720010100012170', '2720010100')]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI()\n",
        "# def gpt(prompt):\n",
        "#   response = client.chat.completions.create(\n",
        "#       model=\"gpt-4o-mini\",\n",
        "#       messages=[\n",
        "#           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#           {\"role\": \"user\", \"content\": input}\n",
        "#       ]\n",
        "#   )\n",
        "class LLM_Model(object):\n",
        "    def __init__(self, model='gpt'):\n",
        "\n",
        "        self.model = model\n",
        "        self.llm = client\n",
        "\n",
        "    def generate_response(self, prompts, max_tokens=1024, temperature=0.01, top_p=0.5):\n",
        "        outputs = []\n",
        "        # if self.tag in ['mistral']:\n",
        "            # messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "\n",
        "        messages_list = [\n",
        "                [{\"role\": \"system\", \"content\": \"You are a helpful SQLite assistant.\"}, {\"role\": \"user\", \"content\": p}]\n",
        "                for p in prompts]\n",
        "        for message in messages_list:\n",
        "          output = self.llm.chat.completions.create(model=\"gpt-4o-mini\",messages= message)\n",
        "          outputs.append(output.choices[0].message.content)\n",
        "        return  outputs\n",
        "\n",
        "\n",
        "generator = LLM_Model()\n",
        "# response_strs = generator.generate_response(prompts=final_prompts)\n",
        "final_prompts = ['hello', '1+1=?']\n",
        "generator.generate_response(prompts=final_prompts)\n"
      ],
      "metadata": {
        "id": "x2Hh7ZfDL2yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nohup python _run_explore.py --task_name bird >> result_mcts_0.txt 2>&1 &\n",
        "python validation_results.py --json_path ./mcts_results/bird_mcts_dev.json ( | spider_mcts_dev.json | spider_syn.json | spider_DK.json | spider_real.json | spider_test.json ) --db_root_path ./dataset/bird/dev/dev_databases --num_cpus 1 --diff_json_path ./dataset/bird/dev/dev.json  --output_file  spider_dev.sql (...)"
      ],
      "metadata": {
        "id": "Tsv0DSSu2lOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "from ordered_set import OrderedSet\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "def dump_json(obj, fname, indent=4, mode='w', encoding=\"utf8\", ensure_ascii=False):\n",
        "    if \"b\" in mode:\n",
        "        encoding = None\n",
        "    with open(fname, \"w\", encoding=encoding) as f:\n",
        "        return json.dump(obj, f, indent=indent, ensure_ascii=ensure_ascii)\n",
        "\n",
        "\n",
        "def log_agent(agent, file_path):\n",
        "    save_dict = agent\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    with open(file_path, 'a') as f:\n",
        "        json.dump(save_dict, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Parsing the input of agents, llms and llm context length.')\n",
        "parser.add_argument(\"--task_name\", type=str, help=\"task_name\", default=\"spider\")  # spider\n",
        "parser.add_argument(\"--input_file\", type=str, help=\"Dev file\", default=\"./\")  # spider\n",
        "# parser.add_argument(\"--output_path\", type=str, help=\"Dev file\", default=\"\")  # spider\n",
        "# parser.add_argument(\"--split\", type=int, help=\"split\", default=0)\n",
        "args = parser.parse_args()\n",
        "\n",
        "para_configs = {\n",
        "    \"mcts_iters\": 8,\n",
        "    \"deapth_limit\": 20,\n",
        "    \"explore_rate\": 100,\n",
        "    \"step_topk\": 3,\n",
        "    \"reflect_threshold\": 50.0,\n",
        "    \"reward_alpha\": 0.4\n",
        "}\n",
        "\n",
        "\n",
        "def run_text2sql():\n",
        "\n",
        "\n",
        "    llm_simulate = f'http://localhost:8000/llm'\n",
        "    llm_reward = f'http://localhost:8000/llm'\n",
        "    base_model = {'select': llm_select, 'simulate': llm_simulate, 'reward': llm_reward}\n",
        "\n",
        "    if args.task_name == \"bird\":\n",
        "        file_path = './dataset/SQL-o1_bird_dev_db_id_0.json'\n",
        "\n",
        "\n",
        "    sql_data = json.load(open(file_path))\n",
        "\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0121_bird_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0121_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0130_syn_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0130_real_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0130_DK_spider_dev_db_id_0_0_0_0.json'))\n",
        "    # sql_data = json.load(open(f'/data/vda/dataset/0210_spider_test_db_id_0_0_0_0.json'))\n",
        "    save_sql_data = []\n",
        "    os.makedirs('mcts_results', exist_ok=True)\n",
        "    save_path = os.path.join('mcts_results', f'{args.task_name}_mcts_dev.json')\n",
        "\n",
        "    # os.makedirs(f'/data/vda/mcts', exist_ok=True)\n",
        "    # save_path = f'/data/vda/mcts/result/{args.task_name}/{args.task_name}_mcts_llama3-8b_2.json'\n",
        "\n",
        "    prompt = para_configs.copy()\n",
        "\n",
        "    for row in tqdm(sql_data):\n",
        "        # if \"such multi - national companies as Dupont , HP\" not in row['input'] and continue_flag:\n",
        "        #     continue\n",
        "        # else:\n",
        "        #     continue_flag = False\n",
        "\n",
        "        # print(row['input'])\n",
        "        world_model = AgentWorldModel(base_model=base_model, prompt=prompt, max_steps=prompt['deapth_limit'])\n",
        "        config = AgentConfig(base_model=base_model, prompt=prompt, reward_alpha=prompt['reward_alpha'])\n",
        "        algorithm = MCTS(depth_limit=prompt['deapth_limit'], disable_tqdm=False, output_trace_in_each_iter=True,\n",
        "                         n_iters=prompt['mcts_iters'], w_exp=prompt['explore_rate'], cum_reward=np.mean, calc_q=max)  #\n",
        "        reasoner_rap = Reasoner(world_model=world_model, search_config=config, search_algo=algorithm)\n",
        "        result_rap = reasoner_rap(row)\n",
        "        if row.get('target', \"\"):\n",
        "            row['target'] = row['target'][:-1] if row['target'].endswith(';;') else row['target']\n",
        "\n",
        "        # [(res[-1].state.blocks_state, res[-1].Q) for res in result_rap.trace_in_each_iter]\n",
        "        # print(\"Answer:\\n\", row['target'])\n",
        "        # for o in list(set([res[-1].state.blocks_state for res in result_rap.trace_in_each_iter])):\n",
        "        #     print(o)\n",
        "\n",
        "        # print(result_rap._output_cum_reward)\n",
        "\n",
        "\n",
        "        row['result_mcts'] = list(OrderedSet([( res[0], res[1][-1].state.blocks_state) for res in result_rap.trace_in_each_iter]))\n",
        "        if result_rap.trace_worst[1]:\n",
        "            row['result_mcts_worst'] = [(result_rap.trace_worst[0], result_rap.trace_worst[1][0][-1].blocks_state)]\n",
        "        else:\n",
        "            row['result_mcts_worst'] = ''\n",
        "\n",
        "        if result_rap.trace[1]:\n",
        "            row['result_mcts_best'] = [(result_rap.trace[0], result_rap.trace[1][0][-1].blocks_state)]\n",
        "        else:\n",
        "            row['result_mcts_best'] = ''\n",
        "\n",
        "        save_sql_data.append(copy.deepcopy(row))\n",
        "        dump_json(save_sql_data, save_path, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_text2sql()"
      ],
      "metadata": {
        "id": "i4kySMAp2oEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python validation_results.py\n",
        "--json_path ./mcts_results/bird_mcts_dev.json ( | spider_mcts_dev.json | spider_syn.json | spider_DK.json | spider_real.json | spider_test.json )\n",
        "--db_root_path ./dataset/bird/dev/dev_databases\n",
        "--num_cpus 1\n",
        "--diff_json_path ./dataset/bird/dev/dev.json\n",
        "--output_file  spider_dev.sql (...)"
      ],
      "metadata": {
        "id": "lTpSB84Y5G4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import sqlite3\n",
        "import multiprocessing as mp\n",
        "from func_timeout import func_timeout, FunctionTimedOut\n",
        "\n",
        "\n",
        "def load_json(json_path):\n",
        "    \"\"\"Load JSON file\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "\n",
        "def result_callback(result):\n",
        "    \"\"\"Callback function to store execution results\"\"\"\n",
        "    exec_result.append(result)\n",
        "\n",
        "\n",
        "def execute_sql(predicted_sql, target_sql, db_path):\n",
        "    \"\"\"Execute the predicted SQL and target SQL on the given database\"\"\"\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        cursor.execute(predicted_sql)\n",
        "        predicted_res = cursor.fetchall()\n",
        "\n",
        "        cursor.execute(target_sql)\n",
        "        target_res = cursor.fetchall()\n",
        "\n",
        "        if set(predicted_res) == set(target_res):\n",
        "            return 1  # Correct execution\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"You can only execute one statement at a time.\" in str(e):\n",
        "            return 1  # Allow single-statement constraint errors\n",
        "        return 0  # Execution error\n",
        "\n",
        "    return 0  # Default incorrect execution\n",
        "\n",
        "\n",
        "# def execute_model(predicted_sqls, target_sql, db_path, db_id, idx, meta_time_out, output_file):\n",
        "#     \"\"\"Execute multiple predicted SQLs and check if any of them succeeds\"\"\"\n",
        "#     try:\n",
        "#         for predicted_sql in predicted_sqls:\n",
        "#             res = func_timeout(meta_time_out, execute_sql, args=(predicted_sql, target_sql, db_path))\n",
        "#             if res == 1:  # If any result is correct, return success\n",
        "#                 with open(output_file, 'a', encoding='utf-8') as f:\n",
        "#                     f.write(predicted_sql + '\\t' + db_id + '\\n')\n",
        "#                 return {'sql_idx': idx, 'res': 1}\n",
        "#\n",
        "#     except KeyboardInterrupt:\n",
        "#         sys.exit(0)\n",
        "#     except FunctionTimedOut:\n",
        "#         return {'sql_idx': idx, 'res': 1}  # Timeout is considered correct\n",
        "#     except Exception:\n",
        "#         pass\n",
        "#\n",
        "#     return {'sql_idx': idx, 'res': 0}  # Default to incorrect execution\n",
        "\n",
        "def execute_model(predicted_sqls, target_sql, db_path, db_id, idx, meta_time_out, output_file):\n",
        "    \"\"\"Execute multiple predicted SQLs and check if any of them succeeds\"\"\"\n",
        "    success = False\n",
        "    flag = False\n",
        "    try:\n",
        "        for predicted_sql in predicted_sqls:\n",
        "            res = func_timeout(meta_time_out, execute_sql, args=(predicted_sql, target_sql, db_path))\n",
        "            if res == 1:  # If any result is correct, record success\n",
        "                with open(output_file, 'a', encoding='utf-8') as f:\n",
        "                    f.write(predicted_sql + '\\t' + db_id + '\\n')\n",
        "                success = True\n",
        "                flag = True\n",
        "                break\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "        # sys.exit(0)\n",
        "    except FunctionTimedOut:\n",
        "        success = False\n",
        "        flag = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if not flag and predicted_sqls:\n",
        "        with open(output_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(predicted_sqls[0] + '\\t' + db_id + '\\n')\n",
        "\n",
        "    return {'sql_idx': idx, 'res': 1 if success else 0}\n",
        "\n",
        "import re\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_sql(pre_sql):\n",
        "    return re.sub(r'\\s+', ' ', pre_sql.replace('\\n', ' ').strip())\n",
        "\n",
        "\n",
        "def extract_sql_from_backticks(pre_sql):\n",
        "    try:\n",
        "        sql_statements = re.findall(r'```(.*?)```', pre_sql, re.DOTALL)\n",
        "        if sql_statements:\n",
        "            sql_cleaned = clean_sql(sql_statements[0])\n",
        "            # \"sql\"sql\n",
        "            if 'sql' in sql_cleaned:\n",
        "                sql_cleaned = sql_cleaned.split('sql', 1)[1]  # 1\n",
        "            return sql_cleaned\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting SQL: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def package_sqls(json_path, db_root_path):\n",
        "    \"\"\"Extract SQLs and database paths from JSON, sorted by result_mcts score in descending order\"\"\"\n",
        "    json_data = load_json(json_path)\n",
        "    # print(len(json_data))\n",
        "    sql_data = []\n",
        "    db_paths = []\n",
        "\n",
        "    for item in json_data:\n",
        "        db_id = item.get(\"db_id\", \"\")\n",
        "        target_sql = item.get(\"target\", \"\")\n",
        "        result_mcts = item.get(\"result_mcts\", [])\n",
        "\n",
        "        result_mcts_sorted = sorted(result_mcts, key=lambda x: x[0], reverse=True) if result_mcts else []\n",
        "\n",
        "        predicted_sqls = [sql_pair[1] for sql_pair in result_mcts_sorted if isinstance(sql_pair, list) and len(sql_pair) == 2]\n",
        "\n",
        "        # new_predicted_sqls = []\n",
        "        # for pre_sql in predicted_sqls:\n",
        "        #     if '`' in pre_sql:\n",
        "        #         sql_cleaned = extract_sql_from_backticks(pre_sql)\n",
        "        #         if not sql_cleaned:\n",
        "        #             sql_cleaned = \"SELECT\"\n",
        "        #     else:\n",
        "        #         sql_cleaned = clean_sql(pre_sql)\n",
        "        #\n",
        "        #     if not sql_cleaned:  #\n",
        "        #         sql_cleaned = \"SELECT\"\n",
        "        #         print(f\"Empty SQL cleaned for input: {pre_sql}\")\n",
        "        #         input()\n",
        "        #\n",
        "        #     new_predicted_sqls.append(sql_cleaned)\n",
        "        #\n",
        "        # #\n",
        "        # # print(new_predicted_sqls)\n",
        "        #\n",
        "        # predicted_sqls = new_predicted_sqls\n",
        "\n",
        "        if not predicted_sqls or not target_sql or not db_id:\n",
        "            continue\n",
        "\n",
        "        db_path = os.path.join(db_root_path, db_id, f\"{db_id}.sqlite\")\n",
        "        sql_data.append((predicted_sqls, target_sql, db_id))\n",
        "        db_paths.append(db_path)\n",
        "\n",
        "    return sql_data, db_paths\n",
        "\n",
        "\n",
        "def run_sqls_parallel(sql_data, db_paths, output_file, num_cpus=1, meta_time_out=30.0):\n",
        "    \"\"\"Execute SQL queries in parallel\"\"\"\n",
        "    pool = mp.Pool(processes=num_cpus)\n",
        "    for i, (predicted_sqls, target_sql, db_id) in enumerate(sql_data):\n",
        "        pool.apply_async(execute_model, args=(predicted_sqls, target_sql, db_paths[i], db_id, i, meta_time_out, output_file),\n",
        "                         callback=result_callback)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def sort_results(list_of_dicts):\n",
        "    \"\"\"Sort execution results by SQL index\"\"\"\n",
        "    return sorted(list_of_dicts, key=lambda x: x['sql_idx'])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--json_path', type=str, default='/data/vda/mcts/bird_mcts_cw_qwen_dev.json', help=\"Path to JSON file containing SQL queries\")  # bird_mcts_plus_cw_dev | bird_mcts_cw_qwen_dev.json\n",
        "    parser.add_argument('--db_root_path', type=str, default='/data/vda/dataset/bird/dev/dev_databases', help=\"Root path of databases\")\n",
        "    parser.add_argument('--num_cpus', type=int, default=1, help=\"Number of CPU cores for parallel execution\")\n",
        "    parser.add_argument('--meta_time_out', type=float, default=30.0, help=\"Timeout per query execution\")\n",
        "    parser.add_argument('--diff_json_path', type=str, default='/data/vda/dataset/bird/dev/dev.json', help=\"Path to JSON file containing difficulty levels\")\n",
        "    parser.add_argument('--output_file', type=str, default='bird_dev_queries_qwen.sql', help=\"File to store successful queries\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    exec_result = []\n",
        "\n",
        "    print(args.json_path)\n",
        "\n",
        "    with open(args.output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    sql_data, db_paths = package_sqls(args.json_path, args.db_root_path)\n",
        "    run_sqls_parallel(sql_data, db_paths, args.output_file, num_cpus=args.num_cpus, meta_time_out=args.meta_time_out)\n",
        "    # print(exec_result)\n",
        "    exec_result = sort_results(exec_result)\n",
        "    print(\"The SQL file has been generated. Please test it using the test suite.\")"
      ],
      "metadata": {
        "id": "naWchKy821jb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}